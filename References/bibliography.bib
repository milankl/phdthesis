
@misc{2016,
  title = {Wayback {{Machine}}},
  year = {2016},
  month = mar,
  howpublished = {https://web.archive.org/web/20160304050107/http://www.chemheritage.org/Downloads/Publications/Books/Understanding-Moores-Law/Understanding-Moores-Law\_Chapter-07.pdf},
  file = {/Users/milan/Zotero/storage/2MDLJMNE/2016 - Wayback Machine.pdf}
}

@misc{2020,
  title = {{{JuliaGraphics}}/{{Luxor}}.Jl},
  year = {2020},
  month = mar,
  abstract = {Simple drawings using vector graphics; Cairo "for tourists!"},
  howpublished = {JuliaGraphics},
  keywords = {cairo,diagrams,drawing,graphics,julia,luxor,simple,vector-graphics}
}

@misc{Aarstol2021,
  title = {Our Company Started 5-Hour Workdays in 2015. {{Here}}'s Why We're Still Doing It},
  author = {Aarstol, Stephan and Aarstol, Stephan and Aarstol, Stephan},
  year = {2021},
  month = mar,
  journal = {Fast Company},
  abstract = {The founder of Tower says he has no doubt that the 25-hour workweek will require further fine-tuning, but the story is, on the whole, one of success.},
  howpublished = {https://www.fastcompany.com/90615133/our-company-started-5-hour-workdays-in-2015-heres-why-were-still-doing-it},
  language = {en-US},
  file = {/Users/milan/Zotero/storage/5IA394W4/our-company-started-5-hour-workdays-in-2015-heres-why-were-still-doing-it.html}
}

@article{Abernathey2010,
  title = {Enhancement of {{Mesoscale Eddy Stirring}} at {{Steering Levels}} in the {{Southern Ocean}}},
  author = {Abernathey, Ryan and Marshall, John and Mazloff, Matt and Shuckburgh, Emily},
  year = {2010},
  month = jan,
  journal = {Journal of Physical Oceanography},
  volume = {40},
  number = {1},
  pages = {170--184},
  issn = {0022-3670, 1520-0485},
  doi = {10.1175/2009JPO4201.1},
  abstract = {Meridional cross sections of effective diffusivity in the Southern Ocean are presented and discussed. The effective diffusivity, Keff, characterizes the rate at which mesoscale eddies stir properties on interior isopycnal surfaces and laterally at the sea surface. The distributions are obtained by monitoring the rate at which eddies stir an idealized tracer whose initial distribution varies monotonically across the Antarctic Circumpolar Current (ACC). In the absence of observed maps of eddying currents in the interior ocean, the advecting velocity field is taken from an eddy-permitting state estimate of the Southern Ocean (SOSE). A threedimensional advection\textendash diffusion equation is solved and the diffusivity diagnosed by applying the Nakamura technique on both horizontal and isopycnal surfaces. The resulting meridional sections of Keff reveal intensified isopycnal eddy stirring (reaching values of ;2000 m2 s21) in a layer deep beneath the ACC but rising toward the surface on the equatorward flank. Lower effective diffusivity values (;500 m2 s21) are found near the surface where the mean flow of the ACC is strongest. It is argued that Keff is enhanced in the vicinity of the steering level of baroclinic waves, which is deep along the axis of the ACC but shallows on the equatorial flank. Values of Keff are also found to be spatially correlated with gradients of potential vorticity on isopycnal surfaces and are large where those gradients are weak and vice versa, as expected from simple dynamical arguments. Finally, implications of the spatial distributions of Keff for the dynamics of the ACC and its overturning circulation are discussed.},
  language = {en}
}

@article{Ackmann2021,
  title = {Mixed-Precision for {{Linear Solvers}} in {{Global Geophysical Flows}}},
  author = {Ackmann, Jan and D{\"u}ben, Peter D. and Palmer, Tim N. and Smolarkiewicz, Piotr K.},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.16120 [physics]},
  eprint = {2103.16120},
  eprinttype = {arxiv},
  primaryclass = {physics},
  abstract = {Semi-implicit time-stepping schemes for atmosphere and ocean models require elliptic solvers that work efficiently on modern supercomputers. This paper reports our study of the potential computational savings when using mixed precision arithmetic in the elliptic solvers. The essential components of a representative elliptic solver are run at precision levels as low as half (16 bits), and accompanied with a detailed evaluation of the impact of reduced precision on the solver convergence and the solution quality.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {Computer Science - Computational Engineering; Finance; and Science,Mathematics - Numerical Analysis,Physics - Atmospheric and Oceanic Physics,Physics - Computational Physics,Physics - Fluid Dynamics},
  file = {/Users/milan/Zotero/storage/Q9VYWIJJ/Ackmann et al. - 2021 - Mixed-precision for Linear Solvers in Global Geoph.pdf}
}

@article{Aiki,
  title = {A Seamlessly Diagnosable Expression for the Energy Flux of All Waves at All Latitudes with Equatorial and Coastal Waveguides},
  author = {Aiki, Hidenori and Greatbatch, Richard J and Claus, Martin},
  pages = {31},
  abstract = {For mid-latitude Rossby waves (RWs) in the atmosphere, the expression for the energy flux for use in a model diagnosis, and without relying on a Fourier analysis or a ray theory, has previously been derived using quasi-geostrophic equations and is singular at the equator. By investigating the analytical solution of equatorial waves, the authors derive a universal expression for the energy flux which is able to indicate the direction of the group velocity for all linear waves at all latitudes. This is achieved by introducing a streamfunction as given by the inversion equation of Ertel's potential vorticity, a new and novel aspect when considering the energy flux. Thus the universal expression for the group-velocity-based energy flux is seamlessly diagnosable from model output at all latitudes with an arbitrary shape of coastlines. The utility of the universal expression is illustrated for a forced/dissipative equatorial basin mode simulated by a single-layer oceanic model that includes both mid-latitude RWs and equatorial waves. Equatorial Kelvin Waves (KWs) propagate eastward along the equator, are partially redirected poleward at the eastern boundary of the basin as coastal KWs, followed by the shedding of mid-latitude RWs that propagate westward into the basin interior. The connection of the equatorial and coastal waveguides has been successfully illustrated by the universal expression of the group-velocity-based energy flux of the present study, which will allow for tropical-extratropical interactions in oceanic and atmospheric model outputs to be diagnosed in terms of an energy cycle in a future study.},
  language = {en}
}

@inbook{Aiki2016,
  title = {{{ENERGETICS OF THE GLOBAL OCEAN}}: {{THE ROLE OF MESOSCALE EDDIES}}},
  shorttitle = {{{ENERGETICS OF THE GLOBAL OCEAN}}},
  booktitle = {World {{Scientific Series}} on {{Asia}}-{{Pacific Weather}} and {{Climate}}},
  author = {Aiki, Hidenori and Zhai, Xiaoming and Greatbatch, Richard J.},
  year = {2016},
  month = feb,
  volume = {7},
  pages = {109--134},
  publisher = {{WORLD SCIENTIFIC}},
  doi = {10.1142/9789814696623_0004},
  collaborator = {Behera, Swadhin Kumar and Yamagata, Toshio},
  isbn = {978-981-4696-61-6 978-981-4696-62-3},
  language = {en}
}

@article{Alted2010,
  title = {Why {{Modern CPUs Are Starving}} and {{What Can Be Done}} about {{It}}},
  author = {Alted, Francesc},
  year = {2010},
  month = mar,
  journal = {Computing in Science Engineering},
  volume = {12},
  number = {2},
  pages = {68--71},
  issn = {1558-366X},
  doi = {10.1109/MCSE.2010.51},
  abstract = {CPUs spend most of their time waiting for data to arrive. Identifying low-level bottlenecks-and how to ameliorate them-can save hours of frustration over poor performance in apparently well-written programs.},
  keywords = {Bandwidth,Central Processing Unit,Clocks,Costs,CPU,Delay,Frequency,I/O bottleneck,Impedance,input-output programs,keywords: CPU performance,low-level bottlenecks,memory I/O,memory performance,memory speed,Multicore processing,multlicore computing,performance bottleneck,processor speed,Random access memory,Read-write memory,software performance evaluation,storage management,well-written programs},
  file = {/Users/milan/Zotero/storage/K46ZEM6G/Alted - 2010 - Why Modern CPUs Are Starving and What Can Be Done .pdf;/Users/milan/Zotero/storage/YZRL9PFD/5432301.html}
}

@article{Andrejczuk2016,
  title = {Oceanic {{Stochastic Parameterizations}} in a {{Seasonal Forecast System}}},
  author = {Andrejczuk, M. and Cooper, F. C. and Juricke, S. and Palmer, T. N. and Weisheimer, A. and Zanna, L.},
  year = {2016},
  month = may,
  journal = {Monthly Weather Review},
  volume = {144},
  number = {5},
  pages = {1867--1875},
  issn = {0027-0644, 1520-0493},
  doi = {10.1175/MWR-D-15-0245.1},
  abstract = {Stochastic parameterization provides a methodology for representing model uncertainty in ensemble forecasts. Here the impact on forecast reliability over seasonal time scales of three existing stochastic parameterizations in the ocean component of a coupled model is studied. The relative impacts of these schemes upon the ocean mean state and ensemble spread are analyzed. The oceanic variability induced by the atmospheric forcing of the coupled system is, in most regions, the major source of ensemble spread. The largest impact on spread and bias came from the stochastically perturbed parameterization tendency (SPPT) scheme, which has proven particularly effective in the atmosphere. The key regions affected are eddy-active regions, namely, the western boundary currents and the Southern Ocean where ensemble spread is increased. However, unlike its impact in the atmosphere, SPPT in the ocean did not result in a significant decrease in forecast error on seasonal time scales. While there are good grounds for implementing stochastic schemes in ocean models, the results suggest that they will have to be more sophisticated. Some suggestions for next-generation stochastic schemes are made.},
  language = {en}
}

@article{Anstey2017,
  title = {A Deformation-Based Parametrization of Ocean Mesoscale Eddy Reynolds Stresses},
  author = {Anstey, James A. and Zanna, Laure},
  year = {2017},
  month = apr,
  journal = {Ocean Modelling},
  volume = {112},
  pages = {99--111},
  issn = {14635003},
  doi = {10.1016/j.ocemod.2017.02.004},
  abstract = {Ocean mesoscale eddies strongly affect the strength and variability of large-scale ocean jets such as the Gulf Stream and Kuroshio Extension. Their spatial scales are too small to be fully resolved in many current climate models and hence their effects on the large-scale circulation need to be parametrized. Here we propose a parametrization of mesoscale eddy momentum fluxes based on large-scale flow deformation. The parametrization is argued to be suitable for use in eddy-permitting ocean general circulation models, and is motivated by an analogy between turbulence in Newtonian fluids (such as water) and laminar flow in non-Newtonian fluids. A primitive-equations model in an idealised double-gyre configuration at eddy-resolving horizontal resolution is used to diagnose the relationship between the proposed closure and the eddy fluxes resolved by the model. Favourable correlations suggest the closure could provide an appropriate deterministic parametrization of mesoscale eddies. The relationship between the closure and different representations of the Reynolds stress tensor is also described. The parametrized forcing possesses the key quasi-geostrophic turbulence properties of energy conservation and enstrophy dissipation, and allows for upgradient fluxes leading to the sharpening of vorticity gradients. The implementation of the closure for eddy-permitting ocean models requires only velocity derivatives and a single parameter that scales with model resolution.},
  language = {en}
}

@incollection{Arakawa1977,
  title = {Computational {{Design}} of the {{Basic Dynamical Processes}} of the {{UCLA General Circulation Model}}},
  booktitle = {Methods in {{Computational Physics}}: {{Advances}} in {{Research}} and {{Applications}}},
  author = {Arakawa, AKIO and Lamb, VIVIAN R.},
  editor = {Chang, JULIUS},
  year = {1977},
  month = jan,
  series = {General {{Circulation Models}} of the {{Atmosphere}}},
  volume = {17},
  pages = {173--265},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-0-12-460817-7.50009-4},
  language = {en},
  file = {/Users/milan/Zotero/storage/CER48SV4/B9780124608177500094.html}
}

@article{Arakawa1990,
  title = {Energy {{Conserving}} and {{Potential}}-{{Enstrophy Dissipating Schemes}} for the {{Shallow Water Equations}}},
  author = {Arakawa, Akio and Hsu, Yueh-Jiuan G.},
  year = {1990},
  month = oct,
  journal = {Monthly Weather Review},
  volume = {118},
  number = {10},
  pages = {1960--1969},
  publisher = {{American Meteorological Society}},
  issn = {0027-0644},
  doi = {10.1175/1520-0493(1990)118<1960:ECAPED>2.0.CO;2},
  abstract = {To incorporate potential enstrophy dissipation into discrete shallow water equations with no or arbitrarily small energy dissipation, a family of finite-difference schemes have been derived with which potential enstrophy is guaranteed to decrease while energy is conserved (when the mass flux is nondivergent and time is continuous). Among this family of schemes, there is a member that minimizes the spurious impact of infinite potential vorticities associated with infinitesimal fluid depth. The scheme is, therefore, useful for problems in which the free surface may intersect with the lower boundary.},
  file = {/Users/milan/Zotero/storage/UWI4RRFV/Arakawa and Hsu - 1990 - Energy Conserving and Potential-Enstrophy Dissipat.pdf;/Users/milan/Zotero/storage/8VBFKY27/1520-0493(1990)1181960ECAPED2.0.html}
}

@book{Arakawa2012,
  title = {General {{Circulation Models}} of the {{Atmosphere}}},
  author = {Arakawa, Akio and Lamb, Vivian R.},
  year = {2012},
  month = dec,
  publisher = {{Elsevier}},
  abstract = {Methods in Computational Physics, Volume 17: General Circulation Models of the Atmosphere is a five-chapter text that covers the fundamentals and application of general circulation models to solving practical problems related to the atmosphere. The first chapter describes the various options in modeling physical processes and computational procedures. The next two chapters illustrate the influence of practical considerations to the compromise between a detailed physical description and reasonable computing time. Other chapters outline the computational details of two different numerical schemes for general circulation models. These chapters particularly provide an in-depth analysis of finite difference methods by proceeding from general considerations of homogeneous incompressible flow to the fine details of the particular numerical scheme. The final chapter discusses the fundamentals of the alternative spectral method for a multilevel spectral model that illustrates the capability of that approach. This book is of value to geoscientists, mathematicians, and physicists.},
  googlebooks = {nN\_4561KTIIC},
  isbn = {978-0-323-15482-6},
  language = {en},
  keywords = {Science / Physics / Mathematical \& Computational}
}

@article{Arbic2007,
  title = {Cascade {{Inequalities}} for {{Forced}}\textendash{{Dissipated Geostrophic Turbulence}}},
  author = {Arbic, Brian K. and Flierl, Glenn R. and Scott, Robert B.},
  year = {2007},
  month = jun,
  journal = {Journal of Physical Oceanography},
  volume = {37},
  number = {6},
  pages = {1470--1487},
  issn = {0022-3670, 1520-0485},
  doi = {10.1175/JPO3067.1},
  abstract = {Analysis of spectral kinetic energy fluxes in satellite altimetry data has demonstrated that an inverse cascade of kinetic energy is ubiquitous in the ocean. In geostrophic turbulence models, a fully developed inverse cascade results in barotropic eddies with large horizontal scales. However, midocean eddies contain substantial energy in the baroclinic mode and in compact horizontal scales (scales comparable to the deformation radius Ld). This paper examines the possibility that relatively strong bottom friction prevents the oceanic cascade from becoming fully developed. The importance of the vertical structure of friction is demonstrated by contrasting numerical simulations of two-layer quasigeostrophic turbulence forced by a baroclinically unstable mean flow and damped by bottom Ekman friction with turbulence damped by vertically symmetric Ekman friction (equal decay rates in the two layers). ``Cascade inequalities'' derived from the energy and enstrophy equations are used to interpret the numerical results. In the symmetric system, the inequality formally requires a cascade to large-scale barotropic flow, independent of the stratification. The inequality is less strict when friction is in the bottom layer only, especially when stratification is surface intensified. Accordingly, model runs with surface-intensified stratification and relatively strong bottom friction retain substantial small-scale baroclinic energy. Altimetric data show that the symmetric inequality is violated in the low- and midlatitude ocean, again suggesting the potential impact of the ``bottomness'' of friction on eddies. Inequalities developed for multilayer turbulence suggest that high baroclinic modes in the mean shear also enhance small-scale baroclinic eddy energy. The inequalities motivate a new interpretation of barotropization in weakly damped turbulence. In that limit the barotropic mode dominates the spatial average of kinetic energy density because large values of barotropic density are found throughout the model domain, consistent with the barotropic cascade to large horizontal scales, while baroclinic density is spatially localized.},
  language = {en}
}

@article{Arbic2008,
  title = {On {{Quadratic Bottom Drag}}, {{Geostrophic Turbulence}}, and {{Oceanic Mesoscale Eddies}}},
  author = {Arbic, Brian K. and Scott, Robert B.},
  year = {2008},
  month = jan,
  journal = {Journal of Physical Oceanography},
  volume = {38},
  number = {1},
  pages = {84--103},
  issn = {0022-3670, 1520-0485},
  doi = {10.1175/2007JPO3653.1},
  abstract = {Many investigators have idealized the oceanic mesoscale eddy field with numerical simulations of geostrophic turbulence forced by a horizontally homogeneous, baroclinically unstable mean flow. To date such studies have employed linear bottom Ekman friction (hereinafter, linear drag). This paper presents simulations of two-layer baroclinically unstable geostrophic turbulence damped by quadratic bottom drag, which is generally thought to be more realistic. The goals of the paper are 1) to describe the behavior of quadratically damped turbulence as drag strength changes, using previously reported behaviors of linearly damped turbulence as a point of comparison, and 2) to compare the eddy energies, baroclinicities, and horizontal scales in both quadratic and linear drag simulations with observations and to discuss the constraints these comparisons place on the form and strength of bottom drag in the ocean. In both quadratic and linear drag simulations, large barotropic eddies develop with weak damping, large equivalent barotropic eddies develop with strong damping, and the comparison in goal 2 above is closest when the nondimensional friction strength parameter is of order 1. Typical values of the quadratic drag coefficient (cd ϳ 0.0025) and of boundary layer depths (Hb ϳ 50 m) imply that the quadratic friction strength parameter cdLd /Hb , where Ld is the deformation radius, may indeed be of order 1 in the ocean. Model eddies are realistic over a wider range of friction strengths when drag is quadratic, because of a reduced sensitivity to friction strength in that case. The quadratic parameter is independent of the mean shear, in contrast to the linear parameter. Plots of eddy length scales, computed from satellite altimeter data, versus mean shear and versus rough estimates of the friction strength parameters suggest that both linear and quadratic bottom drag may be active in the ocean. Topographic wave drag contains terms that are linear in the bottom flow, thus providing some justification for the use of linear bottom drag in models.},
  language = {en}
}

@inproceedings{Baker2014,
  title = {A Methodology for Evaluating the Impact of Data Compression on Climate Simulation Data},
  booktitle = {Proceedings of the 23rd International Symposium on {{High}}-Performance Parallel and Distributed Computing},
  author = {Baker, Allison H. and Xu, Haiying and Dennis, John M. and Levy, Michael N. and Nychka, Doug and Mickelson, Sheri A. and Edwards, Jim and Vertenstein, Mariana and Wegener, Al},
  year = {2014},
  month = jun,
  series = {{{HPDC}} '14},
  pages = {203--214},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2600212.2600217},
  abstract = {High-resolution climate simulations require tremendous computing resources and can generate massive datasets. At present, preserving the data from these simulations consumes vast storage resources at institutions such as the National Center for Atmospheric Research (NCAR). The historical data generation trends are economically unsustainable, and storage resources are already beginning to limit science objectives. To mitigate this problem, we investigate the use of data compression techniques on climate simulation data from the Community Earth System Model. Ultimately, to convince climate scientists to compress their simulation data, we must be able to demonstrate that the reconstructed data reveals the same mean climate as the original data, and this paper is a first step toward that goal. To that end, we develop an approach for verifying the climate data and use it to evaluate several compression algorithms. We find that the diversity of the climate data requires the individual treatment of variables, and, in doing so, the reconstructed data can fall within the natural variability of the system, while achieving compression rates of up to 5:1.},
  isbn = {978-1-4503-2749-7},
  keywords = {data compression,high performance computing}
}

@article{Baker2016,
  title = {Evaluating Lossy Data Compression on Climate Simulation Data within a Large Ensemble},
  author = {Baker, Allison H. and Hammerling, Dorit M. and Mickelson, Sheri A. and Xu, Haiying and Stolpe, Martin B. and Naveau, Phillipe and Sanderson, Ben and {Ebert-Uphoff}, Imme and Samarasinghe, Savini and De Simone, Francesco and Carbone, Francesco and Gencarelli, Christian N. and Dennis, John M. and Kay, Jennifer E. and Lindstrom, Peter},
  year = {2016},
  month = dec,
  journal = {Geoscientific Model Development},
  volume = {9},
  number = {12},
  pages = {4381--4403},
  publisher = {{Copernicus GmbH}},
  issn = {1991-959X},
  doi = {10.5194/gmd-9-4381-2016},
  abstract = {{$<$}p{$><$}strong{$>$}Abstract.{$<$}/strong{$>$} High-resolution Earth system model simulations generate enormous data volumes, and retaining the data from these simulations often strains institutional storage resources. Further, these exceedingly large storage requirements negatively impact science objectives, for example, by forcing reductions in data output frequency, simulation length, or ensemble size. To lessen data volumes from the Community Earth System Model (CESM), we advocate the use of lossy data compression techniques. While lossy data compression does not exactly preserve the original data (as lossless compression does), lossy techniques have an advantage in terms of smaller storage requirements. To preserve the integrity of the scientific simulation data, the effects of lossy data compression on the original data should, at a minimum, not be statistically distinguishable from the natural variability of the climate system, and previous preliminary work with data from CESM has shown this goal to be attainable. However, to ultimately convince climate scientists that it is acceptable to use lossy data compression, we provide climate scientists with access to publicly available climate data that have undergone lossy data compression. In particular, we report on the results of a lossy data compression experiment with output from the CESM Large Ensemble (CESM-LE) Community Project, in which we challenge climate scientists to examine features of the data relevant to their interests, and attempt to identify which of the ensemble members have been compressed and reconstructed. We find that while detecting distinguishing features is certainly possible, the compression effects noticeable in these features are often unimportant or disappear in post-processing analyses. In addition, we perform several analyses that directly compare the original data to the reconstructed data to investigate the preservation, or lack thereof, of specific features critical to climate science. Overall, we conclude that applying lossy data compression to climate simulation data is both advantageous in terms of data reduction and generally acceptable in terms of effects on scientific results.{$<$}/p{$>$}},
  language = {English},
  file = {/Users/milan/Zotero/storage/KUYTQS49/Baker et al. - 2016 - Evaluating lossy data compression on climate simul.pdf;/Users/milan/Zotero/storage/7ZYFTML6/2016.html}
}

@article{Baker2019,
  title = {Evaluating Image Quality Measures to Assess the Impact of Lossy Data Compression Applied to Climate Simulation Data},
  author = {Baker, A. H. and Hammerling, D. M. and Turton, T. L.},
  year = {2019},
  journal = {Computer Graphics Forum},
  volume = {38},
  number = {3},
  pages = {517--528},
  issn = {1467-8659},
  doi = {10.1111/cgf.13707},
  abstract = {Applying lossy data compression to climate model output is an attractive means of reducing the enormous volumes of data generated by climate models. However, because lossy data compression does not exactly preserve the original data, its application to scientific data must be done judiciously. To this end, a collection of measures is being developed to evaluate various aspects of lossy compression quality on climate model output. Given the importance of data visualization to climate scientists interacting with model output, any suite of measures must include a means of assessing whether images generated from the compressed model data are noticeably different from images based on the original model data. Therefore, in this work we conduct a forced-choice visual evaluation study with climate model data that surveyed more than one hundred participants with domain relevant expertise. In addition to the images created from unaltered climate model data, study images are generated from model data that is subjected to two different types of lossy compression approaches and multiple levels (amounts) of compression. Study participants indicate whether a visual difference can be seen, with respect to the reference image, due to lossy compression effects. We assess the relationship between the perceptual scores from the user study to a number of common (full reference) image quality assessment (IQA) measures, and use statistical models to suggest appropriate measures and thresholds for evaluating lossily compressed climate data. We find the structural similarity index (SSIM) to perform the best, and our findings indicate that the threshold required for climate model data is much higher than previous findings in the literature.},
  copyright = {\textcopyright{} 2019 The Author(s) Computer Graphics Forum \textcopyright{} 2019 The Eurographics Association and John Wiley \& Sons Ltd. Published by John Wiley \& Sons Ltd.},
  language = {en},
  keywords = {Categories and Subject Descriptors (according to ACM CCS),E.4 Coding and Information Theory: Data Compaction and Compression—,H.1.2 User/Machine Systems: Human factors—,I.5.2 Design Methodology: Feature evaluation—},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.13707},
  file = {/Users/milan/Zotero/storage/57769EH9/Baker et al. - 2019 - Evaluating image quality measures to assess the im.pdf;/Users/milan/Zotero/storage/J29U9NF2/cgf.html}
}

@article{Balaji2017,
  title = {{{CPMIP}}: Measurements of Real Computational Performance of {{Earth}} System Models in {{CMIP6}}},
  shorttitle = {{{CPMIP}}},
  author = {Balaji, Venkatramani and Maisonnave, Eric and Zadeh, Niki and Lawrence, Bryan N. and Biercamp, Joachim and Fladrich, Uwe and Aloisio, Giovanni and Benson, Rusty and Caubel, Arnaud and Durachta, Jeffrey and Foujols, Marie-Alice and Lister, Grenville and Mocavero, Silvia and Underwood, Seth and Wright, Garrett},
  year = {2017},
  month = jan,
  journal = {Geoscientific Model Development},
  volume = {10},
  number = {1},
  pages = {19--34},
  publisher = {{Copernicus GmbH}},
  issn = {1991-959X},
  doi = {10.5194/gmd-10-19-2017},
  abstract = {{$<$}p{$><$}strong class="journal-contentHeaderColor"{$>$}Abstract.{$<$}/strong{$>$} A climate model represents a multitude of processes on a variety of timescales and space scales: a canonical example of multi-physics multi-scale modeling. The underlying climate system is physically characterized by sensitive dependence on initial conditions, and natural stochastic variability, so very long integrations are needed to extract signals of climate change. Algorithms generally possess weak scaling and can be I/O and/or memory-bound. Such weak-scaling, I/O, and memory-bound multi-physics codes present particular challenges to computational performance. {$<$}br{$><$}br{$>$} Traditional metrics of computational efficiency such as performance counters and scaling curves do not tell us enough about real sustained performance from climate models on different machines. They also do not provide a satisfactory basis for comparative information across models. codes present particular challenges to computational performance. {$<$}br{$><$}br{$>$} We introduce a set of metrics that can be used for the study of computational performance of climate (and Earth system) models. These measures do not require specialized software or specific hardware counters, and should be accessible to anyone. They are independent of platform and underlying parallel programming models. We show how these metrics can be used to measure actually attained performance of Earth system models on different machines, and identify the most fruitful areas of research and development for performance engineering. codes present particular challenges to computational performance. {$<$}br{$><$}br{$>$} We present results for these measures for a diverse suite of models from several modeling centers, and propose to use these measures as a basis for a CPMIP, a computational performance model intercomparison project (MIP).{$<$}/p{$>$}},
  language = {English},
  file = {/Users/milan/Zotero/storage/K3QQJT4S/Balaji et al. - 2017 - CPMIP measurements of real computational performa.pdf;/Users/milan/Zotero/storage/QQTDIEIC/2017.html}
}

@article{Balaji2018,
  title = {Requirements for a Global Data Infrastructure in Support of {{CMIP6}}},
  author = {Balaji, Venkatramani and Taylor, Karl E. and Juckes, Martin and Lawrence, Bryan N. and Durack, Paul J. and Lautenschlager, Michael and Blanton, Chris and Cinquini, Luca and Denvil, S{\'e}bastien and Elkington, Mark and Guglielmo, Francesca and Guilyardi, Eric and Hassell, David and Kharin, Slava and Kindermann, Stefan and Nikonov, Sergey and Radhakrishnan, Aparna and Stockhause, Martina and Weigel, Tobias and Williams, Dean},
  year = {2018},
  month = sep,
  journal = {Geoscientific Model Development},
  volume = {11},
  number = {9},
  pages = {3659--3680},
  publisher = {{Copernicus GmbH}},
  issn = {1991-959X},
  doi = {10.5194/gmd-11-3659-2018},
  abstract = {{$<$}p{$><$}strong{$>$}Abstract.{$<$}/strong{$>$} The World Climate Research Programme (WCRP)'s Working Group on Climate Modelling (WGCM) Infrastructure Panel (WIP) was formed in 2014 in response to the explosive growth in size and complexity of Coupled Model Intercomparison Projects (CMIPs) between CMIP3 (2005\textendash 2006) and CMIP5 (2011\textendash 2012). This article presents the WIP recommendations for the global data infrastructure needed to support CMIP design, future growth, and evolution. Developed in close coordination with those who build and run the existing infrastructure (the Earth System Grid Federation; ESGF), the recommendations are based on several principles beginning with the need to separate requirements, implementation, and operations. Other important principles include the consideration of the diversity of community needs around data \textendash{} a data ecosystem \textendash{} the importance of provenance, the need for automation, and the obligation to measure costs and benefits.{$<$}/p{$><$}p{$>$}This paper concentrates on requirements, recognizing the diversity of communities involved (modelers, analysts, software developers, and downstream users). Such requirements include the need for scientific reproducibility and accountability alongside the need to record and track data usage. One key element is to generate a dataset-centric rather than system-centric focus, with an aim to making the infrastructure less prone to systemic failure.{$<$}/p{$><$}p{$>$}With these overarching principles and requirements, the WIP has produced a set of position papers, which are summarized in the latter pages of this document. They provide specifications for managing and delivering model output, including strategies for replication and versioning, licensing, data quality assurance, citation, long-term archiving, and dataset tracking. They also describe a new and more formal approach for specifying what data, and associated metadata, should be saved, which enables future data volumes to be estimated, particularly for well-defined projects such as CMIP6.{$<$}/p{$><$}p{$>$}The paper concludes with a future facing consideration of the global data infrastructure evolution that follows from the blurring of boundaries between climate and weather, and the changing nature of published scientific results in the digital age.{$<$}/p{$>$}},
  language = {English},
  file = {/Users/milan/Zotero/storage/HGIQ6G8K/Balaji et al. - 2018 - Requirements for a global data infrastructure in s.pdf;/Users/milan/Zotero/storage/YVNMHL6R/gmd-11-3659-2018-discussion.html}
}

@article{Ballester-Ripoll2020,
  title = {{{TTHRESH}}: {{Tensor Compression}} for {{Multidimensional Visual Data}}},
  shorttitle = {{{TTHRESH}}},
  author = {{Ballester-Ripoll}, Rafael and Lindstrom, Peter and Pajarola, Renato},
  year = {2020},
  month = sep,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {26},
  number = {9},
  eprint = {1806.05952},
  eprinttype = {arxiv},
  pages = {2891--2903},
  issn = {1077-2626, 1941-0506, 2160-9306},
  doi = {10.1109/TVCG.2019.2904063},
  abstract = {Memory and network bandwidth are decisive bottlenecks when handling high-resolution multidimensional data sets in visualization applications, and they increasingly demand suitable data compression strategies. We introduce a novel lossy compression algorithm for multidimensional data over regular grids. It leverages the higher-order singular value decomposition (HOSVD), a generalization of the SVD to three dimensions and higher, together with bit-plane, run-length and arithmetic coding to compress the HOSVD transform coefficients. Our scheme degrades the data particularly smoothly and achieves lower mean squared error than other state-of-the-art algorithms at low-to-medium bit rates, as it is required in data archiving and management for visualization purposes. Further advantages of the proposed algorithm include very fine bit rate selection granularity and the ability to manipulate data at very small cost in the compression domain, for example to reconstruct filtered and/or subsampled versions of all (or selected parts) of the data set.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {Computer Science - Graphics},
  file = {/Users/milan/Zotero/storage/M6A765CN/Ballester-Ripoll et al. - 2020 - TTHRESH Tensor Compression for Multidimensional V.pdf}
}

@article{Bauer2015,
  title = {The Quiet Revolution of Numerical Weather Prediction},
  author = {Bauer, Peter and Thorpe, Alan and Brunet, Gilbert},
  year = {2015},
  month = sep,
  journal = {Nature},
  volume = {525},
  number = {7567},
  pages = {47--55},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature14956},
  abstract = {Advances in numerical weather prediction represent a quiet revolution because they have resulted from a steady accumulation of scientific knowledge and technological advances over many years that, with only a few exceptions, have not been associated with the aura of fundamental physics breakthroughs. Nonetheless, the impact of numerical weather prediction is among the greatest of any area of physical science. As a computational problem, global weather prediction is comparable to the simulation of the human brain and of the evolution of the early Universe, and it is performed every day at major operational centres across the world.},
  copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  language = {en},
  file = {/Users/milan/Zotero/storage/5JVZCCV6/nature14956.html}
}

@article{Bauer2015a,
  title = {The Quiet Revolution of Numerical Weather Prediction},
  author = {Bauer, Peter and Thorpe, Alan and Brunet, Gilbert},
  year = {2015},
  month = sep,
  journal = {Nature},
  volume = {525},
  number = {7567},
  pages = {47--55},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14956},
  language = {en}
}

@misc{Bauer2020,
  title = {The {{ECMWF Scalability Programme}}: {{Progress}} and {{Plans}}},
  shorttitle = {The {{ECMWF Scalability Programme}}},
  author = {Bauer, Peter and Quintino, Tiago and Wedi, Nils and Bonanni, Antonino and Chrust, Marcin and Deconinck, Willem and Diamantakis, Michail and D{\"u}ben, Peter and English, Stephen and Flemming, Johannes and Gillies, Paddy and Hadade, Ioan and Hawkes, James and Hawkins, Mike and Iffrig, Olivier and K{\"u}hnlein, Christian and Lange, Michael and Lean, Peter and Marsden, Olivier and M{\"u}ller, Andreas and Saarinen, Sami and Sarmany, Domokos and Sleigh, Michael and Smart, Simon and Smolarkiewicz, Piotr and Thiemert, Daniel and Tumolo, Giovanni and Weihrauch, Christian and Zanna, Cristiano},
  year = {2020},
  doi = {10.21957/gdit22ulm},
  file = {/Users/milan/Zotero/storage/5MBXULVY/19380-ecmwf-scalability-programme-progress-and-plans.html}
}

@article{Bauer2021,
  title = {A Digital Twin of {{Earth}} for the Green Transition},
  author = {Bauer, Peter and Stevens, Bjorn and Hazeleger, Wilco},
  year = {2021},
  month = feb,
  journal = {Nature Climate Change},
  volume = {11},
  number = {2},
  pages = {80--83},
  publisher = {{Nature Publishing Group}},
  issn = {1758-6798},
  doi = {10.1038/s41558-021-00986-y},
  abstract = {For its green transition, the EU plans to fund the development of digital twins of Earth. For these twins to be more than big data atlases, they must create a qualitatively new Earth system simulation and observation capability using a methodological framework responsible for exceptional advances in numerical weather prediction.},
  copyright = {2021 Springer Nature Limited},
  language = {en},
  file = {/Users/milan/Zotero/storage/PI44ATUL/Bauer et al. - 2021 - A digital twin of Earth for the green transition.pdf;/Users/milan/Zotero/storage/ZSANQD78/s41558-021-00986-y.html}
}

@article{Bauer2021a,
  title = {The Digital Revolution of {{Earth}}-System Science},
  author = {Bauer, Peter and Dueben, Peter D. and Hoefler, Torsten and Quintino, Tiago and Schulthess, Thomas C. and Wedi, Nils P.},
  year = {2021},
  month = feb,
  journal = {Nature Computational Science},
  volume = {1},
  number = {2},
  pages = {104--113},
  publisher = {{Nature Publishing Group}},
  issn = {2662-8457},
  doi = {10.1038/s43588-021-00023-0},
  abstract = {Computational science is crucial for delivering reliable weather and climate predictions. However, despite decades of high-performance computing experience, there is serious concern about the sustainability of this application in the post-Moore/Dennard era. Here, we discuss the present limitations in the field and propose the design of a novel infrastructure that is scalable and more adaptable to future, yet unknown computing architectures.},
  copyright = {2021 Springer Nature America, Inc.},
  language = {en},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Reviews Subject\_term: Climate sciences;Computational science Subject\_term\_id: climate-sciences;computational-science},
  file = {/Users/milan/Zotero/storage/VF7NL2S5/Bauer et al. - 2021 - The digital revolution of Earth-system science.pdf;/Users/milan/Zotero/storage/A3DNI8VA/s43588-021-00023-0.html}
}

@article{Bayr2014,
  title = {The Eastward Shift of the {{Walker Circulation}} in Response to Global Warming and Its Relationship to {{ENSO}} Variability},
  author = {Bayr, Tobias and Dommenget, Dietmar and Martin, Thomas and Power, Scott B.},
  year = {2014},
  month = nov,
  journal = {Climate Dynamics},
  volume = {43},
  number = {9-10},
  pages = {2747--2763},
  issn = {0930-7575, 1432-0894},
  doi = {10.1007/s00382-014-2091-y},
  language = {en}
}

@article{Bengtsson2011,
  title = {On the Evaluation of Temperature Trends in the Tropical Troposphere},
  author = {Bengtsson, Lennart and Hodges, Kevin I.},
  year = {2011},
  month = feb,
  journal = {Climate Dynamics},
  volume = {36},
  number = {3-4},
  pages = {419--430},
  issn = {0930-7575, 1432-0894},
  doi = {10.1007/s00382-009-0680-y},
  language = {en}
}

@article{Berloff2005,
  title = {Random-Forcing Model of the Mesoscale Oceanic Eddies},
  author = {Berloff, Pavel S.},
  year = {2005},
  month = apr,
  journal = {Journal of Fluid Mechanics},
  volume = {529},
  pages = {71--95},
  issn = {0022-1120, 1469-7645},
  doi = {10.1017/S0022112005003393},
  language = {en}
}

@article{Berloff2018,
  title = {Dynamically Consistent Parameterization of Mesoscale Eddies. {{Part III}}: {{Deterministic}} Approach},
  shorttitle = {Dynamically Consistent Parameterization of Mesoscale Eddies. {{Part III}}},
  author = {Berloff, Pavel},
  year = {2018},
  month = jul,
  journal = {Ocean Modelling},
  volume = {127},
  pages = {1--15},
  issn = {14635003},
  doi = {10.1016/j.ocemod.2018.04.009},
  abstract = {This work continues development of dynamically consistent parameterizations for representing mesoscale eddy effects in non-eddy-resolving and eddy-permitting ocean circulation models and focuses on the classical doublegyre problem, in which the main dynamic eddy effects maintain eastward jet extension of the western boundary currents and its adjacent recirculation zones via eddy backscatter mechanism. Despite its fundamental importance, this mechanism remains poorly understood, and in this paper we, first, study it and, then, propose and test its novel parameterization.},
  language = {en}
}

@article{Bezanson2017,
  title = {Julia: {{A Fresh Approach}} to {{Numerical Computing}}},
  shorttitle = {Julia},
  author = {Bezanson, Jeff. and Edelman, Alan. and Karpinski, Stefan. and Shah, Viral B.},
  year = {2017},
  month = jan,
  journal = {SIAM Review},
  volume = {59},
  number = {1},
  pages = {65--98},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0036-1445},
  doi = {10.1137/141000671},
  abstract = {Bridging cultures that have often been distant, Julia combines expertise from the diverse fields of computer science and computational science to create a new approach to numerical  computing. Julia is  designed to be easy and fast and questions notions generally held to be ``laws of nature"  by practitioners of numerical computing: \textbackslash beginlist \textbackslash item  High-level dynamic programs have to be slow. \textbackslash item  One must prototype in one language and then rewrite in another language for speed or deployment. \textbackslash item There are parts of a system appropriate for the programmer, and other parts that are best left untouched as they have been built by the experts. \textbackslash endlist We introduce the  Julia programming language and its design---a  dance between specialization and abstraction. Specialization allows for custom treatment. Multiple dispatch,  a  technique from computer science, picks  the right algorithm for the right circumstance. Abstraction, which is what good computation is really about, recognizes what remains the same after differences are stripped away. Abstractions in mathematics are captured as code through another technique from computer science, generic programming. Julia shows that  one can achieve machine performance without sacrificing human convenience.},
  file = {/Users/milan/Zotero/storage/SZM3ENMR/Bezanson et al. - 2017 - Julia A Fresh Approach to Numerical Computing.pdf;/Users/milan/Zotero/storage/UG5NFZ28/141000671.html}
}

@article{Bhowmik2020,
  title = {Powers of 10: Seeking `sweet Spots' for Rapid Climate and Sustainability Actions between Individual and Global Scales},
  shorttitle = {Powers of 10},
  author = {Bhowmik, Avit K. and McCaffrey, Mark S. and Ruskey, Abigail M. and Frischmann, Chad and Gaffney, Owen},
  year = {2020},
  month = aug,
  journal = {Environmental Research Letters},
  volume = {15},
  number = {9},
  pages = {094011},
  publisher = {{IOP Publishing}},
  issn = {1748-9326},
  doi = {10.1088/1748-9326/ab9ed0},
  abstract = {Achieving the goals of the Paris Agreement and related sustainability initiatives will require halving of global greenhouse gas emissions each decade from now on through to 2050, when net zero emissions should be achieved. To reach such significant reductions requires a rapid and strategic scaling of existing and emerging technologies and practices, coupled with economic and social transformations and novel governance solutions. Here we present a new `Powers of 10' (P10) logarithmic framework and demonstrate its potential as a practical tool for decision makers and change agents at multiple scales to inform and catalyze engagement and actions, complementing and adding nuance to existing frameworks. P10 assists in identifying the suitable cohorts and cohort ranges for rapidly deploying climate and sustainability actions between a single individual and the globally projected {$\sim$} 10 billion persons by 2050. Applying a robust dataset of climate solutions from Project Drawdown's Plausible scenario that could cumulatively reduce greenhouse gas emissions by 1051 gigatons (Gt) against a reference scenario (2190 Gt) between 2020 and 2050, we seek to identify a `sweet spot' where these climate and sustainability actions are suitably scaled. We suggest that prioritizing the analyzed climate actions between community and urban scales, where global and local converge, can help catalyze and enhance individual, household and local practices, and support national and international policies and finances for rapid sustainability transformations.},
  language = {en},
  file = {/Users/milan/Zotero/storage/CPHBDXUS/Bhowmik et al. - 2020 - Powers of 10 seeking `sweet spots' for rapid clim.pdf}
}

@article{Bier2017,
  title = {Synoptic {{Control}} of {{Contrail Cirrus Life Cycles}} and {{Their Modification Due}} to {{Reduced Soot Number Emissions}}: {{Contrail Cirrus Life Cycles}}},
  shorttitle = {Synoptic {{Control}} of {{Contrail Cirrus Life Cycles}} and {{Their Modification Due}} to {{Reduced Soot Number Emissions}}},
  author = {Bier, A. and Burkhardt, U. and Bock, L.},
  year = {2017},
  month = nov,
  journal = {Journal of Geophysical Research: Atmospheres},
  volume = {122},
  number = {21},
  pages = {11,584--11,603},
  issn = {2169897X},
  doi = {10.1002/2017JD027011},
  abstract = {The atmospheric state, aircraft emissions, and engine properties determine formation and initial properties of contrails. The synoptic situation controls microphysical and dynamical processes and causes a wide variability of contrail cirrus life cycles. A reduction of soot particle number emissions, resulting, for example, from the use of alternative fuels, strongly impacts initial ice crystal numbers and microphysical process rates of contrail cirrus. We use the European Centre/Hamburg (ECHAM) climate model version 5 including a contrail cirrus modul, studying process rates, properties, and life cycles of contrail cirrus clusters within different synoptic situations. The impact of reduced soot number emissions is approximated by a reduction in the initial ice crystal number, exemplarily studied for 80\%. Contrail cirrus microphysical and macrophysical properties can depend much more strongly on the synoptic situation than on the initial ice crystal number. They can attain a large cover, optical depth, and ice water content in long-lived and large-scale ice-supersaturated areas, making them particularly climate-relevant. In those synoptic situations, the accumulated ice crystal loss due to sedimentation is increased by around 15\% and the volume of contrail cirrus, exceeding an optical depth of 0.02, and their short-wave radiative impact are strongly decreased due to reduced soot emissions. These reductions are of little consequence in short-lived and small-scale ice-supersaturated areas, where contrail cirrus stay optically very thin and attain a low cover. The synoptic situations in which long-lived and climate-relevant contrail cirrus clusters can be found over the eastern U.S. occur in around 25\% of cases.},
  language = {en},
  file = {/Users/milan/Zotero/storage/YFPWJY6P/Bier et al. - 2017 - Synoptic Control of Contrail Cirrus Life Cycles an.pdf}
}

@article{Bier2019,
  title = {Variability in {{Contrail Ice Nucleation}} and {{Its Dependence}} on {{Soot Number Emissions}}},
  author = {Bier, A. and Burkhardt, U.},
  year = {2019},
  journal = {Journal of Geophysical Research: Atmospheres},
  volume = {124},
  number = {6},
  pages = {3384--3400},
  issn = {2169-8996},
  doi = {10.1029/2018JD029155},
  abstract = {Contrail ice nucleation is mainly controlled by aircraft emissions and the atmospheric state. The nucleation rate can have a strong impact on microphysical processes, optical properties, lifetime, and, therefore, on the climate impact of contrail cirrus. We study contrail ice crystal formation offline for specified atmospheric conditions and its spatial variability in a global climate model. Assuming the standard atmosphere, above around 270 hPa (10 km) contrail ice nucleation is mainly controlled by aircraft soot number emissions and below additionally by atmospheric temperature. Parameterizing contrail ice nucleation in a global climate model, we find that in the northern extratropics contrails form frequently far away from their formation threshold. For current soot number emissions and in case of contrail formation, 90\% of emitted soot particles form on average ice crystals around the cruise level and more than 70\% between cruise altitudes and 300 hPa. The number of nucleated ice crystals in the extratropics decreases nearly at the same rate as soot number emissions. In contrast, in the tropics around cruise altitudes approximately 60\% of contrails develop close to their formation threshold so that on average only about 50\% of emitted soot particles can form ice crystals. Below, contrail formation occurs rarely and ice nucleation is reduced more strongly. Of the main air traffic areas, contrail ice nucleation is significantly limited by the atmospheric state over eastern Asia and over the southeastern United States. This limitation is enhanced during the summer months.},
  copyright = {\textcopyright 2019. American Geophysical Union. All Rights Reserved.},
  language = {en},
  keywords = {alternative fuels,atmospheric state,contrails,ice nucleation,soot emissions},
  annotation = {\_eprint: https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2018JD029155},
  file = {/Users/milan/Zotero/storage/K7C37E6A/Bier and Burkhardt - 2019 - Variability in Contrail Ice Nucleation and Its Dep.pdf;/Users/milan/Zotero/storage/KZ52S7NA/2018JD029155.html}
}

@article{Bigg2003,
  title = {The Role of the Oceans in Climate},
  author = {Bigg, G. R. and Jickells, T. D. and Liss, P. S. and Osborn, T. J.},
  year = {2003},
  month = aug,
  journal = {International Journal of Climatology},
  volume = {23},
  number = {10},
  pages = {1127--1159},
  issn = {0899-8418, 1097-0088},
  doi = {10.1002/joc.926},
  abstract = {The ocean is increasingly seen as a vital component of the climate system. It exchanges with the atmosphere large quantities of heat, water, gases, particles and momentum. It is an important part of the global redistribution of heat from tropics to polar regions keeping our planet habitable, particularly equatorward of about 30\textdegree. In this article we review recent work examining the role of the oceans in climate, focusing on research in the Third Assessment Report of the IPCC and later. We discuss the general nature of oceanic climate variability and the large role played by stochastic variability in the interaction of the atmosphere and ocean. We consider the growing evidence for biogeochemical interaction of climatic significance between ocean and atmosphere. Air\textendash sea exchange of several radiatively important gases, in particular CO2, is a major mechanism for altering their atmospheric concentrations. Some more reactive gases, such as dimethyl sulphide, can alter cloud formation and hence albedo. Particulates containing iron and originating over land can alter ocean primary productivity and hence feedbacks to other biogeochemical exchanges. We show that not only the tropical Pacific Ocean basin can exhibit coupled ocean\textendash atmosphere interaction, but also the tropical Atlantic and Indian Oceans. Longer lived interactions in the North Pacific and Southern Ocean (the circumpolar wave) are also reviewed. The role of the thermohaline circulation in long-term and abrupt climatic change is examined, with the freshwater budget of the ocean being a key factor for the degree, and longevity, of change. The potential for the Mediterranean outflow to contribute to abrupt change is raised. We end by examining the probability of thermohaline changes in a future of global warming. Copyright  2003 Royal Meteorological Society.},
  language = {en}
}

@article{Black2007,
  title = {An 8-Century Tropical {{Atlantic SST}} Record from the {{Cariaco Basin}}: {{Baseline}} Variability, Twentieth-Century Warming, and {{Atlantic}} Hurricane Frequency: {{CARIACO SSTS LAST}} 800 {{A}}},
  shorttitle = {An 8-Century Tropical {{Atlantic SST}} Record from the {{Cariaco Basin}}},
  author = {Black, David E. and Abahazi, Matthew A. and Thunell, Robert C. and Kaplan, Alexey and Tappa, Eric J. and Peterson, Larry C.},
  year = {2007},
  month = dec,
  journal = {Paleoceanography},
  volume = {22},
  number = {4},
  pages = {n/a-n/a},
  issn = {08838305},
  doi = {10.1029/2007PA001427},
  language = {en}
}

@article{Blackman2019,
  title = {Scrambled {{Linear Pseudorandom Number Generators}}},
  author = {Blackman, David and Vigna, Sebastiano},
  year = {2019},
  month = aug,
  journal = {arXiv:1805.01407 [cs]},
  eprint = {1805.01407},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Linear pseudorandom number generators are very popular due to their high speed, to the ease with which generators with a sizable state space can be created, and to their provable theoretical properties. However, they suffer from linear artifacts which show as failures in linearity-related statistical tests such as the binary-rank and the linear-complexity test. In this paper, we give three new contributions. First, we introduce two new linear transformations that have been handcrafted to have good statistical properties and at the same time to be programmable very efficiently on superscalar processors, or even directly in hardware. Then, we describe a new test for Hamming-weight dependencies that is able to discover subtle, previously unknown biases in existing generators (in particular, in linear ones). Finally, we describe a number of scramblers, that is, nonlinear functions applied to the state array that reduce or delete the linear artifacts, and propose combinations of linear transformations and scramblers that give extremely fast pseudorandom generators of high quality. A novelty in our approach is that we use ideas from the theory of filtered linear-feedback shift register to prove some properties of our scramblers, rather than relying purely on heuristics. In the end, we provide simple, extremely fast generators that use a few hundred bits of memory, have provable properties and pass very strong statistical tests.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Data Structures and Algorithms,Computer Science - Mathematical Software},
  file = {/Users/milan/Zotero/storage/346WDCF9/Blackman and Vigna - 2019 - Scrambled Linear Pseudorandom Number Generators.pdf;/Users/milan/Zotero/storage/545JPVUX/1805.html}
}

@article{Boghosian2019,
  title = {A {{New Pathology}} in the {{Simulation}} of {{Chaotic Dynamical Systems}} on {{Digital Computers}}},
  author = {Boghosian, Bruce M. and Coveney, Peter V. and Wang, Hongyan},
  year = {2019},
  journal = {Advanced Theory and Simulations},
  volume = {2},
  number = {12},
  pages = {1900125},
  issn = {2513-0390},
  doi = {10.1002/adts.201900125},
  abstract = {Systematic distortions are uncovered in the statistical properties of chaotic dynamical systems when represented and simulated on digital computers using standard IEEE floating-point numbers. This is done by studying a model chaotic dynamical system with a single free parameter {$\beta$}, known as the generalized Bernoulli map, many of whose exact properties are known. Much of the structure of the dynamical system is lost in the floating-point representation. For even integer values of the parameter, the long time behaviour is completely wrong, subsuming the known anomalous behaviour for {$\beta$} = 2. For non-integer {$\beta$}, relative errors in observables can reach 14\%. For odd integer values of {$\beta$}, floating-point results are more accurate, but still produce relative errors two orders of magnitude larger than those attributable to roundoff. The analysis indicates that the pathology described, which cannot be mitigated by increasing the precision of the floating point numbers, is a representative example of a deeper problem in the computation of expectation values for chaotic systems. The findings sound a warning about the uncritical application of numerical methods in studies of the statistical properties of chaotic dynamical systems, such as are routinely performed throughout computational science, including turbulence and molecular dynamics.},
  copyright = {\textcopyright{} 2019 The Authors. Published by WILEY-VCH Verlag GmbH \& Co. KGaA, Weinheim},
  language = {en},
  keywords = {Bernoulli shift,chaos,dynamical systems,floating point arithmetic,pathology},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/adts.201900125},
  file = {/Users/milan/Zotero/storage/388H5PUK/Boghosian et al. - 2019 - A New Pathology in the Simulation of Chaotic Dynam.pdf;/Users/milan/Zotero/storage/5NFTMNWT/adts.html}
}

@article{Bourke1974,
  title = {A {{Multi}}-{{Level Spectral Model}}. {{I}}. {{Formulation}} and {{Hemispheric Integrations}}},
  author = {Bourke, William},
  year = {1974},
  month = oct,
  journal = {Monthly Weather Review},
  volume = {102},
  number = {10},
  pages = {687--701},
  publisher = {{American Meteorological Society}},
  issn = {0027-0644},
  doi = {10.1175/1520-0493(1974)102<0687:AMLSMI>2.0.CO;2},
  abstract = {The formulation of a multi-level spectral model suitable for simulation of atmospheric flow on a hemispheric or global scale is presented. The derived primitive equations are employed together with spectral-grid transform procedures in the multi-level domain. An efficient semi-implicit time integration scheme is detailed and results of numerical integrations initialized from analytic fields and Southern Hemisphere data sets are presented. A simple initializing device of divergence dissipation is suggested and shown to be most effective in eliminating spurious large-scale inertia-gravity oscillations.},
  file = {/Users/milan/Zotero/storage/G62PUXHW/Bourke - 1974 - A Multi-Level Spectral Model. I. Formulation and H.pdf;/Users/milan/Zotero/storage/BNPFTR4M/1520-0493(1974)1020687AMLSMI2.0.html}
}

@article{Brasseur2016,
  title = {Impact of {{Aviation}} on {{Climate}}: {{FAA}}'s {{Aviation Climate Change Research Initiative}} ({{ACCRI}}) {{Phase II}}},
  shorttitle = {Impact of {{Aviation}} on {{Climate}}},
  author = {Brasseur, Guy P. and Gupta, Mohan and Anderson, Bruce E. and Balasubramanian, Sathya and Barrett, Steven and Duda, David and Fleming, Gregg and Forster, Piers M. and Fuglestvedt, Jan and Gettelman, Andrew and Halthore, Rangasayi N. and Jacob, S. Daniel and Jacobson, Mark Z. and Khodayari, Arezoo and Liou, Kuo-Nan and Lund, Marianne T. and {Miake-Lye}, Richard C. and Minnis, Patrick and Olsen, Seth and Penner, Joyce E. and Prinn, Ronald and Schumann, Ulrich and Selkirk, Henry B. and Sokolov, Andrei and Unger, Nadine and Wolfe, Philip and Wong, Hsi-Wu and Wuebbles, Donald W. and Yi, Bingqi and Yang, Ping and Zhou, Cheng},
  year = {2016},
  month = apr,
  journal = {Bulletin of the American Meteorological Society},
  volume = {97},
  number = {4},
  pages = {561--583},
  publisher = {{American Meteorological Society}},
  issn = {0003-0007, 1520-0477},
  doi = {10.1175/BAMS-D-13-00089.1},
  abstract = {{$<$}section class="abstract"{$><$}h2 class="abstractTitle text-title my-1" id="d1115e2"{$>$}Abstract{$<$}/h2{$><$}p{$>$}Under the Federal Aviation Administration's (FAA) Aviation Climate Change Research Initiative (ACCRI), non-CO\textsubscript{2} climatic impacts of commercial aviation are assessed for current (2006) and for future (2050) baseline and mitigation scenarios. The effects of the non-CO\textsubscript{2} aircraft emissions are examined using a number of advanced climate and atmospheric chemistry transport models. Radiative forcing (RF) estimates for individual forcing effects are provided as a range for comparison against those published in the literature. Preliminary results for selected RF components for 2050 scenarios indicate that a 2\% increase in fuel efficiency and a decrease in NO\textsubscript{x} emissions due to advanced aircraft technologies and operational procedures, as well as the introduction of renewable alternative fuels, will significantly decrease future aviation climate impacts. In particular, the use of renewable fuels will further decrease RF associated with sulfate aerosol and black carbon. While this focused ACCRI program effort has yielded significant new knowledge, fundamental uncertainties remain in our understanding of aviation climate impacts. These include several chemical and physical processes associated with NO\textsubscript{x}\textendash O\textsubscript{3}\textendash CH\textsubscript{4} interactions and the formation of aviation-produced contrails and the effects of aviation soot aerosols on cirrus clouds as well as on deriving a measure of change in temperature from RF for aviation non-CO\textsubscript{2} climate impacts\textemdash an important metric that informs decision-making.{$<$}/p{$><$}/section{$>$}},
  chapter = {Bulletin of the American Meteorological Society},
  language = {EN},
  file = {/Users/milan/Zotero/storage/M587IGM5/Brasseur et al. - 2016 - Impact of Aviation on Climate FAA’s Aviation Clim.pdf;/Users/milan/Zotero/storage/Q47NTKE2/bams-d-13-00089.1.html}
}

@article{Bruggemann2014,
  title = {Evaluating {{Different Parameterizations}} for {{Mixed Layer Eddy Fluxes}} Induced by {{Baroclinic Instability}}},
  author = {Br{\"u}ggemann, Nils and Eden, Carsten},
  year = {2014},
  month = sep,
  journal = {Journal of Physical Oceanography},
  volume = {44},
  number = {9},
  pages = {2524--2546},
  issn = {0022-3670, 1520-0485},
  doi = {10.1175/JPO-D-13-0235.1},
  abstract = {In this study, the authors discuss two different parameterizations for the effect of mixed layer eddies, one based on ageostrophic linear stability analysis (ALS) and the other one based on a scaling of the potential energy release by eddies (PER). Both parameterizations contradict each other in two aspects. First, they predict different functional relationships between the magnitude of the eddy fluxes and the Richardson number (Ri) related to the background state. Second, they also predict different vertical structure functions for the horizontal eddy fluxes. Numerical simulations for two different configurations and for a large range of different background conditions are used to evaluate the parameterizations. It turns out that PER is better suited to capture the Ri dependency of the magnitude of the eddy fluxes. On the other hand, the vertical structure of the meridional eddy fluxes predicted by ALS is more accurate than that of PER, while the vertical structure of the vertical eddy fluxes is well predicted by both parameterizations. Therefore, this study suggests the use of the magnitude of PER and the vertical structure functions of ALS for an improved parameterization of mixed layer eddy fluxes.},
  language = {en}
}

@article{Bruggemann2015,
  title = {Routes to {{Dissipation}} under {{Different Dynamical Conditions}}},
  author = {Br{\"u}ggemann, Nils and Eden, Carsten},
  year = {2015},
  month = aug,
  journal = {Journal of Physical Oceanography},
  volume = {45},
  number = {8},
  pages = {2149--2168},
  issn = {0022-3670, 1520-0485},
  doi = {10.1175/JPO-D-14-0205.1},
  abstract = {In this study, it is investigated how ageostrophic dynamics generate an energy flux toward smaller scales. Numerical simulations of baroclinic instability are used with varying dynamical conditions ranging from quasigeostrophic balance to ageostrophic flows. It turns out that dissipation at smaller scales by viscous friction is much more efficient if the flow is dominated by ageostrophic dynamics than in quasigeostrophic conditions. In the presence of ageostrophic dynamics, an energy flux toward smaller scales is observed while energy is transferred toward larger scales for quasigeostrophic dynamics. Decomposing the velocity field into its rotational and divergent components shows that only the divergent velocity component, which becomes stronger for ageostrophic flows, features a downscale flux. Variation of the dynamical conditions from ageostrophic dynamics to quasigeostrophic balanced flows shows that the forward energy flux and therefore the small-scale dissipation decreases as soon as the horizontal divergent velocity component decreases. A functional relationship between the small-scale dissipation and the local Richardson number is estimated. This functional relationship is used to obtain a global estimate of the small-scale dissipation of 0.31 6 0.23 TW from a high-resolution realistic global ocean model. This emphasizes that an ageostrophic direct route to dissipation might be of importance in the ocean energy cycle.},
  language = {en}
}

@inproceedings{Burgess2019,
  title = {Bfloat16 {{Processing}} for {{Neural Networks}}},
  booktitle = {2019 {{IEEE}} 26th {{Symposium}} on {{Computer Arithmetic}} ({{ARITH}})},
  author = {Burgess, Neil and Milanovic, Jelena and Stephens, Nigel and Monachopoulos, Konstantinos and Mansell, David},
  year = {2019},
  month = jun,
  pages = {88--91},
  issn = {1063-6889},
  doi = {10.1109/ARITH.2019.00022},
  abstract = {Bfloat16 ("BF16") is a new floating-point format tailored specifically for high-performance processing of Neural Networks and will be supported by major CPU and GPU architectures as well as Neural Network accelerators. This paper proposes a possible implementation of a BF16 multiply-accumulation operation that relaxes several IEEE Floating-Point Standard features to afford low-cost hardware implementations. Specifically, subnorms are flushed to zero; only one non-standard rounding mode (Round-Odd) is supported; NaNs are not propagated; and IEEE exception flags are not provided. The paper shows that this approach achieves the same network-level accuracy as using IEEE single-precision arithmetic ("FP32") for less than half the datapath area cost and with greater throughput.},
  keywords = {Artificial neural networks,BF16 multiply-accumulation operation,bfloat16 processing,Computer architecture,CPU architectures,Digital arithmetic,Error analysis,floating point arithmetic,floating-point; rounding mode; neural networks,FP32,GPU architectures,high-performance processing,IEEE exception flags,IEEE floating-point standard features,IEEE single-precision arithmetic,low-cost hardware implementations,network-level accuracy,neural nets,Neural Network accelerators,neural networks,nonstandard rounding mode,Standards,Training},
  file = {/Users/milan/Zotero/storage/XMKM6HAG/Burgess et al. - 2019 - Bfloat16 Processing for Neural Networks.pdf;/Users/milan/Zotero/storage/VC5Y2QAC/8877390.html}
}

@article{Burkhardt2018,
  title = {Mitigating the Contrail Cirrus Climate Impact by Reducing Aircraft Soot Number Emissions},
  author = {Burkhardt, Ulrike and Bock, Lisa and Bier, Andreas},
  year = {2018},
  month = oct,
  journal = {npj Climate and Atmospheric Science},
  volume = {1},
  number = {1},
  pages = {1--7},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3722},
  doi = {10.1038/s41612-018-0046-4},
  abstract = {Contrail cirrus are a major component of the climate forcing due to air traffic. For a given contrail cirrus cover, ice water content and ice crystal shape, their impact on radiation is dependent on the number and size of ice crystals. Here we use a global climate model to study the impact of a reduction in initially formed ice crystal numbers, as may be caused by reduced soot number emissions. We find that for reduced initial ice crystal numbers the ice water content is decreased and ice crystal sizes increased, leading to a reduction in contrail cirrus optical depth and doubling the fraction of contrail cirrus that cannot be detected by satellite remote sensing. Contrail cirrus lifetimes and coverage are strongly reduced leading to significant reductions in contrail cirrus radiative forcing. The global climate impact of contrail cirrus is nonlinearly dependent on the reduction in initial ice crystal numbers. A reduction in the initial ice crystal number of 80\% leads to a decrease in contrail cirrus radiative forcing by 50\%, whereas a twofold reduction leads to a decrease in radiative forcing by approximately 20\%. Only a few contrail cirrus outbreaks explain a large percentage of the climate impact. The contrail cirrus climate impact can be effectively mitigated by reducing initial ice crystal concentrations in such outbreak situations. Our results are important for assessments dealing with mitigating the climate impact of aviation and discussions about the use of alternative fuels or lean combustion in aviation.},
  copyright = {2018 The Author(s)},
  language = {en},
  file = {/Users/milan/Zotero/storage/TBUJBABA/Burkhardt et al. - 2018 - Mitigating the contrail cirrus climate impact by r.pdf;/Users/milan/Zotero/storage/LPXRBNER/s41612-018-0046-4.html}
}

@article{Burtscher2020,
  title = {The Carbon Footprint of Large Astronomy Meetings},
  author = {Burtscher, Leonard and Barret, Didier and Borkar, Abhijeet P. and Grinberg, Victoria and Jahnke, Knud and Kendrew, Sarah and Maffey, Gina and McCaughrean, Mark J.},
  year = {2020},
  month = sep,
  journal = {Nature Astronomy},
  volume = {4},
  number = {9},
  pages = {823--825},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3366},
  doi = {10.1038/s41550-020-1207-z},
  abstract = {The annual meeting of the European Astronomical Society took place in Lyon, France, in 2019, but in 2020 it was held online only due the COVID-19 pandemic. The carbon footprint of the virtual meeting was roughly 3,000 times smaller than the face-to-face one, providing encouragement for more ecologically minded conferencing.},
  copyright = {2020 Springer Nature Limited},
  language = {en},
  file = {/Users/milan/Zotero/storage/M7VAARSI/Burtscher et al. - 2020 - The carbon footprint of large astronomy meetings.pdf;/Users/milan/Zotero/storage/DJARMAQC/s41550-020-1207-z.html}
}

@article{Butcher,
  title = {Numerical {{Methods}} for {{Ordinary Differential Equations}}},
  author = {Butcher, John C},
  pages = {486}
}

@incollection{Butcher2008,
  title = {Runge\textendash{{Kutta Methods}}},
  booktitle = {Numerical {{Methods}} for {{Ordinary Differential Equations}}},
  author = {Butcher, J. C.},
  year = {2008},
  pages = {137--316},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9780470753767.ch3},
  abstract = {This chapter contains sections titled: Preliminaries Order Conditions Low Order Explicit Methods Runge\textendash Kutta Methods with Error Estimates Implicit Runge\textendash Kutta Methods Stability of Implicit Runge\textendash Kutta Methods Implementable Implicit Runge\textendash Kutta Methods Symplectic Runge\textendash Kutta Methods Algebraic Properties of Runge\textendash Kutta Methods Implementation Issues},
  chapter = {3},
  isbn = {978-0-470-75376-7},
  language = {en},
  keywords = {‘Radau I quadrature formula’,‘summation convention’,pohutukawa (Metrosideros excelsa) or ‘New Zealand Christmas tree’,rooted tree and non-rooted trees,Runge–Kutta methods,Taylor series of exact solution},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9780470753767.ch3},
  file = {/Users/milan/Zotero/storage/L7MSI4KG/9780470753767.html}
}

@book{Butcher2016,
  title = {Numerical {{Methods}} for {{Ordinary Differential Equations}}},
  author = {Butcher, J. C.},
  year = {2016},
  month = aug,
  edition = {3 edition},
  publisher = {{Wiley}},
  address = {{Chichester, West Sussex, United Kingdom}},
  abstract = {A new edition of this classic work, comprehensively revised to present exciting new developments in this important subject The study of numerical methods for solving ordinary differential equations is constantly developing and regenerating, and this third edition of a popular classic volume, written by one of the world's leading experts in the field, presents an account of the subject which reflects both its historical and well-established place in computational science and its vital role as a cornerstone of modern applied mathematics. In addition to serving as a broad and comprehensive study of numerical methods for initial value problems, this book contains a special emphasis on Runge-Kutta methods by the mathematician who transformed the subject into its modern form dating from his classic 1963 and 1972 papers.~ A second feature is general linear methods which have now matured and grown from being a framework for a unified theory of a wide range of diverse numerical schemes to a source of new and practical algorithms in their own right.~ As the founder of general linear method research, John Butcher has been a leading contributor to its development; his special role is reflected in the text.~ The book is written in the lucid style characteristic of the author, and combines enlightening explanations with rigorous and precise analysis. In addition to these anticipated features, the book breaks new ground by including the latest results on the highly efficient G-symplectic methods which compete strongly with the well-known symplectic Runge-Kutta methods for long-term integration of conservative mechanical systems. This third edition of Numerical Methods for Ordinary Differential Equations will serve as a key text for senior undergraduate and graduate courses in numerical analysis, and is an essential resource for research workers in applied mathematics, physics and engineering.},
  isbn = {978-1-119-12150-3},
  language = {English}
}

@article{Cain2019,
  title = {Improved Calculation of Warming-Equivalent Emissions for Short-Lived Climate Pollutants},
  author = {Cain, Michelle and Lynch, John and Allen, Myles R. and Fuglestvedt, Jan S. and Frame, David J. and Macey, Adrian H.},
  year = {2019},
  month = sep,
  journal = {npj Climate and Atmospheric Science},
  volume = {2},
  number = {1},
  pages = {1--7},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3722},
  doi = {10.1038/s41612-019-0086-4},
  abstract = {Anthropogenic global warming at a given time is largely determined by the cumulative total emissions (or stock) of long-lived climate pollutants (LLCPs), predominantly carbon dioxide (CO2), and the emission rates (or flow) of short-lived climate pollutants (SLCPs) immediately prior to that time. Under the United Nations Framework Convention on Climate Change (UNFCCC), reporting of greenhouse gas emissions has been standardised in terms of CO2-equivalent (CO2-e) emissions using Global Warming Potentials (GWP) over 100-years, but the conventional usage of GWP does not adequately capture the different behaviours of LLCPs and SLCPs, or their impact on global mean surface temperature. An alternative usage of GWP, denoted GWP*, overcomes this problem by equating an increase in the emission rate of an SLCP with a one-off ``pulse'' emission of CO2. We show that this approach, while an improvement on the conventional usage, slightly underestimates the impact of recent increases in SLCP emissions on current rates of warming because the climate does not respond instantaneously to radiative forcing. We resolve this with a modification of the GWP* definition, which incorporates a term for each of the short-timescale and long-timescale climate responses to changes in radiative forcing. The amended version allows ``CO2-warming-equivalent'' (CO2-we) emissions to be calculated directly from reported emissions. Thus SLCPs can be incorporated directly into carbon budgets consistent with long-term temperature goals, because every unit of CO2-we emitted generates approximately the same amount of warming, whether it is emitted as a SLCP or a LLCP. This is not the case for conventionally derived CO2-e.},
  copyright = {2019 The Author(s)},
  language = {en},
  file = {/Users/milan/Zotero/storage/ZZEET5V3/Cain et al. - 2019 - Improved calculation of warming-equivalent emissio.pdf;/Users/milan/Zotero/storage/BHP3F8G8/s41612-019-0086-4.html}
}

@article{Cane1997,
  title = {Twentieth-{{Century Sea Surface Temperature Trends}}},
  author = {Cane, Mark A. and Clement, Amy C. and Kaplan, Alexey and Kushnir, Yochanan and Pozdnyakov, Dmitri and Seager, Richard and Zebiak, Stephen E. and Murtugudde, Ragu},
  year = {1997},
  month = feb,
  journal = {Science},
  volume = {275},
  number = {5302},
  pages = {957--960},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.275.5302.957},
  language = {en}
}

@article{Capet2008,
  title = {Mesoscale to {{Submesoscale Transition}} in the {{California Current System}}. {{Part II}}: {{Frontal Processes}}},
  shorttitle = {Mesoscale to {{Submesoscale Transition}} in the {{California Current System}}. {{Part II}}},
  author = {Capet, X. and McWilliams, J. C. and Molemaker, M. J. and Shchepetkin, A. F.},
  year = {2008},
  month = jan,
  journal = {Journal of Physical Oceanography},
  volume = {38},
  number = {1},
  pages = {44--64},
  issn = {0022-3670, 1520-0485},
  doi = {10.1175/2007JPO3672.1},
  abstract = {This is the second of three papers investigating the regime transition that occurs in numerical simulations for an idealized, equilibrium, subtropical, eastern boundary, upwelling current system similar to the California Current. The emergent upper-ocean submesoscale fronts are analyzed from phenomenological and dynamical perspectives, using a combination of composite averaging and separation of distinctive subregions of the flow. The initiating dynamical process for the transition is near-surface frontogenesis. The frontal behavior is similar to both observed meteorological surface fronts and solutions of the approximate dynamical model called surface dynamics (i.e., uniform interior potential vorticity q and diagnostic force balance) in the intensification of surface density gradients and secondary circulations in response to a mesoscale strain field. However, there are significant behavioral differences compared to the surfacedynamics model. Wind stress acts on fronts through nonlinear Ekman transport and creation and destruction of potential vorticity. The strain-induced frontogenesis is disrupted by vigorous submesoscale frontal instabilities that in turn lead to secondary frontogenesis events, submesoscale vortices, and excitation of even smaller-scale flows. Intermittent, submesoscale breakdown of geostrophic and gradient-wind force balance occurs during the intense frontogenesis and frontal-instability events.},
  language = {en}
}

@article{Capet2008a,
  title = {Mesoscale to {{Submesoscale Transition}} in the {{California Current System}}. {{Part III}}: {{Energy Balance}} and {{Flux}}},
  shorttitle = {Mesoscale to {{Submesoscale Transition}} in the {{California Current System}}. {{Part III}}},
  author = {Capet, X. and McWilliams, J. C. and Molemaker, M. J. and Shchepetkin, A. F.},
  year = {2008},
  month = oct,
  journal = {Journal of Physical Oceanography},
  volume = {38},
  number = {10},
  pages = {2256--2269},
  issn = {0022-3670, 1520-0485},
  doi = {10.1175/2008JPO3810.1},
  abstract = {This is the last of a suite of three papers about the transition that occurs in numerical simulations for an idealized equilibrium, subtropical, eastern-boundary upwelling current system similar to the California Current. The transition is mainly explained by the emergence of ubiquitous submesoscale density fronts and ageostrophic circulations about them in the weakly stratified surface boundary layer. Here the highresolution simulations are further analyzed from the perspective of the kinetic energy (KE) spectrum shape and spectral energy fluxes in the mesoscale-to-submesoscale range in the upper ocean. For wavenumbers greater than the mesoscale energy peak, there is a submesoscale power-law regime in the spectrum with an exponent close to Ϫ2. In the KE balance an important conversion from potential to kinetic energy takes place at all wavenumbers in both mesoscale and submesoscale ranges; this conversion is the energetic counterpart of the vertical restratification flux and frontogenesis discussed in the earlier papers. A significant forward cascade of KE occurs in the submesoscale range en route to dissipation at even smaller scales. This is contrary to the inverse energy cascade of geostrophic turbulence and it is, in fact, fundamentally associated with the horizontally divergent (i.e., ageostrophic) velocity component. The submesoscale dynamical processes of frontogenesis, frontal instability, and breakdown of diagnostic force balance are all essential elements of the energy cycle of potential energy conversion and forward KE cascade.},
  language = {en}
}

@article{Capet2008b,
  title = {Mesoscale to {{Submesoscale Transition}} in the {{California Current System}}. {{Part I}}: {{Flow Structure}}, {{Eddy Flux}}, and {{Observational Tests}}},
  shorttitle = {Mesoscale to {{Submesoscale Transition}} in the {{California Current System}}. {{Part I}}},
  author = {Capet, X. and McWilliams, J. C. and Molemaker, M. J. and Shchepetkin, A. F.},
  year = {2008},
  month = jan,
  journal = {Journal of Physical Oceanography},
  volume = {38},
  number = {1},
  pages = {29--43},
  issn = {0022-3670, 1520-0485},
  doi = {10.1175/2007JPO3671.1},
  abstract = {In computational simulations of an idealized subtropical eastern boundary upwelling current system, similar to the California Current, a submesoscale transition occurs in the eddy variability as the horizontal grid scale is reduced to O(1) km. This first paper (in a series of three) describes the transition in terms of the emergent flow structure and the associated time-averaged eddy fluxes. In addition to the mesoscale eddies that arise from a primary instability of the alongshore, wind-driven currents, significant energy is transferred into submesoscale fronts and vortices in the upper ocean. The submesoscale arises through surface frontogenesis growing off upwelled cold filaments that are pulled offshore and strained in between the mesoscale eddy centers. In turn, some submesoscale fronts become unstable and develop submesoscale meanders and fragment into roll-up vortices. Associated with this phenomenon are a large vertical vorticity and Rossby number, a large vertical velocity, relatively flat horizontal spectra (contrary to the prevailing view of mesoscale dynamics), a large vertical buoyancy flux acting to restratify the upper ocean, a submesoscale energy conversion from potential to kinetic, a significant spatial and temporal intermittency in the upper ocean, and material exchanges between the surface boundary layer and pycnocline. Comparison with available observations indicates that submesoscale fronts and instabilities occur widely in the upper ocean, with characteristics similar to the simulations.},
  language = {en}
}

@article{Casey2001,
  title = {Global and {{Regional Sea Surface Temperature Trends}}},
  author = {Casey, Kenneth S and Cornillon, Peter},
  year = {2001},
  journal = {JOURNAL OF CLIMATE},
  volume = {14},
  pages = {18},
  abstract = {Individual sea surface temperature (SST) anomalies are calculated using a satellite-based climatology and observations from the World Ocean Atlas 1994 (WOA94) and the Comprehensive Ocean\textendash Atmosphere Data Set (COADS) to characterize global and regional changes in ocean surface temperature since 1942. For each of these datasets, anomaly trends are computed using a new method that groups individual anomalies into climatological temperature classes. These temperature class anomaly trends are compared with trends estimated using a technique representative of previous studies based on 5\cyrchar\CYRNJE{} latitude\textendash longitude bins.},
  language = {en}
}

@article{Chang2008,
  title = {Oceanic Link between Abrupt Changes in the {{North Atlantic Ocean}} and the {{African}} Monsoon},
  author = {Chang, Ping and Zhang, Rong and Hazeleger, Wilco and Wen, Caihong and Wan, Xiuquan and Ji, Link and Haarsma, Reindert J. and Breugem, Wim-Paul and Seidel, Howard},
  year = {2008},
  month = jul,
  journal = {Nature Geoscience},
  volume = {1},
  number = {7},
  pages = {444--448},
  issn = {1752-0894, 1752-0908},
  doi = {10.1038/ngeo218},
  language = {en}
}

@article{Chantry2019,
  title = {Scale-{{Selective Precision}} for {{Weather}} and {{Climate Forecasting}}},
  author = {Chantry, Matthew and Thornes, Tobias and Palmer, Tim and D{\"u}ben, Peter},
  year = {2019},
  month = feb,
  journal = {Monthly Weather Review},
  volume = {147},
  number = {2},
  pages = {645--655},
  publisher = {{American Meteorological Society}},
  issn = {0027-0644},
  doi = {10.1175/MWR-D-18-0308.1},
  language = {en},
  file = {/Users/milan/Zotero/storage/35TM5U57/Chantry et al. - 2019 - Scale-Selective Precision for Weather and Climate .pdf;/Users/milan/Zotero/storage/BBQ3E5WP/Scale-Selective-Precision-for-Weather-and-Climate.html}
}

@inproceedings{Chaurasiya2018,
  title = {Parameterized {{Posit Arithmetic Hardware Generator}}},
  booktitle = {2018 {{IEEE}} 36th {{International Conference}} on {{Computer Design}} ({{ICCD}})},
  author = {Chaurasiya, Rohit and Gustafson, John and Shrestha, Rahul and Neudorfer, Jonathan and Nambiar, Sangeeth and Niyogi, Kaustav and Merchant, Farhad and Leupers, Rainer},
  year = {2018},
  month = oct,
  pages = {334--341},
  publisher = {{IEEE}},
  address = {{Orlando, FL, USA}},
  doi = {10.1109/ICCD.2018.00057},
  abstract = {Hardware implementation of Floating Point Units (FPUs) has been a key area of research due to their massive area and energy footprints. Recently, a proposal was made to replace IEEE 754-2008 technical standard compliant FPUs with Posit Arithmetic Units (PAUs) due to the greater accuracy, speed, and simpler hardware design. In this paper, we present the architecture of a parameterized PAU generator that can generate PAU adders and PAU multipliers of any bit-width pre-synthesis. We synthesize generated arithmetic units using the parameterized PAU generator for 8-bit, 16-bit, and 32-bit adders and multipliers and compare them with IEEE 754-2008 compliant adders and multipliers. Both, synthesis for Field Programmable Gate Array (FPGA) and Application Specific Integrated Circuit (ASIC) are performed. In our comparison of m-bit PAU units with n-bit IEEE 754-2008 compliant units, it is observed that the area and energy of a PAU adder and multiplier are comparable to their IEEE 754-2008 compliant counterparts where m = n. We argue that an n-bit IEEE 754-2008 adder and multiplier can be safely replaced with an m-bit PAU adder and multiplier where m {$<$} n, due to superior numerical accuracy of the PAU; we also compare m-bit PAU adders and multipliers with n-bit IEEE 754-2008 compliant adders and multipliers. As an application example, we examine performance in the domain of signal processing with and without PAU adders and multipliers, and show the advantage of our approach.},
  isbn = {978-1-5386-8477-1},
  language = {en}
}

@article{Chelton2007,
  title = {Global Observations of Large Oceanic Eddies: {{GLOBAL OBSERVATIONS OF OCEANIC EDDIES}}},
  shorttitle = {Global Observations of Large Oceanic Eddies},
  author = {Chelton, Dudley B. and Schlax, Michael G. and Samelson, Roger M. and {de Szoeke}, Roland A.},
  year = {2007},
  month = aug,
  journal = {Geophysical Research Letters},
  volume = {34},
  number = {15},
  issn = {00948276},
  doi = {10.1029/2007GL030812},
  language = {en}
}

@article{Chelton2011,
  title = {Global Observations of Nonlinear Mesoscale Eddies},
  author = {Chelton, Dudley B. and Schlax, Michael G. and Samelson, Roger M.},
  year = {2011},
  month = oct,
  journal = {Progress in Oceanography},
  volume = {91},
  number = {2},
  pages = {167--216},
  issn = {00796611},
  doi = {10.1016/j.pocean.2011.01.002},
  abstract = {Sixteen years of sea-surface height (SSH) fields constructed by merging the measurements from two simultaneously operating altimeters are analyzed to investigate mesoscale variability in the global ocean. The prevalence of coherent mesoscale features (referred to here as ``eddies'') with radius scales of O(100 km) is readily apparent in these high-resolution SSH fields. An automated procedure for identifying and tracking mesoscale features based on their SSH signatures yields 35,891 eddies with lifetimes P16 weeks. These long-lived eddies, comprising approximately 1.15 million individual eddy observations, have an average lifetime of 32 weeks and an average propagation distance of 550 km. Their mean amplitude and a speed-based radius scale as defined by the automated procedure are 8 cm and 90 km, respectively.},
  language = {en}
}

@article{Chen2013,
  title = {Simulated Radiative Forcing from Contrails and Contrail Cirrus},
  author = {Chen, C.-C. and Gettelman, A.},
  year = {2013},
  month = dec,
  journal = {Atmospheric Chemistry and Physics},
  volume = {13},
  number = {24},
  pages = {12525--12536},
  publisher = {{Copernicus GmbH}},
  issn = {1680-7316},
  doi = {10.5194/acp-13-12525-2013},
  abstract = {{$<$}p{$><$}strong{$>$}Abstract.{$<$}/strong{$>$} A comprehensive general circulation model including ice supersaturation is used to estimate the climate impact of aviation induced contrails. The model uses a realistic aviation emissions inventory for 2006 to initiate contrails, and allows them to evolve consistently with the model hydrologic cycle. {$<$}br{$><$}br{$>$} The radiative forcing from linear contrails is very sensitive to the diurnal cycle. For linear contrails, including the diurnal cycle of air traffic reduces the estimated radiative forcing by 29\%, and for contrail cirrus estimates, the radiative forcing is reduced by 25\%. Estimated global radiative forcing from linear contrails is 0.0031 {$\pm$} 0.0005 Wm\textsuperscript{-2}. The linear contrail radiative forcing is found to exhibit a strong diurnal cycle. The contrail cirrus radiative forcing is less sensitive to the diurnal cycle of flights. The estimated global radiative forcing from contrail cirrus is 0.013 {$\pm$} 0.01 Wm\textsuperscript{-2}. Over regions with the highest air traffic, the regional effect can be as large as 1 Wm\textsuperscript{-2}.{$<$}/p{$>$}},
  language = {English},
  file = {/Users/milan/Zotero/storage/GBZEIGUY/Chen and Gettelman - 2013 - Simulated radiative forcing from contrails and con.pdf;/Users/milan/Zotero/storage/GCRB8CJM/2013.html;/Users/milan/Zotero/storage/SNIKXEK4/2013.html}
}

@inproceedings{Chen2018,
  title = {A Matrix-Multiply Unit for Posits in Reconfigurable Logic Leveraging (Open){{CAPI}}},
  booktitle = {Proceedings of the {{Conference}} for {{Next Generation Arithmetic}} on - {{CoNGA}} '18},
  author = {Chen, Jianyu and {Al-Ars}, Zaid and Hofstee, H. Peter},
  year = {2018},
  pages = {1--5},
  publisher = {{ACM Press}},
  address = {{Singapore, Singapore}},
  doi = {10.1145/3190339.3190340},
  abstract = {In this paper, we present the design in reconfigurable logic of a matrix multiplier for matrices of 32-bit posit numbers with es=2 [1]. Vector dot products are computed without intermediate rounding as suggested by the proposed posit standard to maximally retain precision. An initial implementation targets the CAPI 1.0 interface on the POWER8 processor and achieves about 10Gpops (Giga posit operations per second). Follow-on implementations targeting CAPI 2.0 and OpenCAPI 3.0 on POWER9 are expected to achieve up to 64Gpops. Our design is available under a permissive open source license at https://github.com/ChenJianyunp/Unum\_matrix\_multiplier. We hope the current work, which works on CAPI 1.0, along with future community contributions, will help enable a more extensive exploration of this proposed new format.},
  isbn = {978-1-4503-6414-0},
  language = {en}
}

@article{Choo2003,
  title = {Two's Complement Computation Sharing Multiplier and Its Applications to High Performance {{DFE}}},
  author = {Choo, Hunsoo and Muhammad, K. and Roy, K.},
  year = {2003},
  journal = {IEEE Transactions on Signal Processing},
  volume = {51},
  pages = {458--469},
  issn = {1053-587X},
  abstract = {Two's complement computation sharing multiplier and its applications to high performance DFE},
  language = {en},
  file = {/Users/milan/Zotero/storage/NTB2YSNF/Twos_complement_computation_sharing_multiplier_and_its_applications_to_high_performance_DFE.html}
}

@article{Christensen2017,
  title = {Stochastic {{Parameterization}} and {{El Ni\~no}}\textendash{{Southern Oscillation}}},
  author = {Christensen, H. M. and Berner, Judith and Coleman, Danielle R. B. and Palmer, T. N.},
  year = {2017},
  month = jan,
  journal = {Journal of Climate},
  volume = {30},
  number = {1},
  pages = {17--38},
  publisher = {{American Meteorological Society}},
  issn = {0894-8755, 1520-0442},
  doi = {10.1175/JCLI-D-16-0122.1},
  abstract = {{$<$}section class="abstract"{$><$}h2 class="abstractTitle text-title my-1" id="d1035e2"{$>$}Abstract{$<$}/h2{$><$}p{$>$}El Ni\~no\textendash Southern Oscillation (ENSO) is the dominant mode of interannual variability in the tropical Pacific. However, the models in the ensemble from phase 5 of the Coupled Model Intercomparison Project (CMIP5) have large deficiencies in ENSO amplitude, spatial structure, and temporal variability. The use of stochastic parameterizations as a technique to address these pervasive errors is considered. The multiplicative stochastically perturbed parameterization tendencies (SPPT) scheme is included in coupled integrations of the National Center for Atmospheric Research (NCAR) Community Atmosphere Model, version 4 (CAM4). The SPPT scheme results in a significant improvement to the representation of ENSO in CAM4, improving the power spectrum and reducing the magnitude of ENSO toward that observed. To understand the observed impact, additive and multiplicative noise in a simple delayed oscillator (DO) model of ENSO is considered. Additive noise results in an increase in ENSO amplitude, but multiplicative noise can reduce the magnitude of ENSO, as was observed for SPPT in CAM4. In light of these results, two complementary mechanisms are proposed by which the improvement occurs in CAM. Comparison of the coupled runs with a set of atmosphere-only runs indicates that SPPT first improve the variability in the zonal winds through perturbing the convective heating tendencies, which improves the variability of ENSO. In addition, SPPT improve the distribution of westerly wind bursts (WWBs), important for initiation of El Ni\~no events, by increasing the stochastic component of WWB and reducing the overly strong dependency on SST compared to the control integration.{$<$}/p{$><$}/section{$>$}},
  chapter = {Journal of Climate},
  language = {EN},
  file = {/Users/milan/Zotero/storage/6F3HCQ8P/Christensen et al. - 2017 - Stochastic Parameterization and El Niño–Southern O.pdf}
}

@article{Claus2014,
  title = {Influence of the {{Barotropic Mean Flow}} on the {{Width}} and the {{Structure}} of the {{Atlantic Equatorial Deep Jets}}},
  author = {Claus, Martin and Greatbatch, Richard J. and Brandt, Peter},
  year = {2014},
  month = sep,
  journal = {Journal of Physical Oceanography},
  volume = {44},
  number = {9},
  pages = {2485--2497},
  issn = {0022-3670, 1520-0485},
  doi = {10.1175/JPO-D-14-0056.1},
  abstract = {A representation of an equatorial basin mode excited in a shallow-water model for a single high-order baroclinic vertical normal mode is used as a simple model for the equatorial deep jets. The model is linearized about both a state of rest and a barotropic mean flow corresponding to the observed Atlantic Equatorial Intermediate Current System. It was found that the eastward mean flow associated with the North and South Intermediate Counter Currents (NICC and SICC, respectively) effectively shields the equator from off-equatorial Rossby waves. The westward propagation of these waves is blocked, and focusing on the equator due to beta dispersion is prevented. This leads to less energetic jets along the equator. On the other hand, the westward barotropic mean flow along the equator reduces the gradient of absolute vorticity and hence widens the cross-equatorial structure of the basin mode. Increasing lateral viscosity predominantly affects the width of the basin modes' Kelvin wave component in the presence of the mean flow, while the Rossby wave is confined by the flanking NICC and SICC. Independent of the presence of the mean flow, the application of sufficient lateral mixing also hinders the focusing of off-equatorial Rossby waves, which is hence an unlikely feature of a low-frequency basin mode in the real ocean.},
  language = {en}
}

@article{Clement2015,
  title = {The {{Atlantic Multidecadal Oscillation}} without a Role for Ocean Circulation},
  author = {Clement, A. and Bellomo, K. and Murphy, L. N. and Cane, M. A. and Mauritsen, T. and Radel, G. and Stevens, B.},
  year = {2015},
  month = oct,
  journal = {Science},
  volume = {350},
  number = {6258},
  pages = {320--324},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aab3980},
  language = {en}
}

@misc{Collet,
  title = {Zstandard {{Compression}} and the Application/Zstd {{Media Type}}},
  author = {Collet, Yann and Kucherawy, Murray},
  howpublished = {https://tools.ietf.org/html/rfc8478},
  language = {en},
  file = {/Users/milan/Zotero/storage/ECF5WPZH/rfc8478.html}
}

@misc{Collet2020,
  title = {Facebook/Zstd},
  author = {Collet, Yann},
  year = {2020},
  month = sep,
  abstract = {Zstandard},
  copyright = {View license         ,                 View license},
  howpublished = {Facebook}
}

@article{Cooper2015,
  title = {Optimisation of an Idealised Ocean Model, Stochastic Parameterisation of Sub-Grid Eddies},
  author = {Cooper, Fenwick C. and Zanna, Laure},
  year = {2015},
  month = apr,
  journal = {Ocean Modelling},
  volume = {88},
  pages = {38--53},
  issn = {14635003},
  doi = {10.1016/j.ocemod.2014.12.014},
  abstract = {An optimisation scheme is developed to accurately represent the sub-grid scale forcing of a high dimensional chaotic ocean system. Using a simple parameterisation scheme, the velocity components of a 30 km resolution shallow water ocean model are optimised to have the same climatological mean and variance as that of a less viscous 7.5 km resolution model. The 5 day lag-covariance is also optimised, leading to a more accurate estimate of the high resolution response to forcing using the low resolution model.},
  language = {en}
}

@article{Courant,
  title = {On the {{Partial Difference Equations}} of {{Mathematical Physics}}},
  author = {Courant, R and Friedrichs, K and Lewyt, H},
  pages = {20},
  language = {en}
}

@article{Courant1967,
  title = {On the {{Partial Difference Equations}} of {{Mathematical Physics}}},
  author = {Courant, R. and Friedrichs, K. and Lewy, H.},
  year = {1967},
  month = mar,
  journal = {IBM Journal of Research and Development},
  volume = {11},
  number = {2},
  pages = {215--234},
  issn = {0018-8646},
  doi = {10.1147/rd.112.0215},
  abstract = {Problems involving the classical linear partial differential equations of mathematical physics can be reduced to algebraic ones of a very much simpler structure by replacing the differentials by difference quotients on some (say rectilinear) mesh. This paper will undertake an elementary discussion of these algebraic problems, in particular of the behavior of the solution as the mesh width tends to zero. For present purposes we limit ourselves mainly to simple but typical cases, and treat them in such a way that the applicability of the method to more general difference equations and to those with arbitrarily many independent variables is made clear.},
  file = {/Users/milan/Zotero/storage/DSWKNWQD/5391985.html}
}

@article{Creutzig2015,
  title = {Transport: {{A}} Roadblock to Climate Change Mitigation?},
  shorttitle = {Transport},
  author = {Creutzig, Felix and Jochem, Patrick and Edelenbosch, Oreane Y. and Mattauch, Linus and van Vuuren, Detlef P. and McCollum, David and Minx, Jan},
  year = {2015},
  month = nov,
  journal = {Science},
  volume = {350},
  number = {6263},
  pages = {911--912},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aac8033},
  abstract = {Urban mobility solutions foster climate mitigation Urban mobility solutions foster climate mitigation},
  chapter = {Policy Forum},
  copyright = {Copyright \textcopyright{} 2015, American Association for the Advancement of Science},
  language = {en},
  pmid = {26586747},
  file = {/Users/milan/Zotero/storage/EPLLV3N2/Creutzig et al. - 2015 - Transport A roadblock to climate change mitigatio.pdf;/Users/milan/Zotero/storage/3ENAXZTL/tab-pdf.html}
}

@article{Croci2020,
  title = {Effects of Round-to-Nearest and Stochastic Rounding in the Numerical Solution of the Heat Equation in Low Precision},
  author = {Croci, Matteo and Giles, Michael Bryce},
  year = {2020},
  month = oct,
  journal = {arXiv:2010.16225 [cs, math]},
  eprint = {2010.16225},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {Motivated by the advent of machine learning, the last few years saw the return of hardware-supported low-precision computing. Computations with fewer digits are faster and more memory and energy efficient, but can be extremely susceptible to rounding errors. An application that can largely benefit from the advantages of low-precision computing is the numerical solution of partial differential equations (PDEs), but a careful implementation and rounding error analysis are required to ensure that sensible results can still be obtained. In this paper we study the accumulation of rounding errors in the solution of the heat equation, a proxy for parabolic PDEs, via Runge-Kutta finite difference methods using round-to-nearest (RtN) and stochastic rounding (SR). We demonstrate how to implement the scheme to reduce rounding errors and we derive \textbackslash emph\{a priori\} estimates for local and global rounding errors. Let \$u\$ be the roundoff unit. While the worst-case local errors are \$O(u)\$ with respect to the discretization parameters, the RtN and SR error behavior is substantially different. We prove that the RtN solution is discretization, initial condition and precision dependent, and always stagnates for small enough \$\textbackslash Delta t\$. Until stagnation, the global error grows like \$O(u\textbackslash Delta t\^\{-1\})\$. In contrast, we show that the leading order errors introduced by SR are zero-mean, independent in space and mean-independent in time, making SR resilient to stagnation and rounding error accumulation. In fact, we prove that for SR the global rounding errors are only \$O(u\textbackslash Delta t\^\{-1/4\})\$ in 1D and are essentially bounded (up to logarithmic factors) in higher dimensions.},
  archiveprefix = {arXiv},
  keywords = {65G50; 65G30; 65M06; 65M12; 65M15; 65M22; 65Y99; 65C20,Computer Science - Performance,Mathematics - Numerical Analysis,Mathematics - Probability},
  file = {/Users/milan/Zotero/storage/TVH5I7SC/Croci and Giles - 2020 - Effects of round-to-nearest and stochastic roundin.pdf;/Users/milan/Zotero/storage/69IK3JIS/2010.html}
}

@article{Dalcin2011,
  title = {Parallel Distributed Computing Using {{Python}}},
  author = {Dalcin, Lisandro D. and Paz, Rodrigo R. and Kler, Pablo A. and Cosimo, Alejandro},
  year = {2011},
  month = sep,
  journal = {Advances in Water Resources},
  volume = {34},
  number = {9},
  pages = {1124--1139},
  issn = {03091708},
  doi = {10.1016/j.advwatres.2011.04.013},
  abstract = {This work presents two software components aimed to relieve the costs of accessing high-performance parallel computing resources within a Python programming environment: MPI for Python and PETSc for Python.},
  language = {en}
}

@inproceedings{Damouche2017,
  title = {Toward a {{Standard Benchmark Format}} and {{Suite}} for {{Floating}}-{{Point Analysis}}},
  booktitle = {Numerical {{Software Verification}}},
  author = {Damouche, Nasrine and Martel, Matthieu and Panchekha, Pavel and Qiu, Chen and {Sanchez-Stern}, Alexander and Tatlock, Zachary},
  editor = {Bogomolov, Sergiy and Martel, Matthieu and Prabhakar, Pavithra},
  year = {2017},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {63--77},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-54292-8_6},
  abstract = {We introduce FPBench, a standard benchmark format for validation and optimization of numerical accuracy in floating-point computations. FPBench is a first step toward addressing an increasing need in our community for comparisons and combinations of tools from different application domains. To this end, FPBench provides a basic floating-point benchmark format and accuracy measures for comparing different tools. The FPBench format and measures allow comparing and composing different floating-point tools. We describe the FPBench format and measures and show that FPBench expresses benchmarks from recent papers in the literature, by building an initial benchmark suite drawn from these papers. We intend for FPBench to grow into a standard benchmark suite for the members of the floating-point tools research community.},
  isbn = {978-3-319-54292-8},
  language = {en},
  keywords = {Abstract Interpretation,Affine Arithmetic,Benchmark Format,Benchmark Suite,Imperative Language}
}

@article{DAngiola2010,
  title = {On-Road Traffic Emissions in a Megacity},
  author = {D'Angiola, Ariela and Dawidowski, Laura E. and G{\'o}mez, Dar{\'i}o R. and Osses, Mauricio},
  year = {2010},
  month = feb,
  journal = {Atmospheric Environment},
  volume = {44},
  number = {4},
  pages = {483--493},
  issn = {1352-2310},
  doi = {10.1016/j.atmosenv.2009.11.004},
  abstract = {A new annual bottom\textendash up emission inventory of criteria pollutants and greenhouse gases from on-road mobile sources was developed for 2006 for the metropolitan area of Buenos Aires, Argentina, within a four-year regional project aimed at providing tools for chemical weather forecast in South America. Under the scarcity of local emission factors, we collected data from measuring campaigns performed in Argentina, Brazil, Chile and Colombia and compiled a data set of regional emission factors representative of Latin American fleets and driving conditions. The estimated emissions were validated with respect to downscaled national estimates and the EDGAR global emission database. Our results highlight the role of older technologies accounting in average for almost 80\% of the emissions of all species. The area exhibits higher specific emissions than developed countries, with figures two times higher for criteria pollutants. We analyzed the effect on emissions of replacing gasoline by compressed natural gas, occurring in Argentina since 1995. We identified (i) a relationship between number of vehicles and a compound socioeconomic indicator, and (ii) time-lags in vehicle technologies between developed and developing countries, which can be respectively applied for spatial disaggregation and the development of projections for other Latin American cities. The results may also be employed to complement global emission inventories and by local policy makers as an environmental management tool.},
  language = {en},
  keywords = {Buenos Aires,Emission inventory,Latin America,Megacities,On-road transport,Regional emission factors},
  file = {/Users/milan/Zotero/storage/VXA78Y9P/S1352231009009352.html}
}

@article{Dawson2017,
  title = {Rpe v5: An Emulator for Reduced Floating-Point Precision in Large Numerical Simulations},
  shorttitle = {Rpe V5},
  author = {Dawson, Andrew and D{\"u}ben, Peter D.},
  year = {2017},
  month = jun,
  journal = {Geoscientific Model Development},
  volume = {10},
  number = {6},
  pages = {2221--2230},
  issn = {1991-9603},
  doi = {10.5194/gmd-10-2221-2017},
  abstract = {This paper describes the rpe (reduced-precision emulator) library which has the capability to emulate the use of arbitrary reduced floating-point precision within large numerical models written in Fortran. The rpe software allows model developers to test how reduced floating-point precision affects the result of their simulations without having to make extensive code changes or port the model onto specialized hardware. The software can be used to identify parts of a program that are problematic for numerical precision and to guide changes to the program to allow a stronger reduction in precision.},
  language = {en}
}

@article{Dawson2018,
  title = {Reliable Low Precision Simulations in Land Surface Models},
  author = {Dawson, Andrew and D{\"u}ben, Peter D. and MacLeod, David A. and Palmer, Tim N.},
  year = {2018},
  month = oct,
  journal = {Climate Dynamics},
  volume = {51},
  number = {7},
  pages = {2657--2666},
  issn = {1432-0894},
  doi = {10.1007/s00382-017-4034-x},
  abstract = {Weather and climate models must continue to increase in both resolution and complexity in order that forecasts become more accurate and reliable. Moving to lower numerical precision may be an essential tool for coping with the demand for ever increasing model complexity in addition to increasing computing resources. However, there have been some concerns in the weather and climate modelling community over the suitability of lower precision for climate models, particularly for representing processes that change very slowly over long time-scales. These processes are difficult to represent using low precision due to time increments being systematically rounded to zero. Idealised simulations are used to demonstrate that a model of deep soil heat diffusion that fails when run in single precision can be modified to work correctly using low precision, by splitting up the model into a small higher precision part and a low precision part. This strategy retains the computational benefits of reduced precision whilst preserving accuracy. This same technique is also applied to a full complexity land surface model, resulting in rounding errors that are significantly smaller than initial condition and parameter uncertainties. Although lower precision will present some problems for the weather and climate modelling community, many of the problems can likely be overcome using a straightforward and physically motivated application of reduced precision.},
  language = {en},
  file = {/Users/milan/Zotero/storage/JR2VFU37/Dawson et al. - 2018 - Reliable low precision simulations in land surface.pdf}
}

@article{Delaunay2019,
  title = {Evaluation of Lossless and Lossy Algorithms for the Compression of Scientific Datasets in {{netCDF}}-4 or {{HDF5}} Files},
  author = {Delaunay, Xavier and Courtois, Aur{\'e}lie and Gouillon, Flavien},
  year = {2019},
  month = sep,
  journal = {Geoscientific Model Development},
  volume = {12},
  number = {9},
  pages = {4099--4113},
  issn = {1991-9603},
  doi = {10.5194/gmd-12-4099-2019},
  abstract = {The increasing volume of scientific datasets requires the use of compression to reduce data storage and transmission costs, especially for the oceanographic or meteorological datasets generated by Earth observation mission ground segments. These data are mostly produced in netCDF files. Indeed, the netCDF-4/HDF5 file formats are widely used throughout the global scientific community because of the useful features they offer. HDF5 in particular offers a dynamically loaded filter plugin so that users can write compression/decompression filters, for example, and process the data before reading or writing them to disk. This study evaluates lossy and lossless compression/decompression methods through netCDF-4 and HDF5 tools on analytical and real scientific floating-point datasets. We also introduce the Digit Rounding algorithm, a new relative error-bounded data reduction method inspired by the Bit Grooming algorithm. The Digit Rounding algorithm offers a high compression ratio while keeping a given number of significant digits in the dataset. It achieves a higher compression ratio than the Bit Grooming algorithm with slightly lower compression speed.},
  language = {en},
  file = {/Users/milan/Zotero/storage/36GHPB8T/Delaunay et al. - 2019 - Evaluation of lossless and lossy algorithms for th.pdf}
}

@article{DelSole2004,
  title = {Predictability and {{Information Theory}}. {{Part I}}: {{Measures}} of {{Predictability}}},
  shorttitle = {Predictability and {{Information Theory}}. {{Part I}}},
  author = {DelSole, Timothy},
  year = {2004},
  month = oct,
  journal = {Journal of the Atmospheric Sciences},
  volume = {61},
  number = {20},
  pages = {2425--2440},
  publisher = {{American Meteorological Society}},
  issn = {0022-4928, 1520-0469},
  doi = {10.1175/1520-0469(2004)061<2425:PAITPI>2.0.CO;2},
  abstract = {{$<$}section class="abstract"{$><$}h2 class="abstractTitle text-title my-1" id="d109753621e59"{$>$}Abstract{$<$}/h2{$><$}p{$>$}This paper gives an introduction to the connection between predictability and information theory, and derives new connections between these concepts. A system is said to be unpredictable if the forecast distribution, which gives the most complete description of the future state based on all available knowledge, is identical to the climatological distribution, which describes the state in the absence of time lag information. It follows that a necessary condition for predictability is for the forecast and climatological distributions to differ. Information theory provides a powerful framework for quantifying the difference between two distributions that agrees with intuition about predictability. Three information theoretic measures have been proposed in the literature: predictive information, relative entropy, and mutual information. These metrics are discussed with the aim of clarifying their similarities and differences. All three metrics have attractive properties for defining predictability, including the fact that they are invariant with respect to nonsingular linear transformations, decrease monotonically in stationary Markov systems in some sense, and are easily decomposed into components that optimize them (in certain cases). Relative entropy and predictive information have the same average value, which in turn equals the mutual information. Optimization of mutual information leads naturally to canonical correlation analysis, when the variables are joint normally distributed. Closed form expressions of these metrics for finite dimensional, stationary, Gaussian, Markov systems are derived. Relative entropy and predictive information differ most significantly in that the former depends on the ``signal to noise ratio'' of a single forecast distribution, whereas the latter does not. Part II of this paper discusses the extension of these concepts to imperfect forecast models.{$<$}/p{$><$}/section{$>$}},
  chapter = {Journal of the Atmospheric Sciences},
  language = {EN},
  file = {/Users/milan/Zotero/storage/ZSS8PT2K/DelSole - 2004 - Predictability and Information Theory. Part I Mea.pdf;/Users/milan/Zotero/storage/5B7QBLDY/1520-0469_2004_061_2425_paitpi_2.0.co_2.html}
}

@inproceedings{Denis2016,
  title = {Verificarlo: {{Checking Floating Point Accuracy}} through {{Monte Carlo Arithmetic}}},
  shorttitle = {Verificarlo},
  booktitle = {2016 {{IEEE}} 23nd {{Symposium}} on {{Computer Arithmetic}} ({{ARITH}})},
  author = {Denis, Christophe and De Oliveira Castro, Pablo and Petit, Eric},
  year = {2016},
  month = jul,
  pages = {55--62},
  issn = {1063-6889},
  doi = {10.1109/ARITH.2016.31},
  abstract = {Numerical accuracy of floating point computation is a well studied topic which has not made its way to the end-user in scientific computing. Yet, it has become a critical issue with the recent requirements for code modernization to harness new highly parallel hardware and perform higher resolution computation. To democratize numerical accuracy analysis, it is important to propose tools and methodologies to study large use cases in a reliable and automatic way. In this paper, we propose verificarlo, an extension to the LLVM compiler to automatically use Monte Carlo Arithmetic in a transparent way for the end-user. It supports all the major languages including C, C++, and Fortran. Unlike source-to-source approaches, our implementation captures the influence of compiler optimizations on the numerical accuracy. We illustrate how Monte Carlo Arithmetic using the verificarlo tool outperforms the existing approaches on various use cases and is a step toward automatic numerical analysis.},
  keywords = {compilers,Computational modeling,floating point arithmetic,Hardware,Instruments,Monte Carlo arithmetic,Monte Carlo methods,numerical analysis,Numerical models,Optimization,Standards},
  file = {/Users/milan/Zotero/storage/WH4RWISH/Denis et al. - 2016 - Verificarlo Checking Floating Point Accuracy thro.pdf;/Users/milan/Zotero/storage/WTZ9CXUB/7563272.html}
}

@article{Deser2010,
  title = {Sea {{Surface Temperature Variability}}: {{Patterns}} and {{Mechanisms}}},
  shorttitle = {Sea {{Surface Temperature Variability}}},
  author = {Deser, Clara and Alexander, Michael A. and Xie, Shang-Ping and Phillips, Adam S.},
  year = {2010},
  month = jan,
  journal = {Annual Review of Marine Science},
  volume = {2},
  number = {1},
  pages = {115--143},
  issn = {1941-1405, 1941-0611},
  doi = {10.1146/annurev-marine-120408-151453},
  abstract = {Patterns of sea surface temperature (SST) variability on interannual and longer timescales result from a combination of atmospheric and oceanic processes. These SST anomaly patterns may be due to intrinsic modes of atmospheric circulation variability that imprint themselves upon the SST field mainly via surface energy fluxes. Examples include SST fluctuations in the Southern Ocean associated with the Southern Annular Mode, a tripolar pattern of SST anomalies in the North Atlantic associated with the North Atlantic Oscillation, and a pan-Pacific mode known as the Pacific Decadal Oscillation (with additional contributions from oceanic processes). They may also result from coupled ocean-atmosphere interactions, such as the El Nin\texttildelow{} o-Southern Oscillation phenomenon in the tropical Indo-Pacific, the tropical Atlantic Nin\texttildelow{} o, and the cross-equatorial meridional modes in the tropical Pacific and Atlantic. Finally, patterns of SST variability may arise from intrinsic oceanic modes, notably the Atlantic Multidecadal Oscillation.},
  language = {en}
}

@article{Deser2010a,
  title = {Twentieth Century Tropical Sea Surface Temperature Trends Revisited: {{TWENTIETH CENTURY TROPICAL SST TRENDS}}},
  shorttitle = {Twentieth Century Tropical Sea Surface Temperature Trends Revisited},
  author = {Deser, Clara and Phillips, Adam S. and Alexander, Michael A.},
  year = {2010},
  month = may,
  journal = {Geophysical Research Letters},
  volume = {37},
  number = {10},
  pages = {n/a-n/a},
  issn = {00948276},
  doi = {10.1029/2010GL043321},
  language = {en}
}

@article{Deutsch1996,
  title = {{{DEFLATE Compressed Data Format Specification}} Version 1.3},
  author = {Deutsch, P.},
  year = {1996},
  number = {RFC 1951},
  issn = {2070-1721},
  file = {/Users/milan/Zotero/storage/KCFCP5G5/rfc1951.html}
}

@article{deVerdiere2009,
  title = {Keeping the {{Freedom}} to {{Build Idealized Climate Models}}},
  author = {{de Verdiere}, Alain Colin},
  year = {2009},
  month = jun,
  journal = {Eos, Transactions American Geophysical Union},
  volume = {90},
  number = {26},
  pages = {224--224},
  issn = {00963941},
  doi = {10.1029/2009EO260005},
  language = {en}
}

@inproceedings{Di2016,
  title = {Fast {{Error}}-{{Bounded Lossy HPC Data Compression}} with {{SZ}}},
  booktitle = {2016 {{IEEE International Parallel}} and {{Distributed Processing Symposium}} ({{IPDPS}})},
  author = {Di, Sheng and Cappello, Franck},
  year = {2016},
  month = may,
  pages = {730--739},
  issn = {1530-2075},
  doi = {10.1109/IPDPS.2016.11},
  abstract = {Today's HPC applications are producing extremely large amounts of data, thus it is necessary to use an efficient compression before storing them to parallel file systems. In this paper, we optimize the error-bounded HPC data compression, by proposing a novel HPC data compression method that works very effectively on compressing large-scale HPC data sets. The compression method starts by linearizing multi-dimensional snapshot data. The key idea is to fit/predict the successive data points with the bestfit selection of curve fitting models. The data that can be predicted precisely will be replaced by the code of the corresponding curve-fitting model. As for the unpredictable data that cannot be approximated by curve-fitting models, we perform an optimized lossy compression via a binary representation analysis. We evaluate our proposed solution using 13 real-world HPC applications across different scientific domains, and compare it to many other state-of-the-art compression methods (including Gzip, FPC, ISABELA, NUMARCK, ZFP, FPZIP, etc.). Experiments show that the compression ratio of our compressor ranges in 3.3/1 - 436/1, which is higher than the second-best solution ZFP by as little as 2x and as much as an order of magnitude for most cases. The compression time of SZ is comparable to other solutions', while its decompression time is less than the second best one by 50\%-90\%. On an extreme-scale use case, experiments show that the compression ratio of SZ exceeds that of ZFP by 80\%.},
  keywords = {Arrays,Atmospheric modeling,curve fitting,curve fitting model,data compression,Data compression,Data models,error-bounded lossy HPC data compression,high performance computing,HPC data compression optimization,Magnetohydrodynamics,multidimensional snapshot data linearization,optimisation,parallel processing,Predictive models,Solid modeling,Squeeze,SZ},
  file = {/Users/milan/Zotero/storage/3TJH7HC3/7516069.html}
}

@article{Diamantakis2013,
  title = {The Semi-{{Lagrangian}} Technique in Atmospheric Modelling: Current Status and Future Challenges},
  author = {Diamantakis, Michail},
  year = {2013},
  pages = {18},
  abstract = {The semi-Lagrangian method is an established numerical technique for integrating the transport equations in atmospheric models. Coupled with semi-implicit time-stepping offers unconditional stability for all forcing terms in equation sets of such models. This distinct advantage has led to the development of very efficient numerical weather prediction systems such as the ECMWF Integrated Forecasting System (IFS).},
  language = {en}
}

@article{DiNapoli2018,
  title = {Assessing Heat-Related Health Risk in {{Europe}} via the {{Universal Thermal Climate Index}} ({{UTCI}})},
  author = {Di Napoli, Claudia and Pappenberger, Florian and Cloke, Hannah L.},
  year = {2018},
  month = jul,
  journal = {International Journal of Biometeorology},
  volume = {62},
  number = {7},
  pages = {1155--1165},
  issn = {1432-1254},
  doi = {10.1007/s00484-018-1518-2},
  abstract = {In this work, the potential of the Universal Thermal Climate Index (UTCI) as a heat-related health risk indicator in Europe is demonstrated. The UTCI is a bioclimate index that uses a multi-node human heat balance model to represent the heat stress induced by meteorological conditions to the human body. Using 38~years of meteorological reanalysis data, UTCI maps were computed to assess the thermal bioclimate of Europe for the summer season. Patterns of heat stress conditions and non-thermal stress regions are identified across Europe. An increase in heat stress up to 1~\textdegree C is observed during recent decades. Correlation with mortality data from 17 European countries revealed that the relationship between the UTCI and death counts depends on the bioclimate of the country, and death counts increase in conditions of moderate and strong stress, i.e., when UTCI is above 26 and 32~\textdegree C. The UTCI's ability to represent mortality patterns is demonstrated for the 2003 European heatwave. These findings confirm the importance of UTCI as a bioclimatic index that is able to both capture the thermal bioclimatic variability of Europe, and relate such variability with the effects it has on human health.},
  language = {en},
  file = {/Users/milan/Zotero/storage/739VQG3C/Di Napoli et al. - 2018 - Assessing heat-related health risk in Europe via t.pdf}
}

@incollection{Dongarra2011,
  title = {{{TOP500}}},
  booktitle = {Encyclopedia of {{Parallel Computing}}},
  author = {Dongarra, Jack and Luszczek, Piotr},
  editor = {Padua, David},
  year = {2011},
  pages = {2055--2057},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  doi = {10.1007/978-0-387-09766-4_157},
  isbn = {978-0-387-09766-4},
  language = {en}
}

@article{Douglass2008,
  title = {A Comparison of Tropical Temperature Trends with Model Predictions},
  author = {Douglass, David H. and Christy, John R. and Pearson, Benjamin D. and Singer, S. Fred},
  year = {2008},
  month = nov,
  journal = {International Journal of Climatology},
  volume = {28},
  number = {13},
  pages = {1693--1701},
  issn = {08998418, 10970088},
  doi = {10.1002/joc.1651},
  language = {en}
}

@article{Duan2007,
  title = {Stochastic Parameterization for Large Eddy Simulation of Geophysical Flows},
  author = {Duan, Jinqiao and Nadiga, Balasubramanya T.},
  year = {2007},
  month = apr,
  journal = {Proceedings of the American Mathematical Society},
  volume = {135},
  number = {04},
  pages = {1187--1187},
  issn = {0002-9939},
  doi = {10.1090/S0002-9939-06-08631-X},
  abstract = {Recently, stochastic, as opposed to deterministic, parameterizations are being investigated to model the effects of unresolved subgrid scales (SGS) in large eddy simulations (LES) of geophysical flows. We analyse such a stochastic approach in the barotropic vorticity equation to show that (i) if the stochastic parameterization approximates the actual SGS stresses, then the solution of the stochastic LES approximates the ``true'' solution at appropriate scale sizes; and that (ii) when the filter scale size approaches zero, the solution of the stochastic LES approaches the true solution.},
  language = {en}
}

@article{Duben2014,
  title = {The Use of Imprecise Processing to Improve Accuracy in Weather \& Climate Prediction},
  author = {D{\"u}ben, Peter D. and McNamara, Hugh and Palmer, T.N.},
  year = {2014},
  month = aug,
  journal = {Journal of Computational Physics},
  volume = {271},
  pages = {2--18},
  issn = {00219991},
  doi = {10.1016/j.jcp.2013.10.042},
  abstract = {The use of stochastic processing hardware and low precision arithmetic in atmospheric models is investigated. Stochastic processors allow hardware-induced faults in calculations, sacrificing bit-reproducibility and precision in exchange for improvements in performance and potentially accuracy of forecasts, due to a reduction in power consumption that could allow higher resolution. A similar trade-off is achieved using low precision arithmetic, with improvements in computation and communication speed and savings in storage and memory requirements. As high-performance computing becomes more massively parallel and power intensive, these two approaches may be important stepping stones in the pursuit of global cloud-resolving atmospheric modelling.},
  language = {en}
}

@article{Duben2014a,
  title = {On the Use of Inexact, Pruned Hardware in Atmospheric Modelling},
  author = {D{\"u}ben, Peter D. and Joven, Jaume and Lingamneni, Avinash and McNamara, Hugh and De Micheli, Giovanni and Palem, Krishna V. and Palmer, T. N.},
  year = {2014},
  month = jun,
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {372},
  number = {2018},
  pages = {20130276},
  issn = {1364-503X, 1471-2962},
  doi = {10.1098/rsta.2013.0276},
  language = {en}
}

@article{Duben2018,
  title = {A {{New Number Format}} for {{Ensemble Simulations}}},
  author = {D{\"u}ben, Peter D.},
  year = {2018},
  month = nov,
  journal = {Journal of Advances in Modeling Earth Systems},
  volume = {10},
  number = {11},
  pages = {2983--2991},
  issn = {1942-2466, 1942-2466},
  doi = {10.1029/2018MS001420},
  language = {en}
}

@article{Duben2018a,
  ids = {Duben2019},
  title = {New {{Methods}} for {{Data Storage}} of {{Model Output}} from {{Ensemble Simulations}}},
  author = {D{\"u}ben, Peter D. and Leutbecher, Martin and Bauer, Peter},
  year = {2018},
  month = dec,
  journal = {Monthly Weather Review},
  volume = {147},
  number = {2},
  pages = {677--689},
  publisher = {{American Meteorological Society}},
  issn = {0027-0644},
  doi = {10.1175/MWR-D-18-0170.1},
  abstract = {Data storage and data processing generate significant cost for weather and climate modeling centers. The volume of data that needs to be stored and data that are disseminated to end users increases with increasing model resolution and the use of larger forecast ensembles. If precision of data is reduced, cost can be reduced accordingly. In this paper, three new methods to allow a reduction in precision with minimal loss of information are suggested and tested. Two of these methods rely on the similarities between ensemble members in ensemble forecasts. Therefore, precision will be high at the beginning of forecasts when ensemble members are more similar, to provide sufficient distinction, and decrease with increasing ensemble spread. To keep precision high for predictable situations and low elsewhere appears to be a useful approach to optimize data storage in weather forecasts. All methods are tested with data of operational weather forecasts of the European Centre for Medium-Range Weather Forecasts.},
  file = {/Users/milan/Zotero/storage/ZSIQUCTM/Düben et al. - 2019 - New Methods for Data Storage of Model Output from .pdf}
}

@article{Duben2019,
  title = {New {{Methods}} for {{Data Storage}} of {{Model Output}} from {{Ensemble Simulations}}},
  author = {D{\"u}ben, Peter D. and Leutbecher, Martin and Bauer, Peter},
  year = {2019},
  month = feb,
  journal = {Monthly Weather Review},
  volume = {147},
  number = {2},
  pages = {677--689},
  publisher = {{American Meteorological Society}},
  issn = {0027-0644},
  doi = {10.1175/MWR-D-18-0170.1},
  language = {en},
  file = {/Users/milan/Zotero/storage/XRZMWNRQ/Düben et al. - 2019 - New Methods for Data Storage of Model Output from .pdf;/Users/milan/Zotero/storage/YH3D2IGS/New-Methods-for-Data-Storage-of-Model-Output-from.html}
}

@article{Duben2019a,
  title = {New {{Methods}} for {{Data Storage}} of {{Model Output}} from {{Ensemble Simulations}}},
  author = {D{\"u}ben, Peter D. and Leutbecher, Martin and Bauer, Peter},
  year = {2019},
  month = feb,
  journal = {Monthly Weather Review},
  volume = {147},
  number = {2},
  pages = {677--689},
  issn = {0027-0644, 1520-0493},
  doi = {10.1175/MWR-D-18-0170.1},
  abstract = {Data storage and data processing generate significant cost for weather and climate modeling centers. The volume of data that needs to be stored and data that are disseminated to end users increases with increasing model resolution and the use of larger forecast ensembles. If precision of data is reduced, cost can be reduced accordingly. In this paper, three new methods to allow a reduction in precision with minimal loss of information are suggested and tested. Two of these methods rely on the similarities between ensemble members in ensemble forecasts. Therefore, precision will be high at the beginning of forecasts when ensemble members are more similar, to provide sufficient distinction, and decrease with increasing ensemble spread. To keep precision high for predictable situations and low elsewhere appears to be a useful approach to optimize data storage in weather forecasts. All methods are tested with data of operational weather forecasts of the European Centre for Medium-Range Weather Forecasts.},
  language = {en}
}

@article{Eden2001,
  title = {North {{Atlantic Interdecadal Variability}}: {{Oceanic Response}} to the {{North Atlantic Oscillation}} (1865\textendash 1997)},
  author = {Eden, Carsten and Jung, Thomas},
  year = {2001},
  journal = {JOURNAL OF CLIMATE},
  volume = {14},
  pages = {16},
  abstract = {In contrast to the atmosphere, knowledge about interdecadal variability of the North Atlantic circulation is relatively restricted. It is the objective of this study to contribute to understanding how the North Atlantic circulation responds to a forcing by the North Atlantic oscillation (NAO) on interdecadal timescales. For this purpose, the authors analyze observed atmospheric and sea surface temperature (SST) data along with the response of an ocean general circulation model to a realistic monthly surface flux forcing that is solely associated with the NAO for the period 1865\textendash 1997.},
  language = {en}
}

@article{Eden2008,
  title = {Towards a Mesoscale Eddy Closure},
  author = {Eden, Carsten and Greatbatch, Richard J.},
  year = {2008},
  month = jan,
  journal = {Ocean Modelling},
  volume = {20},
  number = {3},
  pages = {223--239},
  issn = {14635003},
  doi = {10.1016/j.ocemod.2007.09.002},
  abstract = {A turbulence closure for the effect of mesoscale eddies in non-eddy-resolving ocean models is proposed. The closure consists of a prognostic equation for the eddy kinetic energy (EKE) that is integrated as an additional model equation, and a diagnostic relation for an eddy length scale (L), which is given by the minimum of Rhines scale and Rossby radius. Combining EKE and L using a standard mixing length assumption gives a diffusivity (K), corresponding to the thickness diffusivity in the [Gent, P.R., McWilliams, J.C. 1990. Isopycnal mixing in ocean circulation models. J. Phys. Oceanogr. 20, 150\textendash 155] parameterisation. Assuming downgradient mixing of potential vorticity with identical diffusivity shows how K is related to horizontal and vertical mixing processes in the horizontal momentum equation, and also enables us to parameterise the source of EKE related to eddy momentum fluxes.},
  language = {en}
}

@article{Eden2010,
  title = {Parameterising Meso-Scale Eddy Momentum Fluxes Based on Potential Vorticity Mixing and a Gauge Term},
  author = {Eden, Carsten},
  year = {2010},
  month = jan,
  journal = {Ocean Modelling},
  volume = {32},
  number = {1-2},
  pages = {58--71},
  issn = {14635003},
  doi = {10.1016/j.ocemod.2009.10.008},
  abstract = {Meso-scale fluctuations are known to drive large-scale zonal flows in the ocean, a mechanism which is currently missing in non-eddy-resolving ocean models. A closure for meso-scale eddy momentum fluxes is evaluated in a suite of idealised eddying channel models, featuring eddy-driven zonal jets. It is shown how the appearance of zonal jets, which act as mixing barriers for turbulent exchange, and reduced lateral diffusivities are linked in a natural way by implementing mixing of potential vorticity and using a gauge term to insure that no spurious forces are introduced. It appears, therefore, possible to parameterise the appearance of zonal jets and its effect on the ventilation of interior ocean basins in non-eddyresolving, realistic ocean models.},
  language = {en}
}

@article{Eden2016,
  title = {Closing the Energy Cycle in an Ocean Model},
  author = {Eden, Carsten},
  year = {2016},
  month = may,
  journal = {Ocean Modelling},
  volume = {101},
  pages = {30--42},
  issn = {14635003},
  doi = {10.1016/j.ocemod.2016.02.005},
  abstract = {An effort is discussed to construct a realistic ocean model in Boussinesq approximation which features a closed energy cycle up to numerical precision errors. In such a model, the energy related to the mean variables interacts with all parameterised forms of energy without any spurious energy sources or sinks. First, the concept of the energetics of the model in terms of resolved and unresolved energy variables is outlined using potential and dynamical enthalpy instead of internal and potential energy and without use of the concept of available potential energy. The role of energy transfer terms due to the non-linear, compressible equation of state is clarified. Second, a discretisation of the primitive equations is described in which energy transfers of viscous dissipation and mixing parameterisations are exactly calculated. Third, the model performance is documented using idealised and realistic global model configurations.},
  language = {en}
}

@article{Ellis,
  title = {Build High-Resolution Global Climate Models},
  author = {Ellis, Gerry},
  pages = {2},
  language = {en}
}

@misc{Environment2020,
  title = {Emissions {{Gap Report}} 2020},
  author = {Environment, U. N.},
  year = {Tue, 12/01/2020 - 06:20},
  journal = {UNEP - UN Environment Programme},
  abstract = {The 2020 report finds that the world is still heading for a temperature rise in excess of 3\textdegree C this century but a low-carbon pandemic recovery could reduce the worst impacts of climate change.},
  chapter = {publications},
  howpublished = {http://www.unenvironment.org/emissions-gap-report-2020},
  language = {en},
  file = {/Users/milan/Zotero/storage/DT4WCG2U/emissions-gap-report-2020.html}
}

@article{ErikGundersen2021,
  title = {The Fundamental Principles of Reproducibility},
  author = {Erik Gundersen, Odd},
  year = {2021},
  month = may,
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {379},
  number = {2197},
  pages = {20200210},
  publisher = {{Royal Society}},
  doi = {10.1098/rsta.2020.0210},
  abstract = {Reproducibility is a confused terminology. In this paper, I take a fundamental view on reproducibility rooted in the scientific method. The scientific method is analysed and characterized in order to develop the terminology required to define reproducibility. Furthermore, the literature on reproducibility and replication is surveyed, and experiments are modelled as tasks and problem solving methods. Machine learning is used to exemplify the described approach. Based on the analysis, reproducibility is defined and three different degrees of reproducibility as well as four types of reproducibility are specified.This article is part of the theme issue `Reliability and reproducibility in computational science: implementing verification, validation and uncertainty quantification in silico'.},
  file = {/Users/milan/Zotero/storage/EBIZDZ4G/Erik Gundersen - 2021 - The fundamental principles of reproducibility.pdf;/Users/milan/Zotero/storage/Q5YUTW56/rsta.2020.html}
}

@article{Eyring2016,
  title = {Overview of the {{Coupled Model Intercomparison Project Phase}} 6 ({{CMIP6}}) Experimental Design and Organization},
  author = {Eyring, Veronika and Bony, Sandrine and Meehl, Gerald A. and Senior, Catherine A. and Stevens, Bjorn and Stouffer, Ronald J. and Taylor, Karl E.},
  year = {2016},
  month = may,
  journal = {Geoscientific Model Development},
  volume = {9},
  number = {5},
  pages = {1937--1958},
  issn = {1991-9603},
  doi = {10.5194/gmd-9-1937-2016},
  abstract = {Abstract. By coordinating the design and distribution of global climate model simulations of the past, current, and future climate, the Coupled Model Intercomparison Project (CMIP) has become one of the foundational elements of climate science. However, the need to address an ever-expanding range of scientific questions arising from more and more research communities has made it necessary to revise the organization of CMIP. After a long and wide community consultation, a new and more federated structure has been put in place. It consists of three major elements: (1)~a handful of common experiments, the DECK (Diagnostic, Evaluation and Characterization of Klima) and CMIP historical simulations (1850\textendash near present) that will maintain continuity and help document basic characteristics of models across different phases of CMIP; (2)~common standards, coordination, infrastructure, and documentation that will facilitate the distribution of model outputs and the characterization of the model ensemble; and (3)~an ensemble of CMIP-Endorsed Model Intercomparison Projects (MIPs) that will be specific to a particular phase of CMIP (now CMIP6) and that will build on the DECK and CMIP historical simulations to address a large range of specific questions and fill the scientific gaps of the previous CMIP phases. The DECK and CMIP historical simulations, together with the use of CMIP data standards, will be the entry cards for models participating in CMIP. Participation in CMIP6-Endorsed MIPs by individual modelling groups will be at their own discretion and will depend on their scientific interests and priorities. With the Grand Science Challenges of the World Climate Research Programme (WCRP) as its scientific backdrop, CMIP6 will address three broad questions:  \textendash{} How does the Earth system respond to forcing? \textendash{} What are the origins and consequences of systematic model biases?  \textendash{} How can we assess future climate changes given internal climate variability, predictability, and uncertainties in scenarios? This CMIP6 overview paper presents the background and rationale for the new structure of CMIP, provides a detailed description of the DECK and CMIP6 historical simulations, and includes a brief introduction to the 21~CMIP6-Endorsed MIPs.},
  language = {en}
}

@inproceedings{Fan2019,
  title = {Using {{DCT}}-Based {{Approximate Communication}} to {{Improve MPI Performance}} in {{Parallel Clusters}}},
  booktitle = {2019 {{IEEE}} 38th {{International Performance Computing}} and {{Communications Conference}} ({{IPCCC}})},
  author = {Fan, Qianqian and Lilja, David J. and Sapatnekar, Sachin S.},
  year = {2019},
  month = oct,
  pages = {1--10},
  publisher = {{IEEE}},
  address = {{London, United Kingdom}},
  doi = {10.1109/IPCCC47392.2019.8958720},
  abstract = {Communication overheads in distributed systems constitute a large fraction of the total execution time, and limit the scalability of applications running on these systems. We propose a DCT-based approximate communication scheme that takes advantage of the error resiliency of several widely-used applications, and improves communication efficiency by substantially reducing message lengths. Our scheme is implemented into the Message Passing Interface (MPI) library. When evaluated on several representative MPI applications on a real cluster system, it is seen that the fraction of total execution time devoted to communication reduces from 59\% to 23\%, even accounting for the computational overhead required for DCT encoding. For many communication-intensive applications, it is shown that our approximate communication scheme effectively speeds up the total execution time without much loss in quality of the result.},
  isbn = {978-1-72811-025-7},
  language = {en},
  file = {/Users/milan/Zotero/storage/BWWFMRIX/Fan et al. - 2019 - Using DCT-based Approximate Communication to Impro.pdf}
}

@article{Fehlberg,
  title = {{Klassische Runge-Kutta-Formeln vierter und niedrigerer 0rdnung mit Schrittweiten-Kontrolle und ihre Anwendung auf W\textasciitilde irmeleitungsprobleme}},
  author = {Fehlberg, E},
  pages = {11},
  language = {de}
}

@article{Ferrari2009,
  title = {Ocean {{Circulation Kinetic Energy}}: {{Reservoirs}}, {{Sources}}, and {{Sinks}}},
  shorttitle = {Ocean {{Circulation Kinetic Energy}}},
  author = {Ferrari, Raffaele and Wunsch, Carl},
  year = {2009},
  month = jan,
  journal = {Annual Review of Fluid Mechanics},
  volume = {41},
  number = {1},
  pages = {253--282},
  issn = {0066-4189, 1545-4479},
  doi = {10.1146/annurev.fluid.40.111406.102139},
  abstract = {The ocean circulation is a cause and consequence of fluid scale interactions ranging from millimeters to more than 10,000 km. Although the wind field produces a large energy input to the ocean, all but approximately 10\% appears to be dissipated within about 100 m of the sea surface, rendering observations of the energy divergence necessary to maintain the full watercolumn flow difficult. Attention thus shifts to the physically different kinetic energy (KE) reservoirs of the circulation and their maintenance, dissipation, and possible influence on the very small scales representing irreversible molecular mixing. Oceanic KE is dominated by the geostrophic eddy field, and depending on the vertical structure (barotropic versus low-mode baroclinic), direct and inverse energy cascades are possible. The pathways toward dissipation of the dominant geostrophic eddy KE depend crucially on the direction of the cascade but are difficult to quantify because of serious observational difficulties for wavelengths shorter than approximately 100\textendash 200 km. At high frequencies, KE is dominated by internal waves with near-inertial frequencies (frequencies near the local Coriolis parameter), whose shears appear to be a major source of wave breaking and mixing in the ocean interior.},
  language = {en}
}

@article{Ferrari2010,
  title = {The Distribution of Eddy Kinetic and Potential Energies in the Global Ocean},
  author = {Ferrari, Raffaele and Wunsch, Carl},
  year = {2010},
  month = mar,
  journal = {Tellus A},
  issn = {1600-0870, 0280-6495},
  doi = {10.3402/tellusa.v62i2.15680},
  abstract = {Understanding of the major sources, sinks, and reservoirs of energy in the ocean is briefly updated in a diagram. The nature of the dominant kinetic energy reservoir, that of the balanced variablity, is then found to be indistinguishable in the observations from a sum of barotropic and first baroclinic ordinary quasi-geostrophic modes. Little supporting evidence is available to partition the spectra among forced motions and turbulent cascades, along with significant energy more consistent with weakly non-linear wave dynamics. Linear-response wind-forced motions appear to dominate the high frequency (but subinertial) mooring frequency spectra. Turbulent cascades appear to fill the high wavenumber spectra in altimetric data and numerical simulations. Progress on these issues is hindered by the difficulty in connecting the comparatively easily available frequency spectra with the variety of theoretically predicted wavenumber spectra.},
  language = {en}
}

@inproceedings{Fevotte2019,
  title = {Debugging and {{Optimization}} of {{HPC Programs}} with the {{Verrou Tool}}},
  booktitle = {2019 {{IEEE}}/{{ACM}} 3rd {{International Workshop}} on {{Software Correctness}} for {{HPC Applications}} ({{Correctness}})},
  author = {Fevotte, Fran{\c c}ois and Lathuili{\`e}re, Bruno},
  year = {2019},
  month = nov,
  pages = {1--10},
  doi = {10.1109/Correctness49594.2019.00006},
  abstract = {The analysis of Floating-Point-related issues in HPC codes is becoming a topic of major interest: parallel computing and code optimization often break the reproducibility of numerical results across machines, compilers and even executions of the same program. This paper presents how the Verrou tool can help during all stages of the Floating-Point analysis of HPC codes: diagnose, debugging and optimization. Recent developments of Verrou are presented, along with examples illustrating the interest of these new features for industrial codes such as code aster. More specifically, the Verrou arithmetic back-ends now allow analyzing or emulating mixed-precision programs. Interlibm, an interposition layer for the mathematical library, is introduced to mitigate long-standing issues with algorithms from the libm. Finally, debugging algorithms are extended in order to produce useful information as soon as it is available. All these features are available in released version 2.1.0 and upcoming version 2.2.0.},
  keywords = {Floating-point arithmetic,mixed precision,Verification \& Validation,Verrou},
  file = {/Users/milan/Zotero/storage/7DQ4Z7EL/8950985.html}
}

@techreport{Fleming2019,
  title = {Environmental {{Trends}} in {{Aviation}} to 2050},
  author = {Fleming, Gregg G and {de L{\'e}pinay}, Ivan},
  year = {2019},
  pages = {7},
  institution = {{ICAO}},
  language = {en},
  file = {/Users/milan/Zotero/storage/HTBSXKVN/Fleming - Environmental Trends in Aviation to 2050.pdf}
}

@article{Foltz2003,
  title = {Seasonal Mixed Layer Heat Budget of the Tropical {{Atlantic Ocean}}},
  author = {Foltz, Gregory R.},
  year = {2003},
  journal = {Journal of Geophysical Research},
  volume = {108},
  number = {C5},
  pages = {3146},
  issn = {0148-0227},
  doi = {10.1029/2002JC001584},
  language = {en}
}

@article{Forster2020,
  title = {Current and Future Global Climate Impacts Resulting from {{COVID}}-19},
  author = {Forster, Piers M. and Forster, Harriet I. and Evans, Mat J. and Gidden, Matthew J. and Jones, Chris D. and Keller, Christoph A. and Lamboll, Robin D. and Qu{\'e}r{\'e}, Corinne Le and Rogelj, Joeri and Rosen, Deborah and Schleussner, Carl-Friedrich and Richardson, Thomas B. and Smith, Christopher J. and Turnock, Steven T.},
  year = {2020},
  month = oct,
  journal = {Nature Climate Change},
  volume = {10},
  number = {10},
  pages = {913--919},
  publisher = {{Nature Publishing Group}},
  issn = {1758-6798},
  doi = {10.1038/s41558-020-0883-0},
  abstract = {The global response to the COVID-19 pandemic has led to a sudden reduction of both GHG emissions and air pollutants. Here, using national mobility data, we estimate global emission reductions for ten species during the period February to June 2020. We estimate that global NOx emissions declined by as much as 30\% in April, contributing a short-term cooling since the start of the year. This cooling trend is offset by \textasciitilde 20\% reduction in global SO2 emissions that weakens the aerosol cooling effect, causing short-term warming. As a result, we estimate that the direct effect of the pandemic-driven response will be negligible, with a cooling of around 0.01\,{$\pm$}\,0.005\,\textdegree C by 2030 compared to a baseline scenario that follows current national policies. In contrast, with an economic recovery tilted towards green stimulus and reductions in fossil fuel investments, it is possible to avoid future warming of 0.3\,\textdegree C by 2050.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  language = {en},
  file = {/Users/milan/Zotero/storage/V2MGMGZ3/Forster et al. - 2020 - Current and future global climate impacts resultin.pdf;/Users/milan/Zotero/storage/U8PBA8FA/s41558-020-0883-0.html}
}

@article{Fox-Kemper2008,
  title = {Parameterization of {{Mixed Layer Eddies}}. {{Part II}}: {{Prognosis}} and {{Impact}}},
  shorttitle = {Parameterization of {{Mixed Layer Eddies}}. {{Part II}}},
  author = {{Fox-Kemper}, Baylor and Ferrari, Raffaele},
  year = {2008},
  month = jun,
  journal = {Journal of Physical Oceanography},
  volume = {38},
  number = {6},
  pages = {1166--1179},
  issn = {0022-3670, 1520-0485},
  doi = {10.1175/2007JPO3788.1},
  abstract = {The authors propose a parameterization for restratification by mixed layer eddies that develop from baroclinic instabilities of ocean fronts. The parameterization is cast as an overturning streamfunction that is proportional to the product of horizontal buoyancy gradient, mixed layer depth, and inertial period. The parameterization has remarkable skill for an extremely wide range of mixed layer depths, rotation rates, and vertical and horizontal stratifications. In this paper a coarse resolution prognostic model of the parameterization is compared with submesoscale mixed layer eddy resolving simulations. The parameterization proves accurate in predicting changes to the buoyancy. The climate implications of the proposed parameterization are estimated by applying the restratification scaling to observations: the mixed layer depth is estimated from climatology, and the buoyancy gradients are from satellite altimetry. The vertical fluxes are comparable to monthly mean air\textendash sea fluxes in large areas of the ocean and suggest that restratification by mixed layer eddies is a leading order process in the upper ocean. Critical regions for ocean\textendash atmosphere interaction, such as deep, intermediate, and mode water formation sites, are particularly affected.},
  language = {en}
}

@article{Fox-Kemper2011,
  title = {Parameterization of Mixed Layer Eddies. {{III}}: {{Implementation}} and Impact in Global Ocean Climate Simulations},
  shorttitle = {Parameterization of Mixed Layer Eddies. {{III}}},
  author = {{Fox-Kemper}, B. and Danabasoglu, G. and Ferrari, R. and Griffies, S.M. and Hallberg, R.W. and Holland, M.M. and Maltrud, M.E. and Peacock, S. and Samuels, B.L.},
  year = {2011},
  month = jan,
  journal = {Ocean Modelling},
  volume = {39},
  number = {1-2},
  pages = {61--78},
  issn = {14635003},
  doi = {10.1016/j.ocemod.2010.09.002},
  abstract = {A parameterization for the restratification by finite-amplitude, submesoscale, mixed layer eddies, formulated as an overturning streamfunction, has been recently proposed to approximate eddy fluxes of density and other tracers. Here, the technicalities of implementing the parameterization in the coarseresolution ocean component of global climate models are made explicit, and the primary impacts on model solutions of implementing the parameterization are discussed. Three global ocean general circulation models including this parameterization are contrasted with control simulations lacking the parameterization. The MLE parameterization behaves as expected and fairly consistently in models differing in discretization, boundary layer mixing, resolution, and other parameterizations. The primary impact of the parameterization is a shoaling of the mixed layer, with the largest effect in polar winter regions. Secondary impacts include strengthening the Atlantic meridional overturning while reducing its variability, reducing CFC and tracer ventilation, modest changes to sea surface temperature and air\textendash sea fluxes, and an apparent reduction of sea ice basal melting.},
  language = {en}
}

@article{Fox-Kemper2019,
  title = {Challenges and {{Prospects}} in {{Ocean Circulation Models}}},
  author = {{Fox-Kemper}, Baylor and Adcroft, Alistair and B{\"o}ning, Claus W. and Chassignet, Eric P. and Curchitser, Enrique and Danabasoglu, Gokhan and Eden, Carsten and England, Matthew H. and Gerdes, R{\"u}diger and Greatbatch, Richard J. and Griffies, Stephen M. and Hallberg, Robert W. and Hanert, Emmanuel and Heimbach, Patrick and Hewitt, Helene T. and Hill, Christopher N. and Komuro, Yoshiki and Legg, Sonya and Le Sommer, Julien and Masina, Simona and Marsland, Simon J. and Penny, Stephen G. and Qiao, Fangli and Ringler, Todd D. and Treguier, Anne Marie and Tsujino, Hiroyuki and Uotila, Petteri and Yeager, Stephen G.},
  year = {2019},
  month = feb,
  journal = {Frontiers in Marine Science},
  volume = {6},
  pages = {65},
  issn = {2296-7745},
  doi = {10.3389/fmars.2019.00065},
  language = {en}
}

@inproceedings{Fox2020,
  title = {A {{Block Minifloat Representation}} for {{Training Deep Neural Networks}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Fox, Sean and Rasoulinezhad, Seyedramin and Faraone, Julian and Boland, David and Leong, Philip},
  year = {2020},
  month = sep,
  abstract = {Training Deep Neural Networks (DNN) with high efficiency can be difficult to achieve with native floating-point representations and commercially available hardware. Specialized arithmetic with...},
  language = {en},
  file = {/Users/milan/Zotero/storage/CGWBZP4Q/Fox et al. - 2020 - A Block Minifloat Representation for Training Deep.pdf;/Users/milan/Zotero/storage/XMFSHNDZ/forum.html}
}

@article{Fuhrer2018,
  title = {Near-Global Climate Simulation at 1\&thinsp;Km Resolution: Establishing a Performance Baseline on 4888\&thinsp;{{GPUs}} with {{COSMO}} 5.0},
  shorttitle = {Near-Global Climate Simulation at 1\&thinsp;Km Resolution},
  author = {Fuhrer, Oliver and Chadha, Tarun and Hoefler, Torsten and Kwasniewski, Grzegorz and Lapillonne, Xavier and Leutwyler, David and L{\"u}thi, Daniel and Osuna, Carlos and Sch{\"a}r, Christoph and Schulthess, Thomas C. and Vogt, Hannes},
  year = {2018},
  month = may,
  journal = {Geoscientific Model Development},
  volume = {11},
  number = {4},
  pages = {1665--1681},
  publisher = {{Copernicus GmbH}},
  issn = {1991-959X},
  doi = {10.5194/gmd-11-1665-2018},
  abstract = {{$<$}p{$><$}strong class="journal-contentHeaderColor"{$>$}Abstract.{$<$}/strong{$>$} The best hope for reducing long-standing global climate model biases is by increasing resolution to the kilometer scale. Here we present results from an ultrahigh-resolution non-hydrostatic climate model for a near-global setup running on the full Piz Daint supercomputer on 4888\&thinsp;GPUs (graphics processing units). The dynamical core of the model has been completely rewritten using a domain-specific language (DSL) for performance portability across different hardware architectures. Physical parameterizations and diagnostics have been ported using compiler directives. To our knowledge this represents the first complete atmospheric model being run entirely on accelerators on this scale. At a grid spacing of 930\&thinsp;m (1.9\&thinsp;km), we achieve a simulation throughput of 0.043 (0.23) simulated years per day and an energy consumption of 596\&thinsp;MWh per simulated year. Furthermore, we propose a new memory usage efficiency (MUE) metric that considers how efficiently the memory bandwidth \textendash{} the dominant bottleneck of climate codes \textendash{} is being used.{$<$}/p{$>$}},
  language = {English},
  file = {/Users/milan/Zotero/storage/GDE4HE8D/Fuhrer et al. - 2018 - Near-global climate simulation at 1&thinsp\;km reso.pdf;/Users/milan/Zotero/storage/9BVTQKLE/2018.html}
}

@article{Gagne2014,
  title = {Machine {{Learning Enhancement}} of {{Storm}}-{{Scale Ensemble Probabilistic Quantitative Precipitation Forecasts}}},
  author = {Gagne, David John and McGovern, Amy and Xue, Ming},
  year = {2014},
  month = aug,
  journal = {Weather and Forecasting},
  volume = {29},
  number = {4},
  pages = {1024--1043},
  publisher = {{American Meteorological Society}},
  issn = {1520-0434, 0882-8156},
  doi = {10.1175/WAF-D-13-00108.1},
  abstract = {{$<$}section class="abstract"{$><$}h2 class="abstractTitle text-title my-1" id="d4511638e71"{$>$}Abstract{$<$}/h2{$><$}p{$>$}Probabilistic quantitative precipitation forecasts challenge meteorologists due to the wide variability of precipitation amounts over small areas and their dependence on conditions at multiple spatial and temporal scales. Ensembles of convection-allowing numerical weather prediction models offer a way to produce improved precipitation forecasts and estimates of the forecast uncertainty. These models allow for the prediction of individual convective storms on the model grid, but they often displace the storms in space, time, and intensity, which results in added uncertainty. Machine learning methods can produce calibrated probabilistic forecasts from the raw ensemble data that correct for systemic biases in the ensemble precipitation forecast and incorporate additional uncertainty information from aggregations of the ensemble members and additional model variables. This study utilizes the 2010 Center for Analysis and Prediction of Storms Storm-Scale Ensemble Forecast system and the National Severe Storms Laboratory National Mosaic \&amp; Multi-Sensor Quantitative Precipitation Estimate as input data for training logistic regressions and random forests to produce a calibrated probabilistic quantitative precipitation forecast. The reliability and discrimination of the forecasts are compared through verification statistics and a case study.{$<$}/p{$><$}/section{$>$}},
  chapter = {Weather and Forecasting},
  language = {EN},
  file = {/Users/milan/Zotero/storage/FDEKIKCB/Gagne et al. - 2014 - Machine Learning Enhancement of Storm-Scale Ensemb.pdf;/Users/milan/Zotero/storage/RXRSYD62/waf-d-13-00108_1.html}
}

@article{Gertz2009,
  title = {Near-{{Inertial Oscillations}} and the {{Damping}} of {{Midlatitude Gyres}}: {{A Modeling Study}}},
  shorttitle = {Near-{{Inertial Oscillations}} and the {{Damping}} of {{Midlatitude Gyres}}},
  author = {Gertz, Aaron and Straub, David N.},
  year = {2009},
  month = sep,
  journal = {Journal of Physical Oceanography},
  volume = {39},
  number = {9},
  pages = {2338--2350},
  issn = {0022-3670, 1520-0485},
  doi = {10.1175/2009JPO4058.1},
  abstract = {The classic wind-driven double-gyre problem for a homogeneous (unstratified) thin aspect ratio fluid is considered, but allowing for the flow to be depth dependent. Linear free modes for which the vertical wavenumber kz 6{$\frac{1}{4}$} 0 are inertial oscillations, and they are excited with a large-scale stochastic forcing. This produces a background sea of near-inertial oscillations and their interaction with the vertically averaged flow is the focus of this study. In the absence of 3D forcing, the near-inertial motion vanishes and the barotropic quasigeostrophic system is recovered. With 3D forcing, 2D-to-3D energy transfers\textemdash coupled with a forward cascade of 3D energy and scale-selective dissipation\textemdash provide an energy dissipation mechanism for the gyres. The relative strength of this mechanism and a Rayleigh drag applied to the 2D flow depends on both the 3D forcing strength and the Rayleigh drag coefficient.},
  language = {en}
}

@article{Gill1951,
  title = {A Process for the Step-by-Step Integration of Differential Equations in an Automatic Digital Computing Machine},
  author = {Gill, S.},
  year = {1951},
  month = jan,
  journal = {Mathematical Proceedings of the Cambridge Philosophical Society},
  volume = {47},
  number = {1},
  pages = {96--108},
  publisher = {{Cambridge University Press}},
  issn = {1469-8064, 0305-0041},
  doi = {10.1017/S0305004100026414},
  abstract = {It is advantageous in automatic computers to employ methods of integration which do not require preceding function values to be known. From a general theory given by Kutta, one such process is chosen giving fourth-order accuracy and requiring the minimum number of storage registers. It is developed into a form which gives the highest attainable accuracy and can be carried out by comparatively few instructions. The errors are studied and a simple example is given.},
  language = {en},
  file = {/Users/milan/Zotero/storage/AM69LQPS/39FD4351E30AD4588E2F9C001665C62D.html}
}

@book{Gill1982,
  title = {Atmosphere-Ocean Dynamics},
  author = {Gill, Adrian E.},
  year = {1982},
  series = {International Geophysics Series},
  number = {30},
  publisher = {{Acad. Press}},
  address = {{San Diego}},
  isbn = {978-0-12-283520-9 978-0-12-283522-3},
  language = {en},
  annotation = {OCLC: 249294465}
}

@article{Glaser2017,
  title = {An 826 {{MOPS}}, 210 {{uW}}/{{MHz Unum ALU}} in 65 Nm},
  author = {Glaser, Florian and Mach, Stefan and Rahimi, Abbas and G{\"u}rkaynak, Frank K. and Huang, Qiuting and Benini, Luca},
  year = {2017},
  month = dec,
  journal = {arXiv:1712.01021 [cs]},
  eprint = {1712.01021},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {To overcome the limitations of conventional floatingpoint number formats, an interval arithmetic and variablewidth storage format called universal number (unum) has been recently introduced [1]. This paper presents the first (to the best of our knowledge) silicon implementation measurements of an application-specific integrated circuit (ASIC) for unum floating-point arithmetic. The designed chip includes a 128-bit wide unum arithmetic unit to execute additions and subtractions, while also supporting lossless (for intermediate results) and lossy (for external data movements) compression units to exploit the memory usage reduction potential of the unum format. Our chip, fabricated in a 65 nm CMOS process, achieves a maximum clock frequency of 413 MHz at 1.2 V with an average measured power of 210 uW/MHz.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {Computer Science - Hardware Architecture}
}

@techreport{Gore2020,
  title = {Confronting Carbon Inequality},
  author = {Gore, Tim},
  year = {2020},
  month = sep,
  language = {en},
  file = {/Users/milan/Zotero/storage/9PQ729DN/confronting-carbon-inequality.html}
}

@techreport{Gore2020a,
  title = {Confronting {{Carbon Inequality}}: {{Putting}} Climate Justice at the Heart of the {{COVID}}-19 Recovery},
  author = {Gore, Tim and Alestig, Mira and Ratcliff, Anna},
  year = {2020},
  month = sep,
  pages = {12},
  institution = {{Oxfam}},
  language = {en},
  file = {/Users/milan/Zotero/storage/38PRG7S7/Gore - Confronting Carbon Inequality Putting climate jus.pdf}
}

@article{Gossling2020,
  title = {The Global Scale, Distribution and Growth of Aviation: {{Implications}} for Climate Change},
  shorttitle = {The Global Scale, Distribution and Growth of Aviation},
  author = {G{\"o}ssling, Stefan and Humpe, Andreas},
  year = {2020},
  month = nov,
  journal = {Global Environmental Change},
  volume = {65},
  pages = {102194},
  issn = {0959-3780},
  doi = {10.1016/j.gloenvcha.2020.102194},
  abstract = {Prior to the COVID-19 crisis, global air transport demand was expected to triple between 2020 and 2050. The pandemic, which reduced global air travel significantly, provides an opportunity to discuss the scale, distribution and growth of aviation until 2018, also with a view to consider the climate change implications of a return to volume growth. Industry statistics, data provided by supranational organizations, and national surveys are evaluated to develop a pre-pandemic understanding of air transport demand at global, regional, national and individual scales. Results suggest that the share of the world's population travelling by air in 2018 was 11\%, with at most 4\% taking international flights. Data also supports that a minor share of air travelers is responsible for a large share of warming: The percentile of the most frequent fliers \textendash{} at most 1\% of the world population - likely accounts for more than half of the total emissions from passenger air travel. Individual users of private aircraft can contribute to emissions of up to 7,500~t CO2 per year. Findings are specifically relevant with regard to the insight that a large share of global aviation emissions is not covered by policy agreements.},
  language = {en},
  keywords = {Aviation,Climate policy,CORSIA,Emission equity,Emission gap,Paris Agreement},
  file = {/Users/milan/Zotero/storage/7W254BGC/Gössling and Humpe - 2020 - The global scale, distribution and growth of aviat.pdf;/Users/milan/Zotero/storage/V4MJFUCH/S0959378020307779.html}
}

@article{Gossling2020a,
  title = {Risks, Resilience, and Pathways to Sustainable Aviation: {{A COVID}}-19 Perspective},
  shorttitle = {Risks, Resilience, and Pathways to Sustainable Aviation},
  author = {G{\"o}ssling, Stefan},
  year = {2020},
  month = oct,
  journal = {Journal of Air Transport Management},
  volume = {89},
  pages = {101933},
  issn = {0969-6997},
  doi = {10.1016/j.jairtraman.2020.101933},
  abstract = {This paper discusses the COVID-19 pandemic as an opportunity to reconsider the foundations of the global aviation system. There is much evidence that air transport creates opportunities as well as risks. While the former accrue to businesses and individuals, risks are imposed on society. Pandemics, in which aviation has a role as a vector of pathogen distribution, as well as the sector's contribution to climate change are examples of long-standing negative externalities that continue to be ignored in assessments of aviation's economic performance and societal importance. As commercial aviation has shown limited economic resilience throughout its history, this short paper questions whether a return to business-as-usual, supported by very significant State aid payments, is desirable. The volume growth model championed by industry and aviation proponents may have to be replaced with an alternative model of a slimmed air transport system that is economically less vulnerable and accounting for its environmental impacts.},
  language = {en},
  keywords = {Climate change,COVID-19,Resilience,Risk,State aid,Vulnerability},
  file = {/Users/milan/Zotero/storage/LTTPYCCI/Gössling - 2020 - Risks, resilience, and pathways to sustainable avi.pdf;/Users/milan/Zotero/storage/652TDGP8/S0969699720305160.html}
}

@article{Govett2017,
  title = {Parallelization and {{Performance}} of the {{NIM Weather Model}} on {{CPU}}, {{GPU}}, and {{MIC Processors}}},
  author = {Govett, Mark and Rosinski, Jim and Middlecoff, Jacques and Henderson, Tom and Lee, Jin and MacDonald, Alexander and Wang, Ning and Madden, Paul and Schramm, Julie and Duarte, Antonio},
  year = {2017},
  month = oct,
  journal = {Bulletin of the American Meteorological Society},
  volume = {98},
  number = {10},
  pages = {2201--2213},
  publisher = {{American Meteorological Society}},
  issn = {0003-0007, 1520-0477},
  doi = {10.1175/BAMS-D-15-00278.1},
  abstract = {{$<$}section class="abstract"{$><$}h2 class="abstractTitle text-title my-1" id="d118804581e162"{$>$}Abstract{$<$}/h2{$><$}p{$>$}The design and performance of the Non-Hydrostatic Icosahedral Model (NIM) global weather prediction model is described. NIM is a dynamical core designed to run on central processing unit (CPU), graphics processing unit (GPU), and Many Integrated Core (MIC) processors. It demonstrates efficient parallel performance and scalability to tens of thousands of compute nodes and has been an effective way to make comparisons between traditional CPU and emerging fine-grain processors. The design of the NIM also serves as a useful guide in the fine-grain parallelization of the finite volume cubed (FV3) model recently chosen by the National Weather Service (NWS) to become its next operational global weather prediction model.{$<$}/p{$><$}p{$>$}This paper describes the code structure and parallelization of NIM using standards-compliant open multiprocessing (OpenMP) and open accelerator (OpenACC) directives. NIM uses the directives to support a single, performance-portable code that runs on CPU, GPU, and MIC systems. Performance results are compared for five generations of computer chips including the recently released Intel Knights Landing and NVIDIA Pascal chips. Single and multinode performance and scalability is also shown, along with a cost\textendash benefit comparison based on vendor list prices.{$<$}/p{$><$}/section{$>$}},
  chapter = {Bulletin of the American Meteorological Society},
  language = {EN},
  file = {/Users/milan/Zotero/storage/SRL7LW3X/Govett et al. - 2017 - Parallelization and Performance of the NIM Weather.pdf;/Users/milan/Zotero/storage/ZPT4K4V4/bams-d-15-00278.1.html}
}

@article{GRASSBERGERt,
  title = {{{MEASURING THE STRANGENESS OF STRANGE ATTRACTORS}}},
  author = {GRASSBERGERt, Peter and Procaccia, Itamar},
  pages = {20},
  language = {en}
}

@article{Greatbatch,
  title = {The {{North Atlantic Oscillation}}},
  author = {Greatbatch, R J},
  pages = {30},
  abstract = {The North Atlantic Oscillation (NAO) is the most important mode of variability in the northern hemisphere (NH) atmospheric circulation. Put simply, the NAO measures the strength of the westerly winds blowing across the North Atlantic Ocean between 40\textdegree N and 60\textdegree N. The NAO is not a regional, North Atlantic phenomenon, however, but rather is hemispheric in extent. Based on 60 years of data from 1935 to 1995, Hurrell (1996) estimates that the NAO accounts for 31\% of the variance in hemispheric winter surface air temperature north of 20\textdegree N. The present article provides an overview of the NAO, its role in the atmospheric circulation, its close relationship to the Arctic Oscillation of Thompson and Wallace (1998), and its in\textasciimacron uence on the underlying North Atlantic Ocean. Some discussion is also given on the dynamics of the NAO, the possible role of ocean surface temperature, and recent evidence that the stratosphere plays an important role in modulating the NAO.},
  language = {en}
}

@article{Greatbatch2000,
  title = {Four-{{Gyre Circulation}} in a {{Barotropic Model}} with {{Double}}-{{Gyre Wind Forcing}}},
  author = {Greatbatch, Richard J and Nadiga, B T},
  year = {2000},
  journal = {JOURNAL OF PHYSICAL OCEANOGRAPHY},
  volume = {30},
  pages = {11},
  abstract = {Results from a barotropic vorticity equation model driven by symmetric, double-gyre wind forcing are described. The authors work in a regime in which the model reaches a state of turbulent equilibrium. The timeaverage of the statistically steady state exhibits a four-gyre structure, in contrast to the usual two gyres associated with symmetric double-gyre wind forcing. The four-gyre structure is found in model runs using either free-slip or superslip boundary conditions, and with either Laplacian or biharmonic mixing for the dissipation. It is shown that the vorticity budget of both the inner and outer gyres is dominated by a balance between the wind stress curl and the divergence of the eddy potential vorticity flux, with the explicit dissipation playing a much smaller role. The two inner gyres circulate in the same sense as the wind stress curl and are equilibriated, for the most part, by the eddy flux of potential vorticity. The outer gyres, on the other hand, circulate in the opposite sense to the wind stress curl and are driven by the eddy flux of potential vorticity. It is shown that the gross features of the time-averaged state can be reproduced by a parameterized model in which the divergent part of the potential vorticity flux is represented as a downgradient transfer, and a boundary condition of no normal flux of potential vorticity is applied along the model boundaries. In contrast to the eddy resolving model, the fourgyre structure in the parameterized model depends strongly on the choice of side boundary condition.},
  language = {en}
}

@article{Greatbatch2003,
  title = {The {{Non}}-{{Boussinesq Temporal Residual Mean}}},
  author = {Greatbatch, Richard J and Mcdougall, Trevor J},
  year = {2003},
  journal = {JOURNAL OF PHYSICAL OCEANOGRAPHY},
  volume = {33},
  pages = {9},
  abstract = {A method for modifying currently existing ocean models to make them non-Boussinesq has been advocated by McDougall, Greatbatch, and Lu and implemented in the Parallel Ocean Program model by Greatbatch et al. Here, theoretical justification is provided for combining the above modifications with the temporal residual mean (TRM) approach of McDougall and McIntosh. The TRM is a method for including the skew flux of tracers caused by the adiabatic stirring of mesoscale eddies in non-eddy-resolving ocean models. The paper provides the justification for simultaneously undertaking these two model improvements and the physical interpretation of the model variables in this situation.},
  language = {en}
}

@article{Greatbatch2006,
  title = {Influence of Assimilated Eddies on the Large-Scale Circulation in a Model of the Northwest {{Atlantic Ocean}}},
  author = {Greatbatch, Richard J. and Zhai, Xiaoming},
  year = {2006},
  journal = {Geophysical Research Letters},
  volume = {33},
  number = {2},
  pages = {L02614},
  issn = {0094-8276},
  doi = {10.1029/2005GL025139},
  language = {en}
}

@article{Greatbatch2010,
  title = {Ocean Eddy Momentum Fluxes at the Latitudes of the {{Gulf Stream}} and the {{Kuroshio}} Extensions as Revealed by Satellite Data},
  author = {Greatbatch, Richard John and Zhai, Xiaoming and Kohlmann, Jan-Dirk and Czeschel, Lars},
  year = {2010},
  month = jun,
  journal = {Ocean Dynamics},
  volume = {60},
  number = {3},
  pages = {617--628},
  issn = {1616-7341, 1616-7228},
  doi = {10.1007/s10236-010-0282-6},
  abstract = {Eddy momentum fluxes, i.e. Reynold stresses, are computed for the latitude bands of the Gulf Stream and Kuroshio extensions using 13 years of data from the merged satellite altimeter product of Le Traon et al. The spatial pattern and amplitude of the fluxes is remarkably similar to that found by Ducet and Le Traon using the 5 years of data that were available to them. In addition to updating the work of Ducet and Le Traon, we provide new insight into the role played by the underlying variable bottom topography, both for determining the structure of the eddy momentum fluxes seen in the satellite data and for influencing the way these fluxes feedback on the mean flow. While there is no clear evidence that eddies locally flux momentum into the eastward jets of the Gulf Stream and Kuroshio extensions, a clearer picture emerges after zonally integrating across each of the North Atlantic and North Pacific basins. We argue that the eddy momentum fluxes do indeed drive significant transport, a conclusion supported by preliminary results from a 3-D model calculation. We also present evidence that in the North Pacific, the Reynolds stresses are important for driving the recirculation gyres associated with the Kuroshio extension, taking advantage of new data from both observations and high-resolution model simulations.},
  language = {en}
}

@article{Greatbatch2010a,
  title = {Transport Driven by Eddy Momentum Fluxes in the {{Gulf Stream Extension}} Region: {{TRANSPORT OF THE GULF STREAM EXTENSION}}},
  shorttitle = {Transport Driven by Eddy Momentum Fluxes in the {{Gulf Stream Extension}} Region},
  author = {Greatbatch, R. J. and Zhai, X. and Claus, M. and Czeschel, L. and Rath, W.},
  year = {2010},
  month = dec,
  journal = {Geophysical Research Letters},
  volume = {37},
  number = {24},
  pages = {n/a-n/a},
  issn = {00948276},
  doi = {10.1029/2010GL045473},
  language = {en}
}

@article{Grewe2021,
  title = {Evaluating the Climate Impact of Aviation Emission Scenarios towards the {{Paris}} Agreement Including {{COVID}}-19 Effects},
  author = {Grewe, Volker and Gangoli Rao, Arvind and Gr{\"o}nstedt, Tomas and Xisto, Carlos and Linke, Florian and Melkert, Joris and Middel, Jan and Ohlenforst, Barbara and Blakey, Simon and Christie, Simon and Matthes, Sigrun and Dahlmann, Katrin},
  year = {2021},
  month = jun,
  journal = {Nature Communications},
  volume = {12},
  number = {1},
  pages = {3841},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-24091-y},
  abstract = {Aviation is an important contributor to the global economy, satisfying society's mobility needs. It contributes to climate change through CO2 and non-CO2 effects, including contrail-cirrus and ozone formation. There is currently significant interest in policies, regulations and research aiming to reduce aviation's climate impact. Here we model the effect of these measures on global warming and perform a bottom-up analysis of potential technical improvements, challenging the assumptions of the targets for the sector with a number of scenarios up to 2100. We show that although the emissions targets for aviation are in line with the overall goals of the Paris Agreement, there is a high likelihood that the climate impact of aviation will not meet these goals. Our assessment includes feasible technological advancements and the availability of sustainable aviation fuels. This conclusion is robust for several COVID-19 recovery scenarios, including changes in travel behaviour.},
  copyright = {2021 The Author(s)},
  language = {en},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Climate change;Climate-change mitigation;Environmental impact;Projection and prediction Subject\_term\_id: climate-change;climate-change-mitigation;environmental-impact;projection-and-prediction},
  file = {/Users/milan/Zotero/storage/BZRH9G7F/Grewe et al. - 2021 - Evaluating the climate impact of aviation emission.pdf;/Users/milan/Zotero/storage/QGAL3JB6/s41467-021-24091-y.html}
}

@article{Griffies2000,
  title = {Biharmonic {{Friction}} with a {{Smagorinsky}}-{{Like Viscosity}} for {{Use}} in {{Large}}-{{Scale Eddy}}-{{Permitting Ocean Models}}},
  author = {Griffies, Stephen M and Hallberg, Robert W},
  year = {2000},
  journal = {MONTHLY WEATHER REVIEW},
  volume = {128},
  pages = {12},
  abstract = {This paper discusses a numerical closure, motivated from the ideas of Smagorinsky, for use with a biharmonic operator. The result is a highly scale-selective, state-dependent friction operator for use in eddy-permitting geophysical fluid models. This friction should prove most useful for large-scale ocean models in which there are multiple regimes of geostrophic turbulence. Examples are provided from primitive equation geopotential and isopycnal-coordinate ocean models.},
  language = {en}
}

@article{Griffies2000a,
  title = {Developments in Ocean Climate Modelling},
  author = {Griffies, Stephen M. and B{\"o}ning, Claus and Bryan, Frank O. and Chassignet, Eric P. and Gerdes, R{\"u}diger and Hasumi, Hiroyasu and Hirst, Anthony and Treguier, Anne-Marie and Webb, David},
  year = {2000},
  month = jan,
  journal = {Ocean Modelling},
  volume = {2},
  number = {3-4},
  pages = {123--192},
  issn = {14635003},
  doi = {10.1016/S1463-5003(00)00014-7},
  abstract = {This paper presents some research developments in primitive equation ocean models which could impact the ocean component of realistic global coupled climate models aimed at large-scale, low frequency climate simulations and predictions. It is written primarily to an audience of modellers concerned with the ocean component of climate models, although not necessarily experts in the design and implementation of ocean model algorithms. \'O 2001 Elsevier Science Ltd. All rights reserved.},
  language = {en}
}

@article{Grooms2017,
  title = {A Note on `{{Toward}} a Stochastic Parameterization of Ocean Mesoscale Eddies'},
  author = {Grooms, Ian and Zanna, Laure},
  year = {2017},
  month = may,
  journal = {Ocean Modelling},
  volume = {113},
  pages = {30--33},
  issn = {14635003},
  doi = {10.1016/j.ocemod.2017.03.007},
  abstract = {Porta Mana and Zanna (2014) recently proposed a subgrid-scale parameterization for eddy-permitting quasigeostrophic models. In this model the large-scale fluid is represented as a non-Newtonian viscoelastic medium, with a subgrid-stress closure that involves the Lagrangian derivative of large-scale quantities. This note derives this parameterization, including the nondimensional proportionality coefficient, using only two statistical assumptions: that the subgrid-scale term is locally homogeneous and decorrelates rapidly in space. The parameterization is then verified by comparing against eddy-resolving quasigeostrophic simulations, independently reproducing the results of Porta Mana and Zanna in a simpler model.},
  language = {en}
}

@article{Gulev2013,
  title = {North {{Atlantic Ocean}} Control on Surface Heat Flux on Multidecadal Timescales},
  author = {Gulev, Sergey K. and Latif, Mojib and Keenlyside, Noel and Park, Wonsun and Koltermann, Klaus Peter},
  year = {2013},
  month = jul,
  journal = {Nature},
  volume = {499},
  number = {7459},
  pages = {464--467},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature12268},
  language = {en}
}

@article{Gupta2015,
  title = {Deep {{Learning}} with {{Limited Numerical Precision}}},
  author = {Gupta, Suyog and Agrawal, Ankur and Gopalakrishnan, Kailash and Narayanan, Pritish},
  year = {2015},
  month = feb,
  journal = {arXiv:1502.02551 [cs, stat]},
  eprint = {1502.02551},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Training of large-scale deep neural networks is often constrained by the available computational resources. We study the effect of limited precision data representation and computation on neural network training. Within the context of low-precision fixed-point computations, we observe the rounding scheme to play a crucial role in determining the network's behavior during training. Our results show that deep networks can be trained using only 16-bit wide fixed-point number representation when using stochastic rounding, and incur little to no degradation in the classification accuracy. We also demonstrate an energy-efficient hardware accelerator that implements low-precision fixed-point arithmetic with stochastic rounding.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/milan/Zotero/storage/M2DCVEID/Gupta et al. - 2015 - Deep Learning with Limited Numerical Precision.pdf;/Users/milan/Zotero/storage/SZ8FYIJX/1502.html}
}

@misc{Gustafson2017,
  title = {Posit {{Arithmetic}}},
  author = {Gustafson, John},
  year = {2017},
  language = {English}
}

@article{Gustafson2017a,
  title = {Beating {{Floating Point}} at Its {{Own Game}}: {{Posit Arithmetic}}},
  author = {Gustafson, John L and Yonemoto, Isaac},
  year = {2017},
  journal = {Supercomputing Frontiers and Innovations},
  volume = {4},
  number = {2},
  pages = {16},
  abstract = {A new data type called a posit is designed as a direct drop-in replacement for IEEE Standard 754 floating-point numbers (floats). Unlike earlier forms of universal number (unum) arithmetic, posits do not require interval arithmetic or variable size operands; like floats, they round if an answer is inexact. However, they provide compelling advantages over floats, including larger dynamic range, higher accuracy, better closure, bitwise identical results across systems, simpler hardware, and simpler exception handling. Posits never overflow to infinity or underflow to zero, and ``Nota-Number'' (NaN) indicates an action instead of a bit pattern. A posit processing unit takes less circuitry than an IEEE float FPU. With lower power use and smaller silicon footprint, the posit operations per second (POPS) supported by a chip can be significantly higher than the FLOPS using similar hardware resources. GPU accelerators and Deep Learning processors, in particular, can do more per watt and per dollar with posits, yet deliver superior answer quality. A comprehensive series of benchmarks compares floats and posits for decimals of accuracy produced for a set precision. Low precision posits provide a better solution than ``approximate computing'' methods that try to tolerate decreased answer quality. High precision posits provide more correct decimals than floats of the same size; in some cases, a 32-bit posit may safely replace a 64-bit float. In other words, posits beat floats at their own game.},
  language = {en}
}

@article{Hadjimichael2013,
  title = {Strong {{Stability Preserving Explicit Runge}}--{{Kutta Methods}} of {{Maximal Effective Order}}},
  author = {Hadjimichael, Yiannis and Macdonald, Colin B. and Ketcheson, David I. and Verner, James H.},
  year = {2013},
  month = jan,
  journal = {SIAM Journal on Numerical Analysis},
  volume = {51},
  number = {4},
  pages = {2149--2165},
  issn = {0036-1429, 1095-7170},
  doi = {10.1137/120884201},
  abstract = {We apply the concept of effective order to strong stability preserving (SSP) explicit Runge\textendash Kutta methods. Relative to classical Runge\textendash Kutta methods, methods with an effective order of accuracy are designed to satisfy a relaxed set of order conditions but yield higher order accuracy when composed with special starting and stopping methods. We show that this allows the construction of four-stage SSP methods with effective order four (such methods cannot have classical order four). However, we also prove that effective order five methods\textemdash like classical order five methods\textemdash require the use of nonpositive weights and so cannot be SSP. By numerical optimization, we construct explicit SSP Runge\textendash Kutta methods up to effective order four and establish the optimality of many of them. Numerical experiments demonstrate the validity of these methods in practice.},
  language = {en},
  file = {/Users/milan/Zotero/storage/A3TEFP9I/Hadjimichael et al. - 2013 - Strong Stability Preserving Explicit Runge--Kutta .pdf}
}

@misc{Haiden2019,
  title = {Evaluation of {{ECMWF}} Forecasts, Including the 2019 Upgrade},
  author = {Haiden, Thomas and Janousek, Martin and Vitart, Fr{\'e}d{\'e}ric and Ferranti, Laura and Prates, Fernando and Prates, Fernando},
  year = {2019},
  publisher = {{ECMWF}},
  file = {/Users/milan/Zotero/storage/YGJD6W7R/19277-evaluation-ecmwf-forecasts-including-2019-upgrade.html}
}

@article{Hallberg1997,
  title = {Stable {{Split Time Stepping Schemes}} for {{Large}}-{{Scale Ocean Modeling}}},
  author = {Hallberg, Robert},
  year = {1997},
  month = jul,
  journal = {Journal of Computational Physics},
  volume = {135},
  number = {1},
  pages = {54--65},
  issn = {00219991},
  doi = {10.1006/jcph.1997.5734},
  language = {en}
}

@article{Hallberg2006,
  title = {The {{Role}} of {{Eddies}} in {{Determining}} the {{Structure}} and {{Response}} of the {{Wind}}-{{Driven Southern Hemisphere Overturning}}: {{Results}} from the {{Modeling Eddies}} in the {{Southern Ocean}} ({{MESO}}) {{Project}}},
  shorttitle = {The {{Role}} of {{Eddies}} in {{Determining}} the {{Structure}} and {{Response}} of the {{Wind}}-{{Driven Southern Hemisphere Overturning}}},
  author = {Hallberg, Robert and Gnanadesikan, Anand},
  year = {2006},
  month = dec,
  journal = {Journal of Physical Oceanography},
  volume = {36},
  number = {12},
  pages = {2232--2252},
  issn = {0022-3670, 1520-0485},
  doi = {10.1175/JPO2980.1},
  abstract = {The Modeling Eddies in the Southern Ocean (MESO) project uses numerical sensitivity studies to examine the role played by Southern Ocean winds and eddies in determining the density structure of the global ocean and the magnitude and structure of the global overturning circulation. A hemispheric isopycnal-coordinate ocean model (which avoids numerical diapycnal diffusion) with realistic geometry is run with idealized forcing at a range of resolutions from coarse (2\textdegree ) to eddy-permitting (1{$\fracslash$}6\textdegree ). A comparison of coarse resolutions with fine resolutions indicates that explicit eddies affect both the structure of the overturning and the response of the overturning to wind stress changes. While the presence of resolved eddies does not greatly affect the prevailing qualitative picture of the ocean circulation, it alters the overturning cells involving the Southern Ocean transformation of dense deep waters and light waters of subtropical origin into intermediate waters. With resolved eddies, the surface-to-intermediate water cell extends farther southward by hundreds of kilometers and the deep-to-intermediate cell draws on comparatively lighter deep waters. The overturning response to changes in the winds is also sensitive to the presence of eddies. In noneddying simulations, changing the Ekman transport produces comparable changes in the overturning, much of it involving transformation of deep waters and resembling the mean circulation. In the eddypermitting simulations, a significant fraction of the Ekman transport changes are compensated by eddyinduced transport drawing from lighter waters than does the mean overturning. This significant difference calls into question the ability of coarse-resolution ocean models to accurately capture the impact of changes in the Southern Ocean on the global ocean circulation.},
  language = {en}
}

@article{Hallberg2009,
  title = {Reconciling Estimates of the Free Surface Height in {{Lagrangian}} Vertical Coordinate Ocean Models with Mode-Split Time Stepping},
  author = {Hallberg, Robert and Adcroft, Alistair},
  year = {2009},
  month = jan,
  journal = {Ocean Modelling},
  volume = {29},
  number = {1},
  pages = {15--26},
  issn = {14635003},
  doi = {10.1016/j.ocemod.2009.02.008},
  abstract = {In ocean models that use a mode splitting algorithm for time-stepping the internal- and external-gravity modes, the external and internal solutions each can be used to provide an estimate of the free surface height evolution. In models with time-invariant vertical coordinate spacing, it is standard to force the internal solutions for the free surface height to agree with the external solution by specifying the appropriate vertically averaged velocities; because this is a linear problem, it is relatively straightforward. However, in Lagrangian vertical coordinate ocean models with potentially vanishing layers, nonlinear discretizations of the continuity equations must be used for each interior layer. This paper discusses the options for enforcing agreement between the internal and external estimates of the free surface height, along with the consequences of each choice, and suggests an optimal, essentially exact, approach. Published by Elsevier Ltd.},
  language = {en}
}

@article{Hallberg2013,
  title = {Using a Resolution Function to Regulate Parameterizations of Oceanic Mesoscale Eddy Effects},
  author = {Hallberg, Robert},
  year = {2013},
  month = dec,
  journal = {Ocean Modelling},
  volume = {72},
  pages = {92--103},
  issn = {14635003},
  doi = {10.1016/j.ocemod.2013.08.007},
  abstract = {Mesoscale eddies play a substantial role in the dynamics of the ocean, but the dominant length-scale of these eddies varies greatly with latitude, stratification and ocean depth. Global numerical ocean models with spatial resolutions ranging from 1\textdegree{} down to just a few kilometers include both regions where the dominant eddy scales are well resolved and regions where the model's resolution is too coarse for the eddies to form, and hence eddy effects need to be parameterized. However, common parameterizations of eddy effects via a Laplacian diffusion of the height of isopycnal surfaces (a Gent\textendash McWilliams diffusivity) are much more effective at suppressing resolved eddies than in replicating their effects. A variant of the Phillips model of baroclinic instability illustrates how eddy effects might be represented in ocean models. The ratio of the first baroclinic deformation radius to the horizontal grid spacing indicates where an ocean model could explicitly simulate eddy effects; a function of this ratio can be used to specify where eddy effects are parameterized and where they are explicitly modeled. One viable approach is to abruptly disable all the eddy parameterizations once the deformation radius is adequately resolved; at the discontinuity where the parameterization is disabled, isopycnal heights are locally flattened on the one side while eddies grow rapidly off of the enhanced slopes on the other side, such that the total parameterized and eddy fluxes vary continuously at the discontinuity in the diffusivity. This approach should work well with various specifications for the magnitude of the eddy diffusivities.},
  language = {en}
}

@article{Ham2013,
  title = {Sea Surface Temperature in the North Tropical {{Atlantic}} as a Trigger for {{El Ni\~no}}/{{Southern Oscillation}} Events},
  author = {Ham, Yoo-Geun and Kug, Jong-Seong and Park, Jong-Yeon and Jin, Fei-Fei},
  year = {2013},
  month = feb,
  journal = {Nature Geoscience},
  volume = {6},
  number = {2},
  pages = {112--116},
  issn = {1752-0894, 1752-0908},
  doi = {10.1038/ngeo1686},
  language = {en}
}

@article{Hammerling2012,
  title = {Global {{CO}} {\textsubscript{2}} Distributions over Land from the {{Greenhouse Gases Observing Satellite}} ({{GOSAT}}): {{GLOBAL ACOS GOSAT LEVEL}} 3 {{CO}} {\textsubscript{2}} {{MAPS}}},
  shorttitle = {Global {{CO}} {\textsubscript{2}} Distributions over Land from the {{Greenhouse Gases Observing Satellite}} ({{GOSAT}})},
  author = {Hammerling, Dorit M. and Michalak, Anna M. and O'Dell, Christopher and Kawa, S. Randolph},
  year = {2012},
  month = apr,
  journal = {Geophysical Research Letters},
  volume = {39},
  number = {8},
  pages = {n/a-n/a},
  issn = {00948276},
  doi = {10.1029/2012GL051203},
  language = {en}
}

@inproceedings{Hammerling2019,
  title = {A {{Collaborative Effort}} to {{Improve Lossy Compression Methods}} for {{Climate Data}}},
  booktitle = {2019 {{IEEE}}/{{ACM}} 5th {{International Workshop}} on {{Data Analysis}} and {{Reduction}} for {{Big Scientific Data}} ({{DRBSD}}-5)},
  author = {Hammerling, Dorit M. and Baker, Allison H. and Pinard, Alexander and Lindstrom, Peter},
  year = {2019},
  month = nov,
  pages = {16--22},
  doi = {10.1109/DRBSD-549595.2019.00008},
  abstract = {Climate model simulations produce large volumes of data, and reducing the storage burden with data compression is increasingly of interest to climate scientists. A key concern to the climate community, though, is ensuring that any data loss due to compression does not in any way affect their scientific analysis. For this reason, the climate community is taking a cautious approach to adopting lossy compression by carefully investigating the potential existence of artifacts due to compression in a wide variety of analysis settings. Spatio-temporal statistical analysis in particular can highlight compression-induced features that would go unnoticed by the standard metrics common to the data compression community. Communicating such findings to the algorithm developers in the context of a collaborative improvement cycle is one - in our view productive - way to foster trust within the climate community and pave the way for eventual adoption of lossy compression. In this work, we report on the initial results of a successful and mutually beneficial collaboration between the two communities that led to improvements in a well regarded compression algorithm and more effective compression of climate simulation data.},
  keywords = {Atmospheric modeling,climate community,climate scientists,climate simulation data,climatology,Collaboration,collaborative improvement cycle,compression algorithm,compression-induced features,Computational modeling,data compression,data compression community,Data models,Encoding,image coding,lossy compression methods,Meteorology,scientific analysis,spatio-temporal statistical analysis,Standards,statistical analysis},
  file = {/Users/milan/Zotero/storage/HI2SS9GM/8955110.html}
}

@article{Harris2020,
  title = {Array Programming with {{NumPy}}},
  author = {Harris, Charles R. and Millman, K. Jarrod and {van der Walt}, St{\'e}fan J. and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J. and Kern, Robert and Picus, Matti and Hoyer, Stephan and {van Kerkwijk}, Marten H. and Brett, Matthew and Haldane, Allan and {del R{\'i}o}, Jaime Fern{\'a}ndez and Wiebe, Mark and Peterson, Pearu and {G{\'e}rard-Marchant}, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E.},
  year = {2020},
  month = sep,
  journal = {Nature},
  volume = {585},
  number = {7825},
  pages = {357--362},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-020-2649-2},
  abstract = {Array programming provides a powerful, compact and expressive syntax for accessing, manipulating and operating on data in vectors, matrices and higher-dimensional arrays. NumPy is the primary array programming library for the Python language. It has an essential role in research analysis pipelines in fields as diverse as physics, chemistry, astronomy, geoscience, biology, psychology, materials science, engineering, finance and economics. For example, in astronomy, NumPy was an important part of the software stack used in the discovery of gravitational waves1 and in the first imaging of a black hole2. Here we review how a few fundamental array concepts lead to a simple and powerful programming paradigm for organizing, exploring and analysing scientific data. NumPy is the foundation upon which the scientific Python ecosystem is constructed. It is so pervasive that several projects, targeting audiences with specialized needs, have developed their own NumPy-like interfaces and array objects. Owing to its central position in the ecosystem, NumPy increasingly acts as an interoperability layer between such array computation libraries and, together with its application programming interface (API), provides a flexible framework to support the next decade of scientific and industrial analysis.},
  copyright = {2020 The Author(s)},
  language = {en},
  file = {/Users/milan/Zotero/storage/RFKDU5EF/Harris et al. - 2020 - Array programming with NumPy.pdf;/Users/milan/Zotero/storage/Y5Y7THE6/s41586-020-2649-2.html}
}

@article{Hatfield2017,
  title = {Improving {{Weather Forecast Skill}} through {{Reduced}}-{{Precision Data Assimilation}}},
  author = {Hatfield, Sam and Subramanian, Aneesh and Palmer, Tim and D{\"u}ben, Peter},
  year = {2017},
  month = nov,
  journal = {Monthly Weather Review},
  volume = {146},
  number = {1},
  pages = {49--62},
  publisher = {{American Meteorological Society}},
  issn = {0027-0644},
  doi = {10.1175/MWR-D-17-0132.1},
  abstract = {A new approach for improving the accuracy of data assimilation, by trading numerical precision for ensemble size, is introduced. Data assimilation is inherently uncertain because of the use of noisy observations and imperfect models. Thus, the larger rounding errors incurred from reducing precision may be within the tolerance of the system. Lower-precision arithmetic is cheaper, and so by reducing precision in ensemble data assimilation, computational resources can be redistributed toward, for example, a larger ensemble size. Because larger ensembles provide a better estimate of the underlying distribution and are less reliant on covariance inflation and localization, lowering precision could actually permit an improvement in the accuracy of weather forecasts. Here, this idea is tested on an ensemble data assimilation system comprising the Lorenz '96 toy atmospheric model and the ensemble square root filter. The system is run at double-, single-, and half-precision (the latter using an emulation tool), and the performance of each precision is measured through mean error statistics and rank histograms. The sensitivity of these results to the observation error and the length of the observation window are addressed. Then, by reinvesting the saved computational resources from reducing precision into the ensemble size, assimilation error can be reduced for (hypothetically) no extra cost. This results in increased forecasting skill, with respect to double-precision assimilation.},
  file = {/Users/milan/Zotero/storage/JH9Y4ICI/Hatfield et al. - 2017 - Improving Weather Forecast Skill through Reduced-P.pdf;/Users/milan/Zotero/storage/4Z2XMTPM/MWR-D-17-0132.html}
}

@article{Hatfield2018,
  title = {Choosing the {{Optimal Numerical Precision}} for {{Data Assimilation}} in the {{Presence}} of {{Model Error}}},
  author = {Hatfield, Sam and D{\"u}ben, Peter and Chantry, Matthew and Kondo, Keiichi and Miyoshi, Takemasa and Palmer, Tim},
  year = {2018},
  month = sep,
  journal = {Journal of Advances in Modeling Earth Systems},
  volume = {10},
  number = {9},
  pages = {2177--2191},
  issn = {1942-2466, 1942-2466},
  doi = {10.1029/2018MS001341},
  abstract = {The use of reduced numerical precision within an atmospheric data assimilation system is investigated. An atmospheric model with a spectral dynamical core is used to generate synthetic observations, which are then assimilated back into the same model using an ensemble Kalman filter. The effect on the analysis error of reducing precision from 64 bits to only 22 bits is measured and found to depend strongly on the degree of model uncertainty within the system. When the model used to generate the observations is identical to the model used to assimilate observations, the reduced-precision results suffer substantially. However, when model error is introduced by changing the diffusion scheme in the assimilation model or by using a higher-resolution model to generate observations, the difference in analysis quality between the two levels of precision is almost eliminated. Lower-precision arithmetic has a lower computational cost, so lowering precision could free up computational resources in operational data assimilation and allow an increase in ensemble size or grid resolution.},
  language = {en}
}

@inproceedings{Hatfield2019,
  title = {Accelerating {{High}}-{{Resolution Weather Models}} with {{Deep}}-{{Learning Hardware}}},
  booktitle = {Proceedings of the {{Platform}} for {{Advanced Scientific Computing Conference}}},
  author = {Hatfield, Sam and Chantry, Matthew and D{\"u}ben, Peter and Palmer, Tim},
  year = {2019},
  month = jun,
  series = {{{PASC}} '19},
  pages = {1--11},
  publisher = {{Association for Computing Machinery}},
  address = {{Zurich, Switzerland}},
  doi = {10.1145/3324989.3325711},
  abstract = {The next generation of weather and climate models will have an unprecedented level of resolution and model complexity, and running these models efficiently will require taking advantage of future supercomputers and heterogeneous hardware. In this paper, we investigate the use of mixed-precision hardware that supports floating-point operations at double-, single- and half-precision. In particular, we investigate the potential use of the NVIDIA Tensor Core, a mixed-precision matrix-matrix multiplier mainly developed for use in deep learning, to accelerate the calculation of the Legendre transforms in the Integrated Forecasting System (IFS), one of the leading global weather forecast models. In the IFS, the Legendre transform is one of the most expensive model components and dominates the computational cost for simulations at a very high resolution. We investigate the impact of mixed-precision arithmetic in IFS simulations of operational complexity through software emulation. Through a targeted but minimal use of double-precision arithmetic we are able to use either half-precision arithmetic or mixed half/single-precision arithmetic for almost all of the calculations in the Legendre transform without affecting forecast skill.},
  isbn = {978-1-4503-6770-7},
  keywords = {floating-point arithmetic,half-precision,Legendre transforms,Numerical weather prediction,spectral models,Tensor Core},
  file = {/Users/milan/Zotero/storage/MDBH3WB4/Hatfield et al. - 2019 - Accelerating High-Resolution Weather Models with D.pdf}
}

@article{Hatfield2020,
  title = {Single-{{Precision}} in the {{Tangent}}-{{Linear}} and {{Adjoint Models}} of {{Incremental 4D}}-{{Var}}},
  author = {Hatfield, Sam and McRae, Andrew and Palmer, Tim and D{\"u}ben, Peter},
  year = {2020},
  month = feb,
  journal = {Monthly Weather Review},
  volume = {148},
  number = {4},
  pages = {1541--1552},
  publisher = {{American Meteorological Society}},
  issn = {0027-0644},
  doi = {10.1175/MWR-D-19-0291.1},
  abstract = {The use of single-precision arithmetic in ECMWF's forecasting model gave a 40\% reduction in wall-clock time over double-precision, with no decrease in forecast quality. However, using reduced-precision in 4D-Var data assimilation is relatively unexplored and there are potential issues with using single-precision in the tangent-linear and adjoint models. Here, we present the results of reducing numerical precision in an incremental 4D-Var data assimilation scheme, with an underlying two-layer quasigeostrophic model. The minimizer used is the conjugate gradient method. We show how reducing precision increases the asymmetry between the tangent-linear and adjoint models. For ill-conditioned problems, this leads to a loss of orthogonality among the residuals of the conjugate gradient algorithm, which slows the convergence of the minimization procedure. However, we also show that a standard technique, reorthogonalization, eliminates these issues and therefore could allow the use of single-precision arithmetic. This work is carried out within ECMWF's data assimilation framework, the Object Oriented Prediction System.},
  file = {/Users/milan/Zotero/storage/CIJC4PMH/Hatfield et al. - 2020 - Single-Precision in the Tangent-Linear and Adjoint.pdf;/Users/milan/Zotero/storage/6QQMDAPV/MWR-D-19-0291.html}
}

@article{Haustein2017,
  title = {A Real-Time {{Global Warming Index}}},
  author = {Haustein, K. and Allen, M. R. and Forster, P. M. and Otto, F. E. L. and Mitchell, D. M. and Matthews, H. D. and Frame, D. J.},
  year = {2017},
  month = nov,
  journal = {Scientific Reports},
  volume = {7},
  number = {1},
  pages = {15417},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-017-14828-5},
  abstract = {We propose a simple real-time index of global human-induced warming and assess its robustness to uncertainties in climate forcing and short-term climate fluctuations. This index provides improved scientific context for temperature stabilisation targets and has the potential to decrease the volatility of climate policy. We quantify uncertainties arising from temperature observations, climate radiative forcings, internal variability and the model response. Our index and the associated rate of human-induced warming is compatible with a range of other more sophisticated methods to estimate the human contribution to observed global temperature change.},
  copyright = {2017 The Author(s)},
  language = {en},
  file = {/Users/milan/Zotero/storage/7TV8KD42/Haustein et al. - 2017 - A real-time Global Warming Index.pdf;/Users/milan/Zotero/storage/EMX3IEEU/s41598-017-14828-5.html}
}

@article{Hawkins2009,
  title = {The {{Potential}} to {{Narrow Uncertainty}} in {{Regional Climate Predictions}}},
  author = {Hawkins, Ed and Sutton, Rowan},
  year = {2009},
  month = aug,
  journal = {Bulletin of the American Meteorological Society},
  volume = {90},
  number = {8},
  pages = {1095--1108},
  issn = {0003-0007, 1520-0477},
  doi = {10.1175/2009BAMS2607.1},
  language = {en}
}

@article{Held1994,
  title = {A {{Proposal}} for the {{Intercomparison}} of the {{Dynamical Cores}} of {{Atmospheric General Circulation Models}}},
  author = {Held, Isaac M. and Suarez, Max J. and Held, Isaac M. and Suarez, Max J.},
  year = {1994},
  month = oct,
  journal = {Bulletin of the American Meteorological Society},
  publisher = {{American Meteorological Society}},
  issn = {10.1175/1520-0477(1994)075{$<$}1825:APFTIO{$>$}2.0.CO;2},
  doi = {10.1175/1520-0477-75.10.1825},
  abstract = {A benchmark calculation is proposed for evaluating the dynamical cores of atmospheric general circulation models independently of the physical parameterizations. The test focuses on the long-term s...},
  language = {en},
  file = {/Users/milan/Zotero/storage/HKIF4378/1520-0477(1994)0751825APFTIO2.0.html}
}

@article{Hersbach2000,
  title = {Decomposition of the {{Continuous Ranked Probability Score}} for {{Ensemble Prediction Systems}}},
  author = {Hersbach, Hans},
  year = {2000},
  month = oct,
  journal = {Weather and Forecasting},
  volume = {15},
  number = {5},
  pages = {559--570},
  publisher = {{American Meteorological Society}},
  issn = {1520-0434, 0882-8156},
  doi = {10.1175/1520-0434(2000)015<0559:DOTCRP>2.0.CO;2},
  abstract = {{$<$}section class="abstract"{$><$}h2 class="abstractTitle text-title my-1" id="d5e2"{$>$}Abstract{$<$}/h2{$><$}p{$>$}Some time ago, the continuous ranked probability score (CRPS) was proposed as a new verification tool for (probabilistic) forecast systems. Its focus is on the entire permissible range of a certain (weather) parameter. The CRPS can be seen as a ranked probability score with an infinite number of classes, each of zero width. Alternatively, it can be interpreted as the integral of the Brier score over all possible threshold values for the parameter under consideration. For a deterministic forecast system the CRPS reduces to the mean absolute error.{$<$}/p{$><$}p{$>$}In this paper it is shown that for an ensemble prediction system the CRPS can be decomposed into a reliability part and a resolution/uncertainty part, in a way that is similar to the decomposition of the Brier score. The reliability part of the CRPS is closely connected to the rank histogram of the ensemble, while the resolution/uncertainty part can be related to the average spread within the ensemble and the behavior of its outliers. The usefulness of such a decomposition is illustrated for the ensemble prediction system running at the European Centre for Medium-Range Weather Forecasts. The evaluation of the CRPS and its decomposition proposed in this paper can be extended to systems issuing continuous probability forecasts, by realizing that these can be interpreted as the limit of ensemble forecasts with an infinite number of members.{$<$}/p{$><$}/section{$>$}},
  chapter = {Weather and Forecasting},
  language = {EN},
  file = {/Users/milan/Zotero/storage/I9NYJX7G/Hersbach - 2000 - Decomposition of the Continuous Ranked Probability.pdf;/Users/milan/Zotero/storage/RRY46KP7/1520-0434_2000_015_0559_dotcrp_2_0_co_2.html}
}

@article{Herweijer2005,
  title = {Why Ocean Heat Transport Warms the Global Mean Climate},
  author = {Herweijer, Celine and Seager, Richard and Winton, Michael and Clement, Amy},
  year = {2005},
  month = aug,
  journal = {Tellus A},
  volume = {57},
  number = {4},
  pages = {662--675},
  issn = {0280-6495, 1600-0870},
  doi = {10.1111/j.1600-0870.2005.00121.x},
  abstract = {Observational and modelling evidence suggest that poleward ocean heat transport (OHT) can vary in response to both natural climate variability and greenhouse warming. Recent modelling studies have shown that increased OHT warms both the tropical and global mean climates. Using two different coupled climate models with mixed-layer oceans, with and without OHT, along with a coupled model with a fixed-current ocean component in which the currents are uniformly reduced and increased by 50\%, an attempt is made to explain why this may happen.},
  language = {en}
}

@article{Higham1993,
  title = {The {{Accuracy}} of {{Floating Point Summation}}},
  author = {Higham, Nicholas J.},
  year = {1993},
  month = jul,
  journal = {SIAM Journal on Scientific Computing},
  volume = {14},
  number = {4},
  pages = {783--799},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {1064-8275},
  doi = {10.1137/0914050},
  abstract = {The usual recursive summation technique is just one of several ways of computing the sum of n floating point numbers. Five summation methods and their variations are analyzed here. The accuracy of the methods is compared using rounding error analysis and numerical experiments. Four of the methods are shown to be special cases of a general class of methods, and an error analysis is given for this class. No one method is uniformly more accurate than the others, but some guidelines are given on the choice of method in particular cases.},
  file = {/Users/milan/Zotero/storage/WPXHTIL9/0914050.html}
}

@article{Higham2019,
  title = {Squeezing a {{Matrix}} into {{Half Precision}}, with an {{Application}} to {{Solving Linear Systems}}},
  author = {Higham, Nicholas J. and Pranesh, Srikara and Zounon, Mawussi},
  year = {2019},
  month = jan,
  journal = {SIAM Journal on Scientific Computing},
  volume = {41},
  number = {4},
  pages = {A2536-A2551},
  issn = {1064-8275, 1095-7197},
  doi = {10.1137/18M1229511},
  language = {en},
  file = {/Users/milan/Zotero/storage/AM2F8W6M/Higham et al. - 2019 - Squeezing a Matrix into Half Precision, with an Ap.pdf}
}

@article{Hirose2020,
  title = {Atmospheric Effects of {{Fukushima}} Nuclear Accident: {{A}} Review from a Sight of Atmospheric Monitoring},
  shorttitle = {Atmospheric Effects of {{Fukushima}} Nuclear Accident},
  author = {Hirose, Katsumi},
  year = {2020},
  month = jul,
  journal = {Journal of Environmental Radioactivity},
  volume = {218},
  pages = {106240},
  issn = {0265-931X},
  doi = {10.1016/j.jenvrad.2020.106240},
  abstract = {The Fukushima Daiichi Nuclear Power Plant (FDNPP) accident, which occurred in March 2011, has released large amounts of radionuclides (such as radioiodine and radiocesium) into the atmosphere, resulting in the contamination of terrestrial and marine environments. To assess radiological and environmental effects of the FDNPP-derived radionuclides, huge amounts of radioactive monitoring activities have been conducted to environmental samples. In this paper, we review results of atmospheric radioactivity monitoring for the FDNPP accident. Learning from atmospheric radioactivity monitoring of the FDNPP accident is as follows; 1. At the initial stage of accident, large spatiotemporal variability of emitted radionuclides near the FDNPP site occurred at short (less than 1~h) time scale and small (less than 10~km) space scale due to complicated emissions of radionuclides and variable flow of Fukushima radioactive plume, 2. Chemical form of FDNPP-derived radionuclides, in which a typical example is coexistence of 137Cs-bearing submicron particles and 137Cs-bearing large hot particles in the plume, is important to have better understanding of their atmospheric behaviors as do released mechanisms and their fate in environment, 3. Atmospheric effects of the FDNPP accident continue over 8 years, in which high activity levels of the FDNPP-derived 137Cs in surface air and deposition have continued at least until the end of 2018 owing to the post-accident release and resuspension because most of the FDNPP-derived 137Cs deposited on the ground surface still remains in the soil surface as a potential source of atmospheric 137Cs.},
  language = {en},
  file = {/Users/milan/Zotero/storage/GYU3FLRI/Hirose - 2020 - Atmospheric effects of Fukushima nuclear accident.pdf;/Users/milan/Zotero/storage/KNKSEZXX/S0265931X1930815X.html}
}

@article{Hodges,
  title = {{{DEPARTMENT OF COMMERCE}}},
  author = {Hodges, Luther H and Reichelderfer, W},
  journal = {MONTHLY WEATHER REVIEW},
  pages = {66},
  abstract = {An extended period numerical integration of a baroclinic priniitive equation model has been made for the simulation and the study of the dynamics of the atmosphere's general circulation. The solution corresponding t o external gravitational propagation is filtered b y requiring the vertically integrated divergence to vanish identically. Thc vertical structure permits as dependent variables the horizontal wind at two internal levels and a single temperature, wit8hthe static stability entering as a parameter.},
  language = {en}
}

@article{Hoefler2021,
  title = {Sparsity in {{Deep Learning}}: {{Pruning}} and Growth for Efficient Inference and Training in Neural Networks},
  shorttitle = {Sparsity in {{Deep Learning}}},
  author = {Hoefler, Torsten and Alistarh, Dan and {Ben-Nun}, Tal and Dryden, Nikoli and Peste, Alexandra},
  year = {2021},
  month = jan,
  journal = {arXiv:2102.00554 [cs]},
  eprint = {2102.00554},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The growing energy and performance costs of deep learning have driven the community to reduce the size of neural networks by selectively pruning components. Similarly to their biological counterparts, sparse networks generalize just as well, if not better than, the original dense networks. Sparsity can reduce the memory footprint of regular networks to fit mobile devices, as well as shorten training time for ever growing networks. In this paper, we survey prior work on sparsity in deep learning and provide an extensive tutorial of sparsification for both inference and training. We describe approaches to remove and add elements of neural networks, different training strategies to achieve model sparsity, and mechanisms to exploit sparsity in practice. Our work distills ideas from more than 300 research papers and provides guidance to practitioners who wish to utilize sparsity today, as well as to researchers whose goal is to push the frontier forward. We include the necessary background on mathematical methods in sparsification, describe phenomena such as early structure adaptation, the intricate relations between sparsity and the training process, and show techniques for achieving acceleration on real hardware. We also define a metric of pruned parameter efficiency that could serve as a baseline for comparison of different sparse networks. We close by speculating on how sparsity can improve future workloads and outline major open problems in the field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Hardware Architecture,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/milan/Zotero/storage/DYQ2F7AQ/Hoefler et al. - 2021 - Sparsity in Deep Learning Pruning and growth for .pdf;/Users/milan/Zotero/storage/CA3DSAGI/2102.html}
}

@article{Hoerling2001,
  title = {Tropical {{Origins}} for {{Recent North Atlantic Climate Change}}},
  author = {Hoerling, M. P.},
  year = {2001},
  month = apr,
  journal = {Science},
  volume = {292},
  number = {5514},
  pages = {90--92},
  issn = {00368075, 10959203},
  doi = {10.1126/science.1058582},
  language = {en}
}

@article{Hofbauer1978,
  title = {{$\beta$}-{{Shifts}} Have Unique Maximal Measure},
  author = {Hofbauer, Franz},
  year = {1978},
  month = sep,
  journal = {Monatshefte f\"ur Mathematik},
  volume = {85},
  number = {3},
  pages = {189--198},
  issn = {1436-5081},
  doi = {10.1007/BF01534862},
  abstract = {It is proved that {$\beta$}-shifts have unique measure with maximal entropy by constructing an isomorphism of the {$\beta$}-shift with another topological dynamical system and proving it for this system.},
  language = {en}
}

@book{Holton2004,
  title = {An Introduction to Dynamic Meteorology},
  author = {Holton, James R.},
  year = {2004},
  series = {International Geophysics Series},
  edition = {4th ed},
  number = {v. 88},
  publisher = {{Elsevier Academic Press}},
  address = {{Burlington, MA}},
  isbn = {978-0-12-354015-7 978-0-12-354016-4},
  language = {en},
  lccn = {QC880 .H65 2004},
  keywords = {Dynamic meteorology},
  annotation = {OCLC: 54400282}
}

@article{Hopkins2020,
  title = {Stochastic Rounding and Reduced-Precision Fixed-Point Arithmetic for Solving Neural Ordinary Differential Equations},
  author = {Hopkins, Michael and Mikaitis, Mantas and Lester, Dave R. and Furber, Steve},
  year = {2020},
  month = mar,
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {378},
  number = {2166},
  pages = {20190052},
  publisher = {{Royal Society}},
  doi = {10.1098/rsta.2019.0052},
  abstract = {Although double-precision floating-point arithmetic currently dominates high-performance computing, there is increasing interest in smaller and simpler arithmetic types. The main reasons are potential improvements in energy efficiency and memory footprint and bandwidth. However, simply switching to lower-precision types typically results in increased numerical errors. We investigate approaches to improving the accuracy of reduced-precision fixed-point arithmetic types, using examples in an important domain for numerical computation in neuroscience: the solution of ordinary differential equations (ODEs). The Izhikevich neuron model is used to demonstrate that rounding has an important role in producing accurate spike timings from explicit ODE solution algorithms. In particular, fixed-point arithmetic with stochastic rounding consistently results in smaller errors compared to single-precision floating-point and fixed-point arithmetic with round-to-nearest across a range of neuron behaviours and ODE solvers. A computationally much cheaper alternative is also investigated, inspired by the concept of dither that is a widely understood mechanism for providing resolution below the least significant bit in digital signal processing. These results will have implications for the solution of ODEs in other subject areas, and should also be directly relevant to the huge range of practical problems that are represented by partial differential equations.This article is part of a discussion meeting issue `Numerical algorithms for high-performance computational science'.},
  file = {/Users/milan/Zotero/storage/T8HIKUSG/Hopkins et al. - 2020 - Stochastic rounding and reduced-precision fixed-po.pdf;/Users/milan/Zotero/storage/MNKDL738/rsta.2019.html}
}

@article{Hopkins2020a,
  title = {Stochastic Rounding and Reduced-Precision Fixed-Point Arithmetic for Solving Neural Ordinary Differential Equations},
  author = {Hopkins, Michael and Mikaitis, Mantas and Lester, Dave R. and Furber, Steve},
  year = {2020},
  month = mar,
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {378},
  number = {2166},
  pages = {20190052},
  publisher = {{Royal Society}},
  doi = {10.1098/rsta.2019.0052},
  abstract = {Although double-precision floating-point arithmetic currently dominates high-performance computing, there is increasing interest in smaller and simpler arithmetic types. The main reasons are potential improvements in energy efficiency and memory footprint and bandwidth. However, simply switching to lower-precision types typically results in increased numerical errors. We investigate approaches to improving the accuracy of reduced-precision fixed-point arithmetic types, using examples in an important domain for numerical computation in neuroscience: the solution of ordinary differential equations (ODEs). The Izhikevich neuron model is used to demonstrate that rounding has an important role in producing accurate spike timings from explicit ODE solution algorithms. In particular, fixed-point arithmetic with stochastic rounding consistently results in smaller errors compared to single-precision floating-point and fixed-point arithmetic with round-to-nearest across a range of neuron behaviours and ODE solvers. A computationally much cheaper alternative is also investigated, inspired by the concept of dither that is a widely understood mechanism for providing resolution below the least significant bit in digital signal processing. These results will have implications for the solution of ODEs in other subject areas, and should also be directly relevant to the huge range of practical problems that are represented by partial differential equations.This article is part of a discussion meeting issue `Numerical algorithms for high-performance computational science'.},
  file = {/Users/milan/Zotero/storage/I62FBGZQ/Hopkins et al. - 2020 - Stochastic rounding and reduced-precision fixed-po.pdf;/Users/milan/Zotero/storage/4DM7KHMD/rsta.2019.html}
}

@inproceedings{Hubbe2013,
  title = {Evaluating {{Lossy Compression}} on {{Climate Data}}},
  booktitle = {Supercomputing},
  author = {H{\"u}bbe, Nathanael and Wegener, Al and Kunkel, Julian Martin and Ling, Yi and Ludwig, Thomas},
  editor = {Kunkel, Julian Martin and Ludwig, Thomas and Meuer, Hans Werner},
  year = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {343--356},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-38750-0_26},
  abstract = {While the amount of data used by today's high-performance computing (HPC) codes is huge, HPC users have not broadly adopted data compression techniques, apparently because of a fear that compression will either unacceptably degrade data quality or that compression will be too slow to be worth the effort. In this paper, we examine the effects of three lossy compression methods (GRIB2 encoding, GRIB2 using JPEG 2000 and LZMA, and the commercial Samplify APAX algorithm) on decompressed data quality, compression ratio, and processing time. A careful evaluation of selected lossy and lossless compression methods is conducted, assessing their influence on data quality, storage requirements and performance. The differences between input and decoded datasets are described and compared for the GRIB2 and APAX compression methods. Performance is measured using the compressed file sizes and the time spent on compression and decompression. Test data consists both of 9 synthetic data exposing compression behavior and 123 climate variables output from a climate model. The benefits of lossy compression for HPC systems are described and are related to our findings on data quality.},
  isbn = {978-3-642-38750-0},
  language = {en},
  keywords = {APAX,Data Compression,GRIB2,JPEG 2000},
  file = {/Users/milan/Zotero/storage/R4Z2DTPS/Hübbe et al. - 2013 - Evaluating Lossy Compression on Climate Data.pdf}
}

@article{Huffman1952,
  title = {A {{Method}} for the {{Construction}} of {{Minimum}}-{{Redundancy Codes}}},
  author = {Huffman, David A.},
  year = {1952},
  month = sep,
  journal = {Proceedings of the IRE},
  volume = {40},
  number = {9},
  pages = {1098--1101},
  issn = {2162-6634},
  doi = {10.1109/JRPROC.1952.273898},
  abstract = {An optimum method of coding an ensemble of messages consisting of a finite number of members is developed. A minimum-redundancy code is one constructed in such a way that the average number of coding digits per message is minimized.},
  keywords = {Transmitters},
  file = {/Users/milan/Zotero/storage/8XLA4EGF/4051119.html}
}

@article{Hurrell1995,
  title = {Decadal {{Trends}} in the {{North Atlantic Oscillation}}: {{Regional Temperatures}} and {{Precipitation}}},
  shorttitle = {Decadal {{Trends}} in the {{North Atlantic Oscillation}}},
  author = {Hurrell, J. W.},
  year = {1995},
  month = aug,
  journal = {Science},
  volume = {269},
  number = {5224},
  pages = {676--679},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.269.5224.676},
  language = {en}
}

@article{Hurrell2010,
  title = {North {{Atlantic}} Climate Variability: {{The}} Role of the {{North Atlantic Oscillation}}},
  author = {Hurrell, James W and Deser, Clara},
  year = {2010},
  journal = {Journal of Marine Systems},
  pages = {14},
  abstract = {Marine ecosystems are undergoing rapid change at local and global scales. To understand these changes, including the relative roles of natural variability and anthropogenic effects, and to predict the future state of marine ecosystems requires quantitative understanding of the physics, biogeochemistry and ecology of oceanic systems at mechanistic levels. Central to this understanding is the role played by dominant patterns or ``modes'' of atmospheric and oceanic variability, which orchestrate coherent variations in climate over large regions with profound impacts on ecosystems. We review the spatial structure of extratropical climate variability over the Northern Hemisphere and, specifically, focus on modes of climate variability over the extratropical North Atlantic.},
  language = {en}
}

@techreport{IEA2020,
  title = {Energy {{Technology Perspectives}} 2020 \textendash{} {{Analysis}}},
  author = {IEA},
  year = {2020},
  month = sep,
  institution = {{International Energy Agency}},
  language = {en-GB},
  file = {/Users/milan/Zotero/storage/3J3XANVV/energy-technology-perspectives-2020.html}
}

@article{IEEE1985,
  title = {{{IEEE Standard}} for {{Binary Floating}}-{{Point Arithmetic}}},
  author = {IEEE},
  year = {1985},
  month = oct,
  journal = {ANSI/IEEE Std 754-1985},
  pages = {1--20},
  doi = {10.1109/IEEESTD.1985.82928},
  abstract = {A family of commercially feasible ways for new systems to perform binary floating-point arithmetic is defined. This standard specifies basic and extended floating-point number formats; add, subtract, multiply, divide, square root, remainder, and compare operations; conversions between integer and floating-point formats; conversions between different floating-point formats; conversions between basic-format floating-point numbers and decimal strings; and floating-point exceptions and their handling, including nonnumbers.},
  keywords = {binary,binary floating point arithmetic,digital arithmetic,Digital arithmetic,Floating-point arithmetic,Floating-Point Working Group,IEEE Computer Society,IEEE standard,Microprocessor Standards Subcommittee,standards,Standards,Standards Committee},
  file = {/Users/milan/Zotero/storage/MC3WQU4C/1985 - IEEE Standard for Binary Floating-Point Arithmetic.pdf;/Users/milan/Zotero/storage/89BP6ILG/30711.html}
}

@article{IEEE2008,
  title = {{{IEEE Standard}} for {{Floating}}-{{Point Arithmetic}}},
  author = {IEEE},
  year = {2008},
  month = aug,
  journal = {IEEE Std 754-2008},
  pages = {1--70},
  doi = {10.1109/IEEESTD.2008.4610935},
  abstract = {This standard specifies interchange and arithmetic formats and methods for binary and decimal floating-point arithmetic in computer programming environments. This standard specifies exception conditions and their default handling. An implementation of a floating-point system conforming to this standard may be realized entirely in software, entirely in hardware, or in any combination of software and hardware. For operations specified in the normative part of this standard, numerical results and exceptions are uniquely determined by the values of the input data, sequence of operations, and destination formats, all under user control.},
  keywords = {754-2008,arithmetic,binary,computer,decimal,exponent,floating-point,Floating-point arithmetic,format,Hardware,IEEE Standards,interchange,Microprocessors,NaN,number,rounding,significand,Software,subnormal,Trademarks},
  file = {/Users/milan/Zotero/storage/VXZWD7IE/4610935.html}
}

@article{Inman2008,
  title = {Carbon Is Forever},
  author = {Inman, Mason},
  year = {2008},
  month = dec,
  journal = {Nature Climate Change},
  volume = {1},
  number = {812},
  pages = {156--158},
  publisher = {{Nature Publishing Group}},
  issn = {1758-6798},
  doi = {10.1038/climate.2008.122},
  abstract = {Carbon dioxide emissions and their associated warming could linger for millennia, according to some climate scientists. Mason Inman looks at why the fallout from burning fossil fuels could last far longer than expected.},
  copyright = {2008 Nature Publishing Group},
  language = {en},
  file = {/Users/milan/Zotero/storage/HX9TIQB7/Inman - 2008 - Carbon is forever.pdf;/Users/milan/Zotero/storage/P2FXRSGD/climate.2008.html}
}

@article{Inness2019,
  title = {The {{CAMS}} Reanalysis of Atmospheric Composition},
  author = {Inness, Antje and Ades, Melanie and {Agust{\'i}-Panareda}, Anna and Barr{\'e}, J{\'e}r{\^o}me and Benedictow, Anna and Blechschmidt, Anne-Marlene and Dominguez, Juan Jose and Engelen, Richard and Eskes, Henk and Flemming, Johannes and Huijnen, Vincent and Jones, Luke and Kipling, Zak and Massart, Sebastien and Parrington, Mark and Peuch, Vincent-Henri and Razinger, Miha and Remy, Samuel and Schulz, Michael and Suttie, Martin},
  year = {2019},
  month = mar,
  journal = {Atmospheric Chemistry and Physics},
  volume = {19},
  number = {6},
  pages = {3515--3556},
  publisher = {{Copernicus GmbH}},
  issn = {1680-7316},
  doi = {10.5194/acp-19-3515-2019},
  abstract = {{$<$}p{$><$}strong class="journal-contentHeaderColor"{$>$}Abstract.{$<$}/strong{$>$} The Copernicus Atmosphere Monitoring Service (CAMS) reanalysis is the latest global reanalysis dataset of atmospheric composition produced by the European Centre for Medium-Range Weather Forecasts (ECMWF), consisting of three-dimensional time-consistent atmospheric composition fields, including aerosols and chemical species. The dataset currently covers the period 2003\textendash 2016 and will be extended in the future by adding 1 year each year. A reanalysis for greenhouse gases is being produced separately. The CAMS reanalysis builds on the experience gained during the production of the earlier Monitoring Atmospheric Composition and Climate (MACC) reanalysis and CAMS interim reanalysis. Satellite retrievals of total column CO; tropospheric column NO\textsubscript{2}; aerosol optical depth (AOD); and total column, partial column and profile ozone retrievals were assimilated for the CAMS reanalysis with ECMWF's Integrated Forecasting System. The new reanalysis has an increased horizontal resolution of about 80\&thinsp;km and provides more chemical species at a better temporal resolution (3-hourly analysis fields, 3-hourly forecast fields and hourly surface forecast fields) than the previously produced CAMS interim reanalysis. The CAMS reanalysis has smaller biases compared with most of the independent ozone, carbon monoxide, nitrogen dioxide and aerosol optical depth observations used for validation in this paper than the previous two reanalyses and is much improved and more consistent in time, especially compared to the MACC reanalysis. The CAMS reanalysis is a dataset that can be used to compute climatologies, study trends, evaluate models, benchmark other reanalyses or serve as boundary conditions for regional models for past periods.{$<$}/p{$>$}},
  language = {English},
  file = {/Users/milan/Zotero/storage/S4WVIV93/Inness et al. - 2019 - The CAMS reanalysis of atmospheric composition.pdf;/Users/milan/Zotero/storage/PMEPTEUS/2019.html}
}

@book{IntergovernmentalPanelonClimateChange2015,
  title = {Climate Change 2014: Synthesis Report},
  shorttitle = {Climate Change 2014},
  editor = {{Intergovernmental Panel on Climate Change}},
  year = {2015},
  publisher = {{Intergovernmental Panel on Climate Change}},
  address = {{Geneva, Switzerland}},
  isbn = {978-92-9169-143-2},
  language = {en},
  annotation = {OCLC: 914851124},
  file = {/Users/milan/Zotero/storage/HWDJF7D3/Pachauri et al. - 2015 - Climate change 2014 synthesis report.pdf}
}

@book{IntergovernmentalPanelonClimateChange2018,
  title = {Global Warming of 1.5\textdegree{{C}}},
  author = {{Intergovernmental Panel on Climate Change}},
  year = {2018},
  abstract = {"An IPCC special report on the impacts of global warming of 1.5 \textdegree C above pre-industrial levels and related global greenhouse gas emission pathways, in the context of strengthening the global response to the threat of climate change, sustainable development, and efforts to eradicate poverty."},
  language = {en},
  annotation = {OCLC: 1056192590},
  file = {/Users/milan/Zotero/storage/66HLH928/Intergovernmental Panel on Climate Change - 2018 - Global warming of 1.5°C.pdf}
}

@techreport{InternationalAirTransportAssociation2020,
  title = {{{IATA Annual Review}} 2020},
  author = {International Air Transport Association},
  year = {2020},
  pages = {56},
  language = {en},
  file = {/Users/milan/Zotero/storage/2YKU77JB/IATA Annual Review 2020.pdf}
}

@article{Jansen2014,
  title = {Parameterizing Subgrid-Scale Eddy Effects Using Energetically Consistent Backscatter},
  author = {Jansen, Malte F. and Held, Isaac M.},
  year = {2014},
  month = aug,
  journal = {Ocean Modelling},
  volume = {80},
  pages = {36--48},
  issn = {14635003},
  doi = {10.1016/j.ocemod.2014.06.002},
  abstract = {In the near future we expect the resolution of many IPCC-class ocean models to enter the ``eddy-permitting'' regime. At this resolution models can produce reasonable eddy-like disturbances, but can still not properly resolve geostrophic eddies at all relevant scales. Adequate parameterizations representing subgrid eddy effects are thus necessary. Most eddy-permitting models presently employ some kind of hyperviscosity, which is shown to cause a significant amount of energy dissipation. However, comparison to higher resolution simulations shows that only enstrophy, but almost no energy, should be dissipated below the grid-scale. As a result of the artificial energy sink associated with viscous parameterizations, the eddy fields in eddy permitting models are generally not energetic enough.},
  language = {en}
}

@article{Jansen2015,
  title = {Energy Budget-Based Backscatter in an Eddy Permitting Primitive Equation Model},
  author = {Jansen, Malte F. and Held, Isaac M. and Adcroft, Alistair and Hallberg, Robert},
  year = {2015},
  month = oct,
  journal = {Ocean Modelling},
  volume = {94},
  pages = {15--26},
  issn = {14635003},
  doi = {10.1016/j.ocemod.2015.07.015},
  abstract = {Increasing computational resources are starting to allow global ocean simulations at so-called ``eddypermitting'' resolutions, at which the largest mesoscale eddies can be resolved explicitly. However, an adequate parameterization of the interactions with the unresolved part of the eddy energy spectrum remains crucial. Hyperviscous closures, which are commonly applied in eddy-permitting ocean models, cause spurious energy dissipation at these resolutions, leading to low levels of eddy kinetic energy (EKE) and weak eddy induced transports. It has recently been proposed to counteract the spurious energy dissipation of hyperviscous closures by an additional forcing term, which represents ``backscatter'' of energy from the un-resolved scales to the resolved scales. This study proposes a parameterization of energy backscatter based on an explicit sub-grid EKE budget. Energy dissipated by hyperviscosity acting on the resolved flow is added to the sub-grid EKE, while a backscatter term transfers energy back from the sub-grid EKE to the resolved flow. The backscatter term is formulated deterministically via a negative viscosity, which returns energy at somewhat larger scales than the hyperviscous dissipation, thus ensuring dissipation of enstrophy. The parameterization is tested in an idealized configuration of a primitive equation ocean model, and is shown to significantly improve the solutions of simulations at typical eddy-permitting resolutions.},
  language = {en}
}

@article{Jansen2015a,
  title = {Parameterization of Eddy Fluxes Based on a Mesoscale Energy Budget},
  author = {Jansen, Malte F. and Adcroft, Alistair J. and Hallberg, Robert and Held, Isaac M.},
  year = {2015},
  month = aug,
  journal = {Ocean Modelling},
  volume = {92},
  pages = {28--41},
  issn = {14635003},
  doi = {10.1016/j.ocemod.2015.05.007},
  abstract = {It has recently been proposed to formulate eddy diffusivities in ocean models based on a mesoscale eddy kinetic energy (EKE) budget. Given an appropriate length scale, the mesoscale EKE can be used to estimate an eddy diffusivity based on mixing length theory. This paper discusses some of the open questions associated with the formulation of an EKE budget and mixing length, and proposes an improved energy budget-based parameterization for the mesoscale eddy diffusivity. A series of numerical simulations is performed, using an idealized flat-bottomed {$\beta$}-plane channel configuration with quadratic bottom drag. The results stress the importance of the mixing length formulation, as well as the formulation for the bottom signature of the mesoscale EKE, which is important in determining the rate of EKE dissipation. In the limit of vanishing planetary vorticity gradient, the mixing length is ultimately controlled by bottom drag, though the frictional arrest scale predicted by barotropic turbulence theory needs to be modified to account for the effects of baroclinicity. Any significant planetary vorticity gradient, {$\beta$}, is shown to suppress mixing, and limit the effective mixing length to the Rhines scale. While the EKE remains moderated by bottom friction, the bottom signature of EKE is shown to decrease as the appropriately non-dimensionalized friction increases, which considerably weakens the impact of changes in the bottom friction compared to barotropic turbulence. For moderate changes in the bottom-friction, eddy fluxes are thus reasonably well approximated by the scaling relation proposed by Held and Larichev (1996), which ignores the effect of bottom friction.},
  language = {en}
}

@article{Jeffress2017,
  title = {Bitwise Efficiency in Chaotic Models},
  author = {Jeffress, Stephen and D{\"u}ben, Peter and Palmer, Tim},
  year = {2017},
  month = sep,
  journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {473},
  number = {2205},
  pages = {20170144},
  publisher = {{Royal Society}},
  doi = {10.1098/rspa.2017.0144},
  abstract = {Motivated by the increasing energy consumption of supercomputing for weather and climate simulations, we introduce a framework for investigating the bit-level information efficiency of chaotic models. In comparison with previous explorations of inexactness in climate modelling, the proposed and tested information metric has three specific advantages: (i) it requires only a single high-precision time series; (ii) information does not grow indefinitely for decreasing time step; and (iii) information is more sensitive to the dynamics and uncertainties of the model rather than to the implementation details. We demonstrate the notion of bit-level information efficiency in two of Edward Lorenz's prototypical chaotic models: Lorenz 1963 (L63) and Lorenz 1996 (L96). Although L63 is typically integrated in 64-bit `double' floating point precision, we show that only 16 bits have significant information content, given an initial condition uncertainty of approximately 1\% of the size of the attractor. This result is sensitive to the size of the uncertainty but not to the time step of the model. We then apply the metric to the L96 model and find that a 16-bit scaled integer model would suffice given the uncertainty of the unresolved sub-grid-scale dynamics. We then show that, by dedicating computational resources to spatial resolution rather than numeric precision in a field programmable gate array (FPGA), we see up to 28.6\% improvement in forecast accuracy, an approximately fivefold reduction in the number of logical computing elements required and an approximately 10-fold reduction in energy consumed by the FPGA, for the L96 model.},
  file = {/Users/milan/Zotero/storage/TTDU7IYM/Jeffress et al. - 2017 - Bitwise efficiency in chaotic models.pdf;/Users/milan/Zotero/storage/HNEML9U6/rspa.2017.html}
}

@article{Jenks1971,
  title = {Error on {{Choroplethic Maps}}: {{Definition}}, {{Measurement}}, {{Reduction}}},
  shorttitle = {Error on {{Choroplethic Maps}}},
  author = {Jenks, George F. and Caspall, Fred C.},
  year = {1971},
  journal = {Annals of the Association of American Geographers},
  volume = {61},
  number = {2},
  pages = {217--244},
  issn = {1467-8306},
  doi = {10.1111/j.1467-8306.1971.tb00779.x},
  abstract = {Communication, whether oral, written, or graphic depends upon the ability of one individual to transfer information to another. In this essay we have tried to illustrate the fact that error in choroplethic mapping inhibits the transfer of information and that there are methods for improving this type of map as a communicative tool. We have done this by first defining overview, tabular, and boundary map uses. Second, techniques for the measurement of the error components of these three uses have been developed. Third, new reiterative and forcing manipulative techniques for choroplethic map data processing have been evolved. Lastly, the relationship between map accuracy and the information carrying capacity of a choroplethic map is set forth in hypothetical terms.},
  language = {en},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-8306.1971.tb00779.x},
  file = {/Users/milan/Zotero/storage/YZKTX3S4/j.1467-8306.1971.tb00779.html}
}

@article{Jezequel2008,
  title = {{{CADNA}}: A Library for Estimating Round-off Error Propagation},
  shorttitle = {{{CADNA}}},
  author = {J{\'e}z{\'e}quel, Fabienne and Chesneaux, Jean-Marie},
  year = {2008},
  month = jun,
  journal = {Computer Physics Communications},
  volume = {178},
  number = {12},
  pages = {933--955},
  issn = {0010-4655},
  doi = {10.1016/j.cpc.2008.02.003},
  abstract = {The CADNA library enables one to estimate round-off error propagation using a probabilistic approach. With CADNA the numerical quality of any simulation program can be controlled. Furthermore by detecting all the instabilities which may occur at run time, a numerical debugging of the user code can be performed. CADNA provides new numerical types on which round-off errors can be estimated. Slight modifications are required to control a code with CADNA, mainly changes in variable declarations, input and output. This paper describes the features of the CADNA library and shows how to interpret the information it provides concerning round-off error propagation in a code. Program summary Program title:CADNA Catalogue identifier:AEAT\_v1\_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEAT\_v1\_0.html Program obtainable from:CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions:Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.:53\,420 No. of bytes in distributed program, including test data, etc.:566\,495 Distribution format:tar.gz Programming language:Fortran Computer:PC running LINUX with an i686 or an ia64 processor, UNIX workstations including SUN, IBM Operating system:LINUX, UNIX Classification:4.14, 6.5, 20 Nature of problem:A simulation program which uses floating-point arithmetic generates round-off errors, due to the rounding performed at each assignment and at each arithmetic operation. Round-off error propagation may invalidate the result of a program. The CADNA library enables one to estimate round-off error propagation in any simulation program and to detect all numerical instabilities that may occur at run time. Solution method:The CADNA library [1] implements Discrete Stochastic Arithmetic [2\textendash 4] which is based on a probabilistic model of round-off errors. The program is run several times with a random rounding mode generating different results each time. From this set of results, CADNA estimates the number of exact significant digits in the result that would have been computed with standard floating-point arithmetic. Restrictions:CADNA requires a Fortran 90 (or newer) compiler. In the program to be linked with the CADNA library, round-off errors on complex variables cannot be estimated. Furthermore array functions such as product or sum must not be used. Only the arithmetic operators and the abs, min, max and sqrt functions can be used for arrays. Running time:The version of a code which uses CADNA runs at least three times slower than its floating-point version. This cost depends on the computer architecture and can be higher if the detection of numerical instabilities is enabled. In this case, the cost may be related to the number of instabilities detected. References:[1]The CADNA library, URL address: http://www.lip6.fr/cadna.[2]J.-M. Chesneaux, L'arithm\'etique Stochastique et le Logiciel CADNA, Habilitation \'a diriger des recherches, Universit\'e Pierre et Marie Curie, Paris, 1995.[3]J. Vignes, A stochastic arithmetic for reliable scientific computation, Math. Comput. Simulation 35 (1993) 233\textendash 261.[4]J. Vignes, Discrete stochastic arithmetic for validating results of numerical software, Numer. Algorithms 37 (2004) 377\textendash 390.},
  language = {en},
  keywords = {CADNA,CESTAC method,Discrete Stochastic Arithmetic,Floating-point arithmetic,Numerical validation,Round-off errors},
  file = {/Users/milan/Zotero/storage/U42PJ8CW/Jézéquel and Chesneaux - 2008 - CADNA a library for estimating round-off error pr.pdf}
}

@article{Johnson,
  title = {Rethinking Floating Point for Deep Learning},
  author = {Johnson, Jeff},
  pages = {8},
  abstract = {Reducing hardware overhead of neural networks for faster or lower power inference and training is an active area of research. Uniform quantization using integer multiply-add has been thoroughly investigated, which requires learning many quantization parameters, fine-tuning training or other prerequisites. Little effort is made to improve floating point relative to this baseline; it remains energy inefficient, and word size reduction yields drastic loss in needed dynamic range. We improve floating point to be more energy efficient than equivalent bit width integer hardware on a 28 nm ASIC process while retaining accuracy in 8 bits with a novel hybrid log multiply/linear add, Kulisch accumulation and tapered encodings from Gustafson's posit format. With no network retraining, and drop-in replacement of all math and float32 parameters via round-to-nearest-even only, this open-sourced 8-bit log float is within 0.9\% top-1 and 0.2\% top-5 accuracy of the original float32 ResNet-50 CNN model on ImageNet. Unlike int8 quantization, it is still a general purpose floating point arithmetic, interpretable out-of-the-box. Our 8/38-bit log float multiply-add is synthesized and power profiled at 28 nm at 0.96\texttimes{} the power and 1.12\texttimes{} the area of 8/32-bit integer multiply-add. In 16 bits, our log float multiply-add is 0.59\texttimes{} the power and 0.68\texttimes{} the area of IEEE 754 float16 fused multiply-add, maintaining the same signficand precision and dynamic range, proving useful for training ASICs as well.},
  language = {en},
  file = {/Users/milan/Zotero/storage/N4BUHYUQ/Johnson - Rethinking ﬂoating point for deep learning.pdf}
}

@article{Johnson2018,
  title = {Rethinking Floating Point for Deep Learning},
  author = {Johnson, Jeff},
  year = {2018},
  month = nov,
  journal = {arXiv:1811.01721 [cs]},
  eprint = {1811.01721},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Reducing hardware overhead of neural networks for faster or lower power inference and training is an active area of research. Uniform quantization using integer multiply-add has been thoroughly investigated, which requires learning many quantization parameters, fine-tuning training or other prerequisites. Little effort is made to improve floating point relative to this baseline; it remains energy inefficient, and word size reduction yields drastic loss in needed dynamic range. We improve floating point to be more energy efficient than equivalent bit width integer hardware on a 28 nm ASIC process while retaining accuracy in 8 bits with a novel hybrid log multiply/linear add, Kulisch accumulation and tapered encodings from Gustafson's posit format. With no network retraining, and drop-in replacement of all math and float32 parameters via round-to-nearest-even only, this open-sourced 8-bit log float is within 0.9\% top-1 and 0.2\% top-5 accuracy of the original float32 ResNet-50 CNN model on ImageNet. Unlike int8 quantization, it is still a general purpose floating point arithmetic, interpretable out-of-the-box. Our 8/38-bit log float multiply-add is synthesized and power profiled at 28 nm at 0.96\texttimes{} the power and 1.12\texttimes{} the area of 8/32-bit integer multiply-add. In 16 bits, our log float multiply-add is 0.59\texttimes{} the power and 0.68\texttimes{} the area of IEEE 754 float16 fused multiply-add, maintaining the same signficand precision and dynamic range, proving useful for training ASICs as well.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis}
}

@article{Johnson2020,
  title = {Efficient, Arbitrarily High Precision Hardware Logarithmic Arithmetic for Linear Algebra},
  author = {Johnson, Jeff},
  year = {2020},
  month = apr,
  journal = {arXiv:2004.09313 [cs, math]},
  eprint = {2004.09313},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {The logarithmic number system (LNS) is arguably not broadly used due to exponential circuit overheads for summation tables relative to arithmetic precision. Methods to reduce this overhead have been proposed, yet still yield designs with high chip area and power requirements. Use remains limited to lower precision or high multiply/add ratio cases, while much of linear algebra (near 1:1 multiply/add ratio) does not qualify. We present a dual-base approximate logarithmic arithmetic comparable to floating point in use, yet unlike LNS it is easily fully pipelined, extendable to arbitrary precision with \$O(n\^2)\$ overhead, and energy efficient at a 1:1 multiply/add ratio. Compared to float32 or float64 vector inner product with FMA, our design is respectively 2.3x and 4.6x more energy efficient in 7 nm CMOS. It depends on exp and log evaluation 5.4x and 3.2x more energy efficient, at 0.23x and 0.37x the chip area for equivalent accuracy versus standard hyperbolic CORDIC using shift-and-add and approximated ODE integration in the style of Revol and Yakoubsohn. This technique is a useful design alternative for low power, high precision hardened linear algebra in computer vision, graphics, computational photography and machine learning applications.},
  archiveprefix = {arXiv},
  keywords = {B.2.0,G.1.0,G.1.2,Mathematics - Numerical Analysis},
  file = {/Users/milan/Zotero/storage/8WUGNZ5M/Johnson - 2020 - Efficient, arbitrarily high precision hardware log.pdf;/Users/milan/Zotero/storage/YWAKYJGB/2004.html}
}

@article{Jones2021,
  title = {Gridded Fossil {{CO}} 2 Emissions and Related {{O}} 2 Combustion Consistent with National Inventories 1959\textendash 2018},
  author = {Jones, Matthew W. and Andrew, Robbie M. and Peters, Glen P. and {Janssens-Maenhout}, Greet and {De-Gol}, Anthony J. and Ciais, Philippe and Patra, Prabir K. and Chevallier, Frederic and Le Qu{\'e}r{\'e}, Corinne},
  year = {2021},
  month = jan,
  journal = {Scientific Data},
  volume = {8},
  number = {1},
  pages = {2},
  publisher = {{Nature Publishing Group}},
  issn = {2052-4463},
  doi = {10.1038/s41597-020-00779-6},
  abstract = {Quantification of CO2 fluxes at the Earth's surface is required to evaluate the causes and drivers of observed increases in atmospheric CO2 concentrations. Atmospheric inversion models disaggregate observed variations in atmospheric CO2 concentration to variability in CO2 emissions and sinks. They require prior constraints fossil CO2 emissions. Here we describe GCP-GridFED (version 2019.1), a gridded fossil emissions dataset that is consistent with the national CO2 emissions reported by the Global Carbon Project (GCP). GCP-GridFEDv2019.1 provides monthly fossil CO2 emissions estimates for the period 1959\textendash 2018 at a spatial resolution of 0.1\textdegree. Estimates are provided separately for oil, coal and natural gas, for mixed international bunker fuels, and for the calcination of limestone during cement production. GCP-GridFED also includes gridded estimates of O2 uptake based on oxidative ratios for oil, coal and natural gas. It will be updated annually and made available for atmospheric inversions contributing to GCP global carbon budget assessments, thus aligning the prior constraints on top-down fossil CO2 emissions with the bottom-up estimates compiled by the GCP.},
  copyright = {2021 The Author(s)},
  language = {en},
  file = {/Users/milan/Zotero/storage/EP23S3M4/Jones et al. - 2021 - Gridded fossil CO 2 emissions and related O 2 comb.pdf;/Users/milan/Zotero/storage/64V6EV49/s41597-020-00779-6.html}
}

@inproceedings{Jouppi2017,
  title = {In-{{Datacenter Performance Analysis}} of a {{Tensor Processing Unit}}},
  booktitle = {Proceedings of the 44th {{Annual International Symposium}} on {{Computer Architecture}}},
  author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
  year = {2017},
  month = jun,
  series = {{{ISCA}} '17},
  pages = {1--12},
  publisher = {{Association for Computing Machinery}},
  address = {{Toronto, ON, Canada}},
  doi = {10.1145/3079856.3080246},
  abstract = {Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC---called a Tensor Processing Unit (TPU) --- deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95\% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X -- 30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X -- 80X higher. Moreover, using the CPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU.},
  isbn = {978-1-4503-4892-8},
  keywords = {accelerator,CNN,deep learning,DNN,domain-specific architecture,GPU,LSTM,MLP,neural network,RNN,TensorFlow,TPU},
  file = {/Users/milan/Zotero/storage/V7Z7TZAM/Jouppi et al. - 2017 - In-Datacenter Performance Analysis of a Tensor Pro.pdf;/Users/milan/Zotero/storage/ZDX2MXAM/Jouppi et al. - 2017 - In-Datacenter Performance Analysis of a Tensor Pro.pdf}
}

@article{Jouppi2018,
  title = {A Domain-Specific Architecture for Deep Neural Networks},
  author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David},
  year = {2018},
  month = aug,
  journal = {Communications of the ACM},
  volume = {61},
  number = {9},
  pages = {50--59},
  issn = {0001-0782},
  doi = {10.1145/3154484},
  abstract = {Tensor processing units improve performance per watt of neural networks in Google datacenters by roughly 50x.},
  file = {/Users/milan/Zotero/storage/Z4NRNJF9/Jouppi et al. - 2018 - A domain-specific architecture for deep neural net.pdf}
}

@article{Jouppi2018a,
  title = {Motivation for and {{Evaluation}} of the {{First Tensor Processing Unit}}},
  author = {Jouppi, Norman and Young, Cliff and Patil, Nishant and Patterson, David},
  year = {2018},
  month = may,
  journal = {IEEE Micro},
  volume = {38},
  number = {3},
  pages = {10--19},
  issn = {1937-4143},
  doi = {10.1109/MM.2018.032271057},
  abstract = {The first-generation tensor processing unit (TPU) runs deep neural network (DNN) inference 15-30 times faster with 30-80 times better energy efficiency than contemporary CPUs and GPUs in similar semiconductor technologies. This domain-specific architecture (DSA) is a custom chip that has been deployed in Google datacenters since 2015, where it serves billions of people.},
  keywords = {Central Processing Unit,computer centres,coprocessors,Data centers,deep neural network,deep neural network inference,DNN,domain-specific architecture,energy efficiency,Energy efficiency,feedforward neural nets,first-generation tensor processing unit,Google datacenters,GPU,Graphics processing units,hardware,machine learning,microprocessor,Microprocessors,motivation,Neural networks,parallel architectures,Semiconductor devices,semiconductor technologies,Tensile stress,tensor processing unit,tensors,TPU},
  file = {/Users/milan/Zotero/storage/YVFJ4I7D/8358031.html}
}

@article{Jung2010,
  title = {Diagnosing the {{Origin}} of {{Extended}}-{{Range Forecast Errors}}},
  author = {Jung, T. and Miller, M. J. and Palmer, T. N.},
  year = {2010},
  month = jun,
  journal = {Monthly Weather Review},
  volume = {138},
  number = {6},
  pages = {2434--2446},
  issn = {0027-0644, 1520-0493},
  doi = {10.1175/2010MWR3255.1},
  abstract = {Experiments with the ECMWF model are carried out to study the influence that a correct representation of the lower boundary conditions, the tropical atmosphere, and the Northern Hemisphere stratosphere would have on extended-range forecast skill of the extratropical Northern Hemisphere troposphere during boreal winter. Generation of forecast errors during the course of the integration is artificially reduced by relaxing the ECMWF model toward the 40-yr ECMWF Re-Analysis (ERA-40) in certain regions. Prescribing rather than persisting sea surface temperature and sea ice fields leads to a modest forecast error reduction in the extended range, especially over the North Pacific and North America; no beneficial influence is found in the medium range. Relaxation of the tropical troposphere leads to reduced extended-range forecast errors especially over the North Pacific, North America, and the North Atlantic. It is shown that a better representation of the Madden\textendash Julian oscillation is of secondary importance for explaining the results of the tropical relaxation experiments. The influence from the tropical stratosphere is negligible. Relaxation of the Northern Hemisphere stratosphere leads to forecast error reduction primarily in high latitudes and over Europe. However, given the strong influence from the troposphere onto the Northern Hemisphere stratosphere it is argued that stratospherically forced experiments are very difficult to interpret in terms of their implications for extendedrange predictability of the tropospheric flow. The results are discussed in the context of future forecasting system development.},
  language = {en}
}

@article{Juricke,
  title = {Ocean Kinetic Energy Backscatter on Unstructured Grids},
  author = {Juricke, Stephan and Danilov, Sergey and Kutsenko, Anton and Oliver, Marcel},
  pages = {52},
  abstract = {We present a new energy backscatter parameterization for primitive equation ocean models at eddy-permitting resolution, specifically for unstructured grids. Traditional eddy parameterizations in terms of viscosity closures lead to excessive dissipation of kinetic energy when used with eddy-permitting meshes. Incorporating the backscatter parameterization into the FESOM2 ocean model improves the simulated mean flow and increases eddy kinetic energy to a level achieved by eddy-resolving meshes, leading to a more realistic kinetic energy cycle. The parameterization maintains a reservior of dissipated energy and reinjects this subgrid energy at larger scales at a controlled rate. It improves kinetic energy conservation and better maintains the inverse energy cascade. The separation between dissipation and backscatter scales is achieved by using different-order differential operators in combination with spatial smoothing. This ensures that the model remains numerically stable and energizes the flow on all spatial scales.},
  language = {en}
}

@article{Kahan1965,
  title = {Pracniques: Further Remarks on Reducing Truncation Errors},
  shorttitle = {Pracniques},
  author = {Kahan, W.},
  year = {1965},
  month = jan,
  journal = {Communications of the ACM},
  volume = {8},
  number = {1},
  pages = {40},
  issn = {0001-0782},
  doi = {10.1145/363707.363723}
}

@article{Kalamkar2019,
  title = {A {{Study}} of {{BFLOAT16}} for {{Deep Learning Training}}},
  author = {Kalamkar, Dhiraj and Mudigere, Dheevatsa and Mellempudi, Naveen and Das, Dipankar and Banerjee, Kunal and Avancha, Sasikanth and Vooturi, Dharma Teja and Jammalamadaka, Nataraj and Huang, Jianyu and Yuen, Hector and Yang, Jiyan and Park, Jongsoo and Heinecke, Alexander and Georganas, Evangelos and Srinivasan, Sudarshan and Kundu, Abhisek and Smelyanskiy, Misha and Kaul, Bharat and Dubey, Pradeep},
  year = {2019},
  month = jun,
  journal = {arXiv:1905.12322 [cs, stat]},
  eprint = {1905.12322},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {This paper presents the first comprehensive empirical study demonstrating the efficacy of the Brain Floating Point (BFLOAT16) half-precision format for Deep Learning training across image classification, speech recognition, language modeling, generative networks and industrial recommendation systems. BFLOAT16 is attractive for Deep Learning training for two reasons: the range of values it can represent is the same as that of IEEE 754 floating-point format (FP32) and conversion to/from FP32 is simple. Maintaining the same range as FP32 is important to ensure that no hyper-parameter tuning is required for convergence; e.g., IEEE 754 compliant half-precision floating point (FP16) requires hyper-parameter tuning. In this paper, we discuss the flow of tensors and various key operations in mixed precision training, and delve into details of operations, such as the rounding modes for converting FP32 tensors to BFLOAT16. We have implemented a method to emulate BFLOAT16 operations in Tensorflow, Caffe2, IntelCaffe, and Neon for our experiments. Our results show that deep learning training using BFLOAT16 tensors achieves the same state-of-the-art (SOTA) results across domains as FP32 tensors in the same number of iterations and with no changes to hyper-parameters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/milan/Zotero/storage/IBGRBTIN/Kalamkar et al. - 2019 - A Study of BFLOAT16 for Deep Learning Training.pdf;/Users/milan/Zotero/storage/V4DDGS7C/1905.html}
}

@article{Karcher2021,
  title = {Process-Oriented Analysis of Aircraft Soot-Cirrus Interactions Constrains the Climate Impact of Aviation},
  author = {K{\"a}rcher, Bernd and Mahrt, Fabian and Marcolli, Claudia},
  year = {2021},
  month = jun,
  journal = {Communications Earth \& Environment},
  volume = {2},
  number = {1},
  pages = {1--9},
  publisher = {{Nature Publishing Group}},
  issn = {2662-4435},
  doi = {10.1038/s43247-021-00175-x},
  abstract = {Fully accounting for the climate impact of aviation requires a process-level understanding of the impact of aircraft soot particle emissions on the formation of ice clouds. Assessing this impact with the help of global climate models remains elusive and direct observations are lacking. Here we use a high-resolution cirrus column model to investigate how aircraft-emitted soot particles, released after ice crystals sublimate at the end of the lifetime of contrails and contrail cirrus, perturb the formation of cirrus. By allying cloud simulations with a measurement-based description of soot-induced ice formation, we find that only a small fraction ({$<$}1\%) of the soot particles succeeds in forming cloud ice alongside homogeneous freezing of liquid aerosol droplets. Thus, soot-perturbed and homogeneously-formed cirrus fundamentally do not differ in optical depth. Our results imply that climate model estimates of global radiative forcing from interactions between aircraft soot and large-scale cirrus may be overestimates. The improved scientific understanding reported here provides a process-based underpinning for improved climate model parametrizations and targeted field observations.},
  copyright = {2021 The Author(s)},
  language = {en},
  file = {/Users/milan/Zotero/storage/JKQI4JI4/Kärcher et al. - 2021 - Process-oriented analysis of aircraft soot-cirrus .pdf;/Users/milan/Zotero/storage/IUD2QAQY/s43247-021-00175-x.html}
}

@article{Ketcheson2014,
  title = {Internal {{Error Propagation}} in {{Explicit Runge}}--{{Kutta Methods}}},
  author = {Ketcheson, David I. and L{\'o}czi, Lajos and Parsani, Matteo},
  year = {2014},
  month = jan,
  journal = {SIAM Journal on Numerical Analysis},
  volume = {52},
  number = {5},
  pages = {2227--2249},
  issn = {0036-1429, 1095-7170},
  doi = {10.1137/130936245},
  abstract = {In practical computation with Runge\textendash Kutta methods, the stage equations are not satisfied exactly, due to roundoff errors, algebraic solver errors, and so forth. We show by example that propagation of such errors within a single step can have catastrophic effects for otherwise practical and well-known methods. We perform a general analysis of internal error propagation, emphasizing that it depends significantly on how the method is implemented. We show that for a fixed method, essentially any set of internal stability polynomials can be obtained by modifying the implementation details. We provide bounds on the internal error amplification constants for some classes of methods with many stages, including strong stability preserving methods and extrapolation methods. These results are used to prove error bounds in the presence of roundoff or other internal errors.},
  language = {en},
  file = {/Users/milan/Zotero/storage/38LELDSR/Ketcheson et al. - 2014 - Internal Error Propagation in Explicit Runge--Kutt.pdf}
}

@article{Ketefian2011,
  title = {A Mass, Energy, Vorticity, and Potential Enstrophy Conserving Lateral Boundary Scheme for the Shallow Water Equations Using Piecewise Linear Boundary Approximations},
  author = {Ketefian, G.S. and Jacobson, M.Z.},
  year = {2011},
  month = apr,
  journal = {Journal of Computational Physics},
  volume = {230},
  number = {8},
  pages = {2751--2793},
  issn = {00219991},
  doi = {10.1016/j.jcp.2010.11.008},
  abstract = {A numerical scheme for treating fluid\textendash land boundaries in inviscid shallow water flows is derived that approximates boundary profiles with piecewise linear segments (shaved cells) while conserving the domain-summed mass, energy, vorticity, and potential enstrophy. The new scheme is a generalization of a previous scheme that also conserves these quantities but uses stairsteps to approximate boundary profiles. Numerical simulations are carried out demonstrating the conservation properties and accuracy of the piecewise linear boundary scheme (the PLS) for inviscid flows and comparing its performance with that of the stairstep scheme (the STS). It is found that while both schemes conserve all four domain-summed quantities, the PLS generates depth and velocity fields that are one-half to one order more accurate than those generated by the STS, and it generates vorticity and potential vorticity fields that are at least as accurate as those generated by the STS and often more accurate. The higher accuracy of the PLS is due to its ability to generate smoother flow fields near boundaries of arbitrary shape.},
  language = {en}
}

@article{Ketterer2016,
  title = {Mapping the {{Physiologically Equivalent Temperature}} in Urban Areas Using Artificial Neural Network},
  author = {Ketterer, Christine and Matzarakis, Andreas},
  year = {2016},
  month = jun,
  journal = {Landscape and Urban Planning},
  volume = {150},
  pages = {1--9},
  issn = {0169-2046},
  doi = {10.1016/j.landurbplan.2016.02.010},
  abstract = {The gap between point measurements made during a measurement campaign and the required discrete data of human thermal comfort in the form of maps could be overcome by statistical or numerical models. City planners usually demand thermal maps with a resolution below 50m. The required input data for the statistical models were meteorological data at high resolution as well as land use and land cover data including morphological data. Meteorological data were obtained through car traverses on a measuring campaign on hot summer days in July 2014. The chosen statistical approaches of stepwise multiple linear regression and artificial neural network were compared for the case study area Stuttgart, Germany. The Physiologically Equivalent Temperature (PET) was applied to analyse the human thermal conditions taking into account both the meteorological environment and the thermo-physiological parameters including the human energy balance. The polycentric and complex spatial distribution of heat stress and heat load is clearly visible in the created maps. One hot spot is the city centre and its surrounding residential neighbourhood, the other hot spot can be detected in Bad Cannstatt (easterly of the Neckar river valley), including industrial and residential areas. Thereby, the non-linear artificial neural network model delivers better results than the stepwise multiple linear regression model. Advantages of the artificial neural network arise from the possibility to reveal non-linear dependencies and interactions between the variables resulting in a better model fit.},
  language = {en},
  keywords = {Artificial neural network,Mapping,Multiple linear regression,Physiologically Equivalent Temperature,Stuttgart},
  file = {/Users/milan/Zotero/storage/Z2LSGXFM/S0169204616000256.html}
}

@article{Kleeman2011,
  title = {Information {{Theory}} and {{Dynamical System Predictability}}},
  author = {Kleeman, Richard},
  year = {2011},
  month = mar,
  journal = {Entropy},
  volume = {13},
  number = {3},
  pages = {612--649},
  publisher = {{Molecular Diversity Preservation International}},
  doi = {10.3390/e13030612},
  abstract = {Predicting the future state of a turbulent dynamical system such as the atmosphere has been recognized for several decades to be an essentially statistical undertaking. Uncertainties from a variety of sources are magnified by dynamical mechanisms and given sufficient time, compromise any prediction. In the last decade or so this process of uncertainty evolution has been studied using a variety of tools from information theory. These provide both a conceptually general view of the problem as well as a way of probing its non-linearity. Here we review these advances from both a theoretical and practical perspective. Connections with other theoretical areas such as statistical mechanics are emphasized. The importance of obtaining practical results for prediction also guides the development presented.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  language = {en},
  keywords = {information theory,predictability,statistical physics},
  file = {/Users/milan/Zotero/storage/J4W32LHE/Kleeman - 2011 - Information Theory and Dynamical System Predictabi.pdf;/Users/milan/Zotero/storage/DQM7E8S6/612.html}
}

@article{Kleeman2011a,
  title = {Information {{Theory}} and {{Dynamical System Predictability}}},
  author = {Kleeman, Richard},
  year = {2011},
  month = mar,
  journal = {Entropy},
  volume = {13},
  number = {3},
  pages = {612--649},
  issn = {1099-4300},
  doi = {10.3390/e13030612},
  abstract = {Predicting the future state of a turbulent dynamical system such as the atmosphere has been recognized for several decades to be an essentially statistical undertaking. Uncertainties from a variety of sources are magnified by dynamical mechanisms and given sufficient time, compromise any prediction. In the last decade or so this process of uncertainty evolution has been studied using a variety of tools from information theory. These provide both a conceptually general view of the problem as well as a way of probing its non-linearity. Here we review these advances from both a theoretical and practical perspective. Connections with other theoretical areas such as statistical mechanics are emphasized. The importance of obtaining practical results for prediction also guides the development presented.},
  language = {en}
}

@article{Klower2014,
  title = {Atlantic Meridional Overturning Circulation and the Prediction of {{North Atlantic}} Sea Surface Temperature},
  author = {Kl{\"o}wer, M. and Latif, M. and Ding, H. and Greatbatch, R.J. and Park, W.},
  year = {2014},
  month = nov,
  journal = {Earth and Planetary Science Letters},
  volume = {406},
  pages = {1--6},
  issn = {0012821X},
  doi = {10.1016/j.epsl.2014.09.001},
  abstract = {The Atlantic Meridional Overturning Circulation (AMOC), a major current system in the Atlantic Ocean, is thought to be an important driver of climate variability, both regionally and globally and on a large range of time scales from decadal to centennial and even longer. Measurements to monitor the AMOC strength have only started in 2004, which is too short to investigate its link to long-term climate variability. Here the surface heat flux-driven part of the AMOC during 1900\textendash 2010 is reconstructed from the history of the North Atlantic Oscillation, the most energetic mode of internal atmospheric variability in the Atlantic sector. The decadal variations of the AMOC obtained in that way are shown to precede the observed decadal variations in basin-wide North Atlantic sea surface temperature (SST), known as the Atlantic Multidecadal Oscillation (AMO) which strongly impacts societally important quantities such as Atlantic hurricane activity and Sahel rainfall. The future evolution of the AMO is forecast using the AMOC reconstructed up to 2010. The present warm phase of the AMO is predicted to continue until the end of the next decade, but with a negative tendency.},
  language = {en}
}

@article{Klower2018,
  title = {Energy Budget-Based Backscatter in a Shallow Water Model of a Double Gyre Basin},
  author = {Kl{\"o}wer, Milan and Jansen, Malte F. and Claus, Martin and Greatbatch, Richard J. and Thomsen, S{\"o}ren},
  year = {2018},
  month = dec,
  journal = {Ocean Modelling},
  volume = {132},
  pages = {1--11},
  issn = {1463-5003},
  doi = {10.1016/j.ocemod.2018.09.006},
  abstract = {The parameterization of sub-grid scale processes is one of the key challenges towards improved numerical simulations of the atmospheric and oceanic circulation. Numerical weather prediction models as well as climate models would benefit from more sophisticated turbulence closures that allow for less spurious dissipation at the grid-scale and consequently higher and more realistic levels of eddy kinetic energy (EKE). Recent studies propose to use a hyperviscous closure in combination with an additional deterministic forcing term as a negative viscosity to represent backscatter of energy from unresolved scales. The sub-grid EKE is introduced as an additional prognostic variable that is fed by dissipation at the grid scale, and enables recycling of EKE via the backscatter term at larger scales. This parameterization was previously shown to work well in zonally re-entrant channel configurations. Here, a generalization in the form of a Rossby number-dependent scaling for the strength of the backscatter is introduced to represent the emergence of a forward energy-cascade in unbalanced flows near the boundaries. We apply the parameterization to a shallow water model of a double gyre basin and provide evidence for its general applicability. In terms of mean state and variability, a low resolution model is considerably improved towards a high resolution control run at low additional computational cost.},
  language = {en},
  keywords = {Backscatter,Double gyre,Eddy parameterization,Eddy-permitting,Energy budget,Mesoscale,Negative viscosity,Shallow water model},
  file = {/Users/milan/Zotero/storage/299ZDMQY/Klöwer et al. - 2018 - Energy budget-based backscatter in a shallow water.pdf;/Users/milan/Zotero/storage/IBP2VLG8/S1463500318300301.html}
}

@misc{Klower2019,
  title = {{{SoftPosit}}.Jl -  {{A}} Posit Arithmetic Emulator},
  shorttitle = {Milankl/{{SoftPosit}}.Jl},
  author = {Kl{\"o}wer, Milan and Giordano, Mose},
  year = {2019},
  month = dec,
  doi = {10.5281/zenodo.3590291},
  howpublished = {Zenodo},
  file = {/Users/milan/Zotero/storage/U7UMAYDJ/3590291.html}
}

@inproceedings{Klower2019a,
  title = {Posits as an Alternative to Floats for Weather and Climate Models},
  booktitle = {Proceedings of the {{Conference}} for {{Next Generation Arithmetic}} 2019 on   - {{CoNGA}}'19},
  author = {Kl{\"o}wer, Milan and D{\"u}ben, Peter D. and Palmer, Tim N.},
  year = {2019},
  pages = {1--8},
  publisher = {{ACM Press}},
  address = {{Singapore, Singapore}},
  doi = {10.1145/3316279.3316281},
  abstract = {Posit numbers, a recently proposed alternative to floating-point numbers, claim to have smaller arithmetic rounding errors in many applications. By studying weather and climate models of low and medium complexity (the Lorenz system and a shallow water model) we present benefits of posits compared to floats at 16 bit. As a standardised posit processor does not exist yet, we emulate posit arithmetic on a conventional CPU. Using a shallow water model, forecasts based on 16-bit posits with 1 or 2 exponent bits are clearly more accurate than half precision floats. We therefore propose 16 bit with 2 exponent bits as a standard posit format, as its wide dynamic range of 32 orders of magnitude provides a great potential for many weather and climate models. Although the focus is on geophysical fluid simulations, the results are also meaningful and promising for reduced precision posit arithmetic in the wider field of computational fluid dynamics.},
  isbn = {978-1-4503-7139-1},
  language = {en}
}

@misc{Klower2019b,
  title = {Sonums - a Maximum Entropy Number Format},
  shorttitle = {Milankl/{{Sonums}}.Jl},
  author = {Kl{\"o}wer, Milan},
  year = {2019},
  month = nov,
  doi = {10.5281/zenodo.3531887},
  abstract = {Contains primitive types Sonum8 (8bit), Sonum16 (16bit) conversion to and from Float16, Float32, Float64 via binary tree search maximum-entropy training other training methods via setSonum8, setSonum16 functions arithmetic operations +,-,*,/ with the fillSonum8Tables, fillSonum16Tables sqrt and inv underflow mode true or false, but only no-overflow rounding mode Tested against the SoftPosit.jl library for Sonum8 emulating Posit8.},
  howpublished = {Zenodo},
  file = {/Users/milan/Zotero/storage/XFG7IEI4/3531887.html}
}

@misc{Klower2019c,
  title = {Sonums - a Maximum Entropy Number Format},
  author = {Kl{\"o}wer, Milan},
  year = {2019},
  month = nov,
  abstract = {The Self-Organizing NUMbers. A number format that learns from data.},
  copyright = {MIT}
}

@misc{Klower2020,
  title = {Milankl/{{Sherlogs}}.Jl: {{An}} Analysis-Number Format That Inspects Code},
  shorttitle = {Milankl/{{Sherlogs}}.Jl},
  author = {Kl{\"o}wer, Milan},
  year = {2020},
  month = mar,
  doi = {10.5281/zenodo.3693756},
  abstract = {Sherlogs v0.1.0 Closed issues: Ambiguous type definition (\#8) Merged pull requests: Sherlog64 v0.1 (\#1) (@milankl) with test .ymls (\#2) (@milankl) logbook index with Int64 (\#3) (@milankl) Sherlog32 added (\#4) (@milankl) with example bitpattern hist (\#5) (@milankl) Sherlog16 + eps, oneunit implemented (\#6) (@milankl) matrix solve example (\#7) (@milankl) Sherlog16 ambiguity removed (\#9) (@milankl) Ambiguity (\#10) (@milankl)},
  howpublished = {Zenodo},
  file = {/Users/milan/Zotero/storage/Z8EMZLJT/3693756.html}
}

@article{Klower2020a,
  title = {Number {{Formats}}, {{Error Mitigation}}, and {{Scope}} for 16-{{Bit Arithmetics}} in {{Weather}} and {{Climate Modeling Analyzed With}} a {{Shallow Water Model}}},
  author = {Kl{\"o}wer, M. and D{\"u}ben, P. D. and Palmer, T. N.},
  year = {2020},
  journal = {Journal of Advances in Modeling Earth Systems},
  volume = {12},
  number = {10},
  pages = {e2020MS002246},
  issn = {1942-2466},
  doi = {10.1029/2020MS002246},
  abstract = {The need for high-precision calculations with 64-bit or 32-bit floating-point arithmetic for weather and climate models is questioned. Lower-precision numbers can accelerate simulations and are increasingly supported by modern computing hardware. This paper investigates the potential of 16-bit arithmetic when applied within a shallow water model that serves as a medium complexity weather or climate application. There are several 16-bit number formats that can potentially be used (IEEE half precision, BFloat16, posits, integer, and fixed-point). It is evident that a simple change to 16-bit arithmetic will not be possible for complex weather and climate applications as it will degrade model results by intolerable rounding errors that cause a stalling of model dynamics or model instabilities. However, if the posit number format is used as an alternative to the standard floating-point numbers, the model degradation can be significantly reduced. Furthermore, mitigation methods, such as rescaling, reordering, and mixed precision, are available to make model simulations resilient against a precision reduction. If mitigation methods are applied, 16-bit floating-point arithmetic can be used successfully within the shallow water model. The results show the potential of 16-bit formats for at least parts of complex weather and climate models where rounding errors would be entirely masked by initial condition, model, or discretization error.},
  copyright = {\textcopyright 2020. The Authors.},
  language = {en},
  keywords = {16-bit arithmetic,climate models,floating-point numbers,posit numbers,Reduced precision,rounding error},
  annotation = {\_eprint: https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2020MS002246},
  file = {/Users/milan/Zotero/storage/C84IFDBM/Klöwer et al. - 2020 - Number Formats, Error Mitigation, and Scope for 16.pdf;/Users/milan/Zotero/storage/KSGTUS6X/2020MS002246.html}
}

@article{Klower2020b,
  title = {An Analysis of Ways to Decarbonize Conference Travel after {{COVID}}-19},
  author = {Kl{\"o}wer, Milan and Hopkins, Debbie and Allen, Myles and Higham, James},
  year = {2020},
  month = jul,
  journal = {Nature},
  volume = {583},
  number = {7816},
  pages = {356--359},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/d41586-020-02057-2},
  abstract = {Biennials, regional hubs and virtual attendance can slash emissions, new calculations show.},
  copyright = {2020 Nature},
  language = {en},
  file = {/Users/milan/Zotero/storage/ZCHV66TQ/d41586-020-02057-2.html}
}

@misc{Klower2021,
  title = {Milankl/{{ShallowWaters}}.Jl: {{A}} Type-Flexible 16-Bit Shallow Water Model},
  shorttitle = {Milankl/{{ShallowWaters}}.Jl},
  author = {Kl{\"o}wer, Milan},
  year = {2021},
  month = jun,
  doi = {10.5281/zenodo.4890413},
  howpublished = {Zenodo},
  file = {/Users/milan/Zotero/storage/IXFL9B33/4890413.html}
}

@article{Klower2021a,
  title = {Compressing Atmospheric Data into Its Real Information Content},
  author = {Kl{\"o}wer, Milan and Razinger, Miha and Dominguez, Juan Jose and D{\"u}ben, Peter D. and Palmer, Tim N.},
  year = {2021, in review},
  journal = {Nature Computational Science}
}

@article{Klower2021b,
  title = {Fluid Simulations Accelerated with 16 Bit: {{Approaching}} 4x Speedup on {{A64FX}} by Squeezing {{ShallowWaters}}.Jl into {{Float16}}},
  author = {Kl{\"o}wer, Milan and Hatfield, Sam and Croci, Matteo and D{\"u}ben, Peter D. and Palmer, Tim N.},
  year = {2021, in preparation}
}

@misc{Klower2021c,
  title = {Milankl/{{LogFixPoint16s}}.Jl},
  author = {Kl{\"o}wer, Milan},
  year = {2021},
  month = mar,
  abstract = {A 16bit logarithmic fixed-point number format},
  copyright = {MIT}
}

@misc{Klower2021d,
  title = {Milankl/{{StochasticRounding}}.Jl},
  author = {Kl{\"o}wer, Milan},
  year = {2021},
  month = jul,
  abstract = {Up or down? Maybe both?},
  copyright = {MIT}
}

@misc{Klower2021e,
  title = {Milankl/{{Lorenz96}}.Jl},
  author = {Kl{\"o}wer, Milan},
  year = {2021},
  month = jul,
  abstract = {A type-flexible Lorenz 1996 model.},
  copyright = {MIT}
}

@article{Kohn,
  title = {Ventilation of Oxygen Minimum Zones by Geostrophic Turbulence in a Shallow Water Model},
  author = {Kohn, Eike},
  pages = {120},
  language = {en}
}

@article{Koll2013,
  title = {Why {{Tropical Sea Surface Temperature}} Is {{Insensitive}} to {{Ocean Heat Transport Changes}}},
  author = {Koll, Daniel D. B. and Abbot, Dorian S.},
  year = {2013},
  month = sep,
  journal = {Journal of Climate},
  volume = {26},
  number = {18},
  pages = {6742--6749},
  issn = {0894-8755, 1520-0442},
  doi = {10.1175/JCLI-D-13-00192.1},
  abstract = {Previous studies have shown that increases in poleward ocean heat transport (OHT) do not strongly affect tropical SST. The goal of this paper is to explain this observation. To do so, the authors force two atmospheric global climate models (GCMs) in aquaplanet configuration with a variety of prescribed OHTs. It is found that increased OHT weakens the Hadley circulation, which decreases equatorial cloud cover and shortwave reflection, as well as reduces surface winds and evaporation, which both limit changes in tropical SST. The authors also modify one of the GCMs by alternatively setting the radiative effect of clouds to zero and disabling wind-driven evaporation changes to show that the cloud feedback is more important than the wind\textendash evaporation feedback for maintaining constant equatorial SST as OHT changes. This work highlights the fact that OHT can reduce the meridional SST gradient without affecting tropical SST and could therefore serve as an additional degree of freedom for explaining past warm climates.},
  language = {en}
}

@article{Kouznetsov2020,
  title = {A Note on Precision-Preserving Compression of Scientific Data},
  author = {Kouznetsov, Rostislav},
  year = {2020},
  month = jul,
  journal = {Geoscientific Model Development Discussions},
  pages = {1--9},
  publisher = {{Copernicus GmbH}},
  issn = {1991-959X},
  doi = {10.5194/gmd-2020-239},
  abstract = {{$<$}p{$><$}strong{$>$}Abstract.{$<$}/strong{$>$} Lossy compression of scientific data arrays is a powerful tool to save network bandwidth and storage space. Properly applied lossy compression can reduce the size of a dataset by orders of magnitude keeping all essential information, whereas a wrong choice of lossy compression parameters leads to the loss of valuable data. The paper considers statistical properties of several lossy compression methods implemented in "NetCDF operators" (NCO), a popular tool for handling and transformation of numerical data in NetCDF format. We compare the effects of imprecisions and artifacts resulting from use of a lossy compression of floating-point data arrays. In particular, we show that a popular Bit Grooming algorithm (default in NCO) has sub-optimal accuracy and produces substantial artifacts in multipoint statistics. We suggest a simple implementation of two algorithms that are free from these artifacts and have twice higher precision. Besides that, we suggest a way to rectify the data already processed with Bit Grooming. {$<$}/p{$>$} {$<$}p{$>$}The algorithm has been contributed to NCO mainstream. The supplementary material contains the implementation of the algorithm in Python 3.{$<$}/p{$>$}},
  language = {English},
  file = {/Users/milan/Zotero/storage/MFQPH5EW/Kouznetsov - 2020 - A note on precision-preserving compression of scie.pdf;/Users/milan/Zotero/storage/NQV8T8UD/gmd-2020-239.html}
}

@article{Kraskov2004,
  title = {Estimating Mutual Information},
  author = {Kraskov, Alexander and St{\"o}gbauer, Harald and Grassberger, Peter},
  year = {2004},
  month = jun,
  journal = {Physical Review E},
  volume = {69},
  number = {6},
  pages = {066138},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevE.69.066138},
  abstract = {We present two classes of improved estimators for mutual information M(X,Y), from samples of random points distributed according to some joint probability density {$\mu$}(x,y). In contrast to conventional estimators based on binnings, they are based on entropy estimates from k-nearest neighbor distances. This means that they are data efficient (with k=1 we resolve structures down to the smallest possible scales), adaptive (the resolution is higher where data are more numerous), and have minimal bias. Indeed, the bias of the underlying entropy estimates is mainly due to nonuniformity of the density at the smallest resolved scale, giving typically systematic errors which scale as functions of k/N for N points. Numerically, we find that both families become exact for independent distributions, i.e. the estimator \textasciicircum M(X,Y) vanishes (up to statistical fluctuations) if {$\mu$}(x,y)={$\mu$}(x){$\mu$}(y). This holds for all tested marginal distributions and for all dimensions of x and y. In addition, we give estimators for redundancies between more than two random variables. We compare our algorithms in detail with existing algorithms. Finally, we demonstrate the usefulness of our estimators for assessing the actual independence of components obtained from independent component analysis (ICA), for improving ICA, and for estimating the reliability of blind source separation., This article appears in the following collections:},
  file = {/Users/milan/Zotero/storage/UZC97DPD/Kraskov et al. - 2004 - Estimating mutual information.pdf;/Users/milan/Zotero/storage/WFKHLYLW/PhysRevE.69.html}
}

@article{Kuhn2016,
  title = {Data Compression for Climate Data},
  author = {Kuhn, Michael and Kunkel, Julian M. and Ludwig, Thomas},
  year = {2016},
  journal = {Supercomputing Frontiers and Innovations},
  volume = {3},
  number = {1},
  pages = {75--94},
  publisher = {{South Urals University}},
  issn = {2313-8734},
  abstract = {The different rates of increase for computational power and storage capabilities of supercomputers turn data storage into a technical and economical problem. Because storage capabilities are lagging behind, investments and operational costs for storage systems have increased to keep up with the supercomputers' I/O requirements. One promising approach is to reduce the amount of data that is stored. In this paper, we take a look at the impact of compression on performance and costs of high performance systems. To this end, we analyze the applicability of compression on all layers of the I/O stack, that is, main memory, network and storage. Based on the Mistral system of the German Climate Computing Center (Deutsches Klimarechenzentrum, DKRZ), we illustrate potential performance improvements and cost savings. Making use of compression on a large scale can decrease investments and operational costs by 50\% without negatively impacting performance. Additionally, we present ongoing work for supporting enhanced adaptive compression in the parallel distributed file system Lustre and application-specific compression.},
  copyright = {cc\_by\_nc},
  language = {en},
  file = {/Users/milan/Zotero/storage/G72C56PA/Kuhn et al. - 2016 - Data compression for climate data.pdf;/Users/milan/Zotero/storage/M82KK457/77684.html}
}

@article{Kuhn2016a,
  title = {Data {{Compression}} for {{Climate Data}}},
  author = {Kuhn, Michael and Kunkel, Julian and Ludwig, Thomas},
  year = {2016},
  month = jun,
  journal = {Supercomputing Frontiers and Innovations},
  volume = {3},
  number = {1},
  pages = {75--94},
  issn = {2313-8734},
  doi = {10.14529/jsfi160105},
  abstract = {The different rates of increase for computational power and storage capabilities of supercomputers turn data storage into a technical and economical problem.~Because storage capabilities are lagging behind, investments and operational costs for storage systems have increased to keep up with the supercomputers' I/O requirements.~One promising approach is to reduce the amount of data that is stored.~In this paper, we take a look at the impact of compression on performance and costs of high performance systems.~To this end, we analyze the applicability of compression on all layers of the I/O stack, that is, main memory, network and storage.~Based on the Mistral system of the German Climate Computing Center (Deutsches Klimarechenzentrum, DKRZ), we illustrate potential performance improvements and cost savings.~Making use of compression on a large scale can decrease investments and operational costs by 50\% without negatively impacting performance.~Additionally, we present ongoing work for supporting enhanced adaptive compression in the parallel distributed file system Lustre and application-specific compression.},
  copyright = {Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a~ Creative Commons Attribution License ~that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.},
  language = {en},
  file = {/Users/milan/Zotero/storage/G3BMAVEW/Kuhn et al. - 2016 - Data Compression for Climate Data.pdf;/Users/milan/Zotero/storage/A655AJH3/101.html}
}

@article{Kunkel2014,
  title = {Exascale {{Storage Systems}} -- {{An Analytical Study}} of {{Expenses}}},
  author = {Kunkel, Julian Martin and Kuhn, Michael and Ludwig, Thomas},
  year = {2014},
  month = jun,
  journal = {Supercomputing Frontiers and Innovations},
  volume = {1},
  number = {1},
  pages = {116--134},
  issn = {2313-8734},
  doi = {10.14529/jsfi140106},
  abstract = {The computational power and storage capability of supercomputers are growing at a different pace, with storage lagging behind; the widening gap necessitates new approaches to keep the investment and running costs for storage systems at bay. In this paper, we aim to unify previous models and compare different approaches for solving these problems.By extrapolating the characteristics of the German Climate Computing Center's previous supercomputers to the future, cost factors are identified and quantified in order to foster adequate research and development.Using models to estimate the execution costs of two prototypical use cases, we are discussing the potential of three concepts: re-computation, data deduplication and data compression.},
  copyright = {Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a~ Creative Commons Attribution License ~that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.},
  language = {en},
  file = {/Users/milan/Zotero/storage/ITNEBY5N/Kunkel et al. - 2014 - Exascale Storage Systems -- An Analytical Study of.pdf;/Users/milan/Zotero/storage/S4YWUDHK/20.html}
}

@inproceedings{Kurth2018,
  title = {Exascale Deep Learning for Climate Analytics},
  booktitle = {Proceedings of the {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}}, and {{Analysis}}},
  author = {Kurth, Thorsten and Treichler, Sean and Romero, Joshua and Mudigonda, Mayur and Luehr, Nathan and Phillips, Everett and Mahesh, Ankur and Matheson, Michael and Deslippe, Jack and Fatica, Massimiliano and Prabhat and Houston, Michael},
  year = {2018},
  month = nov,
  series = {{{SC}} '18},
  pages = {1--12},
  publisher = {{IEEE Press}},
  address = {{Dallas, Texas}},
  abstract = {We extract pixel-level masks of extreme weather patterns using variants of Tiramisu and DeepLabv3+ neural networks. We describe improvements to the software frameworks, input pipeline, and the network training algorithms necessary to efficiently scale deep learning on the Piz Daint and Summit systems. The Tiramisu network scales to 5300 P100 GPUs with a sustained throughput of 21.0 PF/s and parallel efficiency of 79.0\%. DeepLabv3+ scales up to 27360 V100 GPUs with a sustained throughput of 325.8 PF/s and a parallel efficiency of 90.7\% in single precision. By taking advantage of the FP16 Tensor Cores, a half-precision version of the DeepLabv3+ network achieves a peak and sustained throughput of 1.13 EF/s and 999.0 PF/s respectively.}
}

@article{Kuttippurath2012,
  title = {A Comparative Study of the Major Sudden Stratospheric Warmings in the {{Arctic}} Winters 2003/2004\&ndash;2009/2010},
  author = {Kuttippurath, J. and Nikulin, G.},
  year = {2012},
  month = sep,
  journal = {Atmospheric Chemistry and Physics},
  volume = {12},
  number = {17},
  pages = {8115--8129},
  issn = {1680-7324},
  doi = {10.5194/acp-12-8115-2012},
  abstract = {We present an analysis of the major sudden stratospheric warmings (SSWs) in the Arctic winters 2003/04\textendash 2009/10. There were 6 major SSWs (major warmings [MWs]) in 6 out of the 7 winters, in which the MWs of 2003/04, 2005/06, and 2008/09 were in January and those of 2006/07, 2007/08, and 2009/10 were in February. Although the winter 2009/10 was relatively cold from mid-December to mid-January, strong wave 1 activity led to a MW in early February, for which the largest momentum flux among the winters was estimated at 60{$\smwhtcircle$} N/10 hPa, about 450 m2 s-2. The strongest MW, however, was observed in 2008/09 and the weakest in 2006/07. The MW in 2008/09 was triggered by intense wave 2 activity and was a vortex split event. In contrast, strong wave 1 activity led to the MWs of other winters and were vortex displacement events. Large amounts of Eliassen-Palm (EP) and wave 1/2 EP fluxes (about 2\textendash 4 \texttimes 105 kg s-2) are estimated shortly before the MWs at 100 hPa averaged over 45\textendash 75{$\smwhtcircle$} N in all winters, suggesting profound tropospheric forcing for the MWs. We observe an increase in the occurrence of MWs ({$\sim$} 1.1 MWs/winter) in recent years (1998/99\textendash 2009/10), as there were 13 MWs in the 12 Arctic winters, although the long-term average (1957/58\textendash 2009/10) of the frequency stays around its historical value ({$\sim$} 0.7 MWs/winter), consistent with the findings of previous studies. An analysis of the chemical ozone loss in the past 17 Arctic winters (1993/94\textendash 2009/10) suggests that the loss is inversely proportional to the intensity and timing of MWs in each winter, where early (December\textendash January) MWs lead to minimal ozone loss. Therefore, this high frequency of MWs in recent Arctic winters has significant implications for stratospheric ozone trends in the northern hemisphere.},
  language = {en}
}

@article{Kwasniok2014,
  title = {Enhanced Regime Predictability in Atmospheric Low-Order Models Due to Stochastic Forcing},
  author = {Kwasniok, Frank},
  year = {2014},
  month = jun,
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {372},
  number = {2018},
  pages = {20130286},
  issn = {1364-503X, 1471-2962},
  doi = {10.1098/rsta.2013.0286},
  language = {en}
}

@article{Langroudi2019,
  title = {Deep {{Learning Training}} on the {{Edge}} with {{Low}}-{{Precision Posits}}},
  author = {Langroudi, Hamed F. and Carmichael, Zachariah and Kudithipudi, Dhireesha},
  year = {2019},
  month = jul,
  journal = {arXiv:1907.13216 [cs, stat]},
  eprint = {1907.13216},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Recently, the posit numerical format has shown promise for DNN data representation and compute with ultra-low precision ([5..8]-bit). However, majority of studies focus only on DNN inference. In this work, we propose DNN training using posits and compare with the floating point training. We evaluate on both MNIST and Fashion MNIST corpuses, where 16-bit posits outperform 16-bit floating point for end-to-end DNN training.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/milan/Zotero/storage/FJE5BC8K/Langroudi et al. - 2019 - Deep Learning Training on the Edge with Low-Precis.pdf;/Users/milan/Zotero/storage/2ST243AB/1907.html}
}

@article{Large2006,
  title = {Attribution and {{Impacts}} of {{Upper}}-{{Ocean Biases}} in {{CCSM3}}},
  author = {Large, W. G. and Danabasoglu, G.},
  year = {2006},
  month = jun,
  journal = {Journal of Climate},
  volume = {19},
  number = {11},
  pages = {2325--2346},
  issn = {0894-8755, 1520-0442},
  doi = {10.1175/JCLI3740.1},
  abstract = {The largest and potentially most important ocean near-surface biases are examined in the Community Climate System Model coupled simulation of present-day conditions. They are attributed to problems in the component models of the ocean or atmosphere, or both. Tropical biases in sea surface salinity (SSS) are associated with precipitation errors, with the most striking being a band of excess rainfall across the South Pacific at about 8\textdegree S. Cooler-than-observed equatorial Pacific sea surface temperature (SST) is necessary to control a potentially catastrophic positive feedback, involving precipitation along the equator. The strength of the wind-driven gyres and interbasin exchange is in reasonable agreement with observations, despite the generally too strong near-surface winds. However, the winds drive far too much transport through Drake Passage [Ͼ190 Sv (1 Sv {$\epsilon$} 106 m3 sϪ1)], but with little effect on SST and SSS. Problems with the width, separation, and location of western boundary currents and their extensions create large correlated SST and SSS biases in midlatitudes. Ocean model deficiencies are suspected because similar signals are seen in uncoupled ocean solutions, but there is no evidence of serious remote impacts. The seasonal cycles of SST and winds in the equatorial Pacific are not well represented, and numerical experiments suggest that these problems are initiated by the coupling of either or both wind components. The largest mean SST biases develop along the eastern boundaries of subtropical gyres, and the overall coupled model response is found to be linear. In the South Atlantic, surface currents advect these biases across much of the tropical basin. Significant precipitation responses are found both in the northwest Indian Ocean, and locally where the net result is the loss of an identifiable Atlantic intertropical convergence zone, which can be regained by controlling the coastal temperatures and salinities. Biases off South America and Baja California are shown to significantly degrade precipitation across the Pacific, subsurface ocean properties on both sides of the equator, and the seasonal cycle of equatorial SST in the eastern Pacific. These signals extend beyond the reach of surface currents, so connections via the atmosphere and subsurface ocean are implicated. Other experimental results indicate that the local atmospheric forcing is only part of the problem along eastern boundaries, with the representation of ocean upwelling another likely contributor.},
  language = {en}
}

@article{Leach2018,
  title = {Current Level and Rate of Warming Determine Emissions Budgets under Ambitious Mitigation},
  author = {Leach, Nicholas J. and Millar, Richard J. and Haustein, Karsten and Jenkins, Stuart and Graham, Euan and Allen, Myles R.},
  year = {2018},
  month = aug,
  journal = {Nature Geoscience},
  volume = {11},
  number = {8},
  pages = {574--579},
  publisher = {{Nature Publishing Group}},
  issn = {1752-0908},
  doi = {10.1038/s41561-018-0156-y},
  abstract = {Some of the differences between recent estimates of the remaining budget of carbon dioxide (CO2) emissions consistent with limiting warming to 1.5\,\textdegree C arise from different estimates of the level of warming to date relative to pre-industrial conditions, but not all. Here we show that, for simple geometrical reasons, the combination of both the level and rate of human-induced warming provides a remarkably accurate prediction of remaining emission budgets to peak warming across a broad range of scenarios, if budgets are expressed in terms of CO2-forcing-equivalent emissions. These in turn predict CO2 emissions budgets if (but only if) the fractional contribution of non-CO2 drivers to warming remains approximately unchanged, as it does in some ambitious mitigation scenarios, indicating a best-estimate remaining budget for 1.5\,\textdegree C of about 22 years' current emissions from mid-2017, with a `likely' (1 standard error) range of 13\textendash 32 years. This provides a simple, transparent and model-independent metric of progress towards an ambitious temperature stabilization goal that could be used to inform the Paris Agreement stocktake process. It is less applicable to less ambitious goals. Alternative definitions of current warming and scenarios for non-CO2 drivers give lower 1.5\,\textdegree C budgets. Lower budgets based on the MAGICC simple modelling system widely used in integrated assessment studies reflect its relatively high simulated current warming rates.},
  copyright = {2018 The Author(s)},
  language = {en},
  file = {/Users/milan/Zotero/storage/QUH349Z6/Leach et al. - 2018 - Current level and rate of warming determine emissi.pdf;/Users/milan/Zotero/storage/BQTCBXG8/s41561-018-0156-y.html}
}

@article{Lee2009,
  title = {Aviation and Global Climate Change in the 21st Century},
  author = {Lee, David S. and Fahey, David W. and Forster, Piers M. and Newton, Peter J. and Wit, Ron C.N. and Lim, Ling L. and Owen, Bethan and Sausen, Robert},
  year = {2009},
  month = jul,
  journal = {Atmospheric Environment},
  volume = {43},
  number = {22-23},
  pages = {3520--3537},
  issn = {13522310},
  doi = {10.1016/j.atmosenv.2009.04.024},
  abstract = {Aviation emissions contribute to the radiative forcing (RF) of climate. Of importance are emissions of carbon dioxide (CO2), nitrogen oxides (NOx), aerosols and their precursors (soot and sulphate), and increased cloudiness in the form of persistent linear contrails and induced-cirrus cloudiness. The recent Fourth Assessment Report (AR4) of the Intergovernmental Panel on Climate Change (IPCC) quantified aviation's RF contribution for 2005 based upon 2000 operations data. Aviation has grown strongly over the past years, despite world-changing events in the early 2000s; the average annual passenger traffic growth rate was 5.3\% yr\`A1 between 2000 and 2007, resulting in an increase of passenger traffic of 38\%. Presented here are updated values of aviation RF for 2005 based upon new operations data that show an increase in traffic of 22.5\%, fuel use of 8.4\% and total aviation RF of 14\% (excluding induced-cirrus enhancement) over the period 2000\textendash 2005. The lack of physical process models and adequate observational data for aviationinduced cirrus effects limit confidence in quantifying their RF contribution. Total aviation RF (excluding induced cirrus) in 2005 was w55 mW m\`A2 (23\textendash 87 mW m\`A2, 90\% likelihood range), which was 3.5\% (range 1.3\textendash 10\%, 90\% likelihood range) of total anthropogenic forcing. Including estimates for aviation-induced cirrus RF increases the total aviation RF in 2005\textendash 78 mW m\`A2 (38\textendash 139 mW m\`A2, 90\% likelihood range), which represents 4.9\% of total anthropogenic forcing (2\textendash 14\%, 90\% likelihood range). Future scenarios of aviation emissions for 2050 that are consistent with IPCC SRES A1 and B2 scenario assumptions have been presented that show an increase of fuel usage by factors of 2.7\textendash 3.9 over 2000. Simplified calculations of total aviation RF in 2050 indicate increases by factors of 3.0\textendash 4.0 over the 2000 value, representing 4\textendash 4.7\% of total RF (excluding induced cirrus). An examination of a range of future technological options shows that substantive reductions in aviation fuel usage are possible only with the introduction of radical technologies. Incorporation of aviation into an emissions trading system offers the potential for overall (i.e., beyond the aviation sector) CO2 emissions reductions. Proposals exist for introduction of such a system at a European level, but no agreement has been reached at a global level.},
  language = {en},
  file = {/Users/milan/Zotero/storage/6XEZZPDP/Lee et al. - 2009 - Aviation and global climate change in the 21st cen.pdf}
}

@article{Lee2009a,
  title = {Aviation and Global Climate Change in the 21st Century},
  author = {Lee, David S. and Fahey, David W. and Forster, Piers M. and Newton, Peter J. and Wit, Ron C. N. and Lim, Ling L. and Owen, Bethan and Sausen, Robert},
  year = {2009},
  month = jul,
  journal = {Atmospheric Environment},
  volume = {43},
  number = {22},
  pages = {3520--3537},
  issn = {1352-2310},
  doi = {10.1016/j.atmosenv.2009.04.024},
  abstract = {Aviation emissions contribute to the radiative forcing (RF) of climate. Of importance are emissions of carbon dioxide (CO2), nitrogen oxides (NOx), aerosols and their precursors (soot and sulphate), and increased cloudiness in the form of persistent linear contrails and induced-cirrus cloudiness. The recent Fourth Assessment Report (AR4) of the Intergovernmental Panel on Climate Change (IPCC) quantified aviation's RF contribution for 2005 based upon 2000 operations data. Aviation has grown strongly over the past years, despite world-changing events in the early 2000s; the average annual passenger traffic growth rate was 5.3\% yr-1 between 2000 and 2007, resulting in an increase of passenger traffic of 38\%. Presented here are updated values of aviation RF for 2005 based upon new operations data that show an increase in traffic of 22.5\%, fuel use of 8.4\% and total aviation RF of 14\% (excluding induced-cirrus enhancement) over the period 2000\textendash 2005. The lack of physical process models and adequate observational data for aviation-induced cirrus effects limit confidence in quantifying their RF contribution. Total aviation RF (excluding induced cirrus) in 2005 was {$\sim$}55~mW~m-2 (23\textendash 87~mW~m-2, 90\% likelihood range), which was 3.5\% (range 1.3\textendash 10\%, 90\% likelihood range) of total anthropogenic forcing. Including estimates for aviation-induced cirrus RF increases the total aviation RF in 2005\textendash 78~mW~m-2 (38\textendash 139~mW~m-2, 90\% likelihood range), which represents 4.9\% of total anthropogenic forcing (2\textendash 14\%, 90\% likelihood range). Future scenarios of aviation emissions for 2050 that are consistent with IPCC SRES A1 and B2 scenario assumptions have been presented that show an increase of fuel usage by factors of~2.7\textendash 3.9 over 2000. Simplified calculations of total aviation RF in 2050 indicate increases by factors of 3.0\textendash 4.0 over the 2000 value, representing 4\textendash 4.7\% of total RF (excluding induced cirrus). An examination of a range of future technological options shows that substantive reductions in aviation fuel usage are possible only with the introduction of radical technologies. Incorporation of aviation into an emissions trading system offers the potential for overall (i.e., beyond the aviation sector) CO2 emissions reductions. Proposals exist for introduction of such a system at a European level, but no agreement has been reached at a global level.},
  language = {en},
  keywords = {AR4,Aviation,Aviation emissions,Aviation trends,Aviation-induced cirrus,Climate change,Climate change adaptation,Climate change mitigation,Contrails,IPCC,Radiative forcing},
  file = {/Users/milan/Zotero/storage/ZB7EAU8K/Lee et al. - 2009 - Aviation and global climate change in the 21st cen.pdf;/Users/milan/Zotero/storage/SCCN6WJJ/S1352231009003574.html}
}

@article{Lee2009b,
  title = {Aviation and Global Climate Change in the 21st Century},
  author = {Lee, David S. and Fahey, David W. and Forster, Piers M. and Newton, Peter J. and Wit, Ron C.N. and Lim, Ling L. and Owen, Bethan and Sausen, Robert},
  year = {2009},
  month = jul,
  journal = {Atmospheric Environment},
  volume = {43},
  number = {22-23},
  pages = {3520--3537},
  issn = {13522310},
  doi = {10.1016/j.atmosenv.2009.04.024},
  abstract = {Aviation emissions contribute to the radiative forcing (RF) of climate. Of importance are emissions of carbon dioxide (CO2), nitrogen oxides (NOx), aerosols and their precursors (soot and sulphate), and increased cloudiness in the form of persistent linear contrails and induced-cirrus cloudiness. The recent Fourth Assessment Report (AR4) of the Intergovernmental Panel on Climate Change (IPCC) quantified aviation's RF contribution for 2005 based upon 2000 operations data. Aviation has grown strongly over the past years, despite world-changing events in the early 2000s; the average annual passenger traffic growth rate was 5.3\% yr\`A1 between 2000 and 2007, resulting in an increase of passenger traffic of 38\%. Presented here are updated values of aviation RF for 2005 based upon new operations data that show an increase in traffic of 22.5\%, fuel use of 8.4\% and total aviation RF of 14\% (excluding induced-cirrus enhancement) over the period 2000\textendash 2005. The lack of physical process models and adequate observational data for aviationinduced cirrus effects limit confidence in quantifying their RF contribution. Total aviation RF (excluding induced cirrus) in 2005 was w55 mW m\`A2 (23\textendash 87 mW m\`A2, 90\% likelihood range), which was 3.5\% (range 1.3\textendash 10\%, 90\% likelihood range) of total anthropogenic forcing. Including estimates for aviation-induced cirrus RF increases the total aviation RF in 2005\textendash 78 mW m\`A2 (38\textendash 139 mW m\`A2, 90\% likelihood range), which represents 4.9\% of total anthropogenic forcing (2\textendash 14\%, 90\% likelihood range). Future scenarios of aviation emissions for 2050 that are consistent with IPCC SRES A1 and B2 scenario assumptions have been presented that show an increase of fuel usage by factors of 2.7\textendash 3.9 over 2000. Simplified calculations of total aviation RF in 2050 indicate increases by factors of 3.0\textendash 4.0 over the 2000 value, representing 4\textendash 4.7\% of total RF (excluding induced cirrus). An examination of a range of future technological options shows that substantive reductions in aviation fuel usage are possible only with the introduction of radical technologies. Incorporation of aviation into an emissions trading system offers the potential for overall (i.e., beyond the aviation sector) CO2 emissions reductions. Proposals exist for introduction of such a system at a European level, but no agreement has been reached at a global level.},
  language = {en},
  file = {/Users/milan/Zotero/storage/DLHDQ3W2/Lee et al. - 2009 - Aviation and global climate change in the 21st cen.pdf}
}

@article{Lee2020,
  title = {The Contribution of Global Aviation to Anthropogenic Climate Forcing for 2000 to 2018},
  author = {Lee, David S. and Fahey, D. W. and Skowron, A. and Allen, M. R. and Burkhardt, U. and Chen, Q. and Doherty, S. J. and Freeman, S. and Forster, P. M. and Fuglestvedt, J. and Gettelman, A. and De Le{\'o}n, R. R. and Lim, L. L. and Lund, M. T. and Millar, R. J. and Owen, B. and Penner, J. E. and Pitari, G. and Prather, M. J. and Sausen, R. and Wilcox, L. J.},
  year = {2020},
  month = sep,
  journal = {Atmospheric Environment},
  pages = {117834},
  issn = {1352-2310},
  doi = {10.1016/j.atmosenv.2020.117834},
  abstract = {Global aviation operations contribute to anthropogenic climate change via a complex set of processes that lead to a net surface warming. Of importance are aviation emissions of carbon dioxide (CO2), nitrogen oxides (NOx), water vapor, soot and sulfate aerosols, and increased cloudiness due to contrail formation. Aviation grew strongly over the past decades (1960\textendash 2018) in terms of activity, with revenue passenger kilometers increasing from 109 to 8269 billion km yr-1, and in terms of climate change impacts, with CO2 emissions increasing by a factor of 6.8 to 1034~Tg CO2 yr-1. Over the period 2013\textendash 2018, the growth rates in both terms show a marked increase. Here, we present a new comprehensive and quantitative approach for evaluating aviation climate forcing terms. Both radiative forcing (RF) and effective radiative forcing (ERF) terms and their sums are calculated for the years 2000\textendash 2018. Contrail cirrus, consisting of linear contrails and the cirrus cloudiness arising from them, yields the largest positive net (warming) ERF term followed by CO2 and NOx emissions. The formation and emission of sulfate aerosol yields a negative (cooling) term. The mean contrail cirrus ERF/RF ratio of 0.42 indicates that contrail cirrus is less effective in surface warming than other terms. For 2018 the net aviation ERF is +100.9~milliwatts (mW) m-2 (5\textendash 95\% likelihood range of (55, 145)) with major contributions from contrail cirrus (57.4~mW~m-2), CO2 (34.3~mW~m-2), and NOx (17.5~mW~m-2). Non-CO2 terms sum to yield a net positive (warming) ERF that accounts for more than half (66\%) of the aviation net ERF in 2018. Using normalization to aviation fuel use, the contribution of global aviation in 2011 was calculated to be 3.5 (4.0, 3.4) \% of the net anthropogenic ERF of 2290 (1130, 3330) mW m-2. Uncertainty distributions (5\%, 95\%) show that non-CO2 forcing terms contribute about 8 times more than CO2 to the uncertainty in the aviation net ERF in 2018. The best estimates of the ERFs from aviation aerosol-cloud interactions for soot and sulfate remain undetermined. CO2-warming-equivalent emissions based on global warming potentials (GWP* method) indicate that aviation emissions are currently warming the climate at approximately three times the rate of that associated with aviation CO2 emissions alone. CO2 and NOx aviation emissions and cloud effects remain a continued focus of anthropogenic climate change research and policy discussions.},
  language = {en},
  keywords = {Aviation,Climate,CO,Contrail cirrus,NO,Radiative forcing},
  file = {/Users/milan/Zotero/storage/ETX4FHB4/Lee et al. - 2020 - The contribution of global aviation to anthropogen.pdf;/Users/milan/Zotero/storage/ECNJD8QA/S1352231020305689.html}
}

@article{Leith1967,
  title = {Diffusion {{Approximation}} to {{Inertial Energy Transfer}} in {{Isotropic Turbulence}}},
  author = {Leith, C. E.},
  year = {1967},
  journal = {Physics of Fluids},
  volume = {10},
  number = {7},
  pages = {1409},
  issn = {00319171},
  doi = {10.1063/1.1762300},
  language = {en}
}

@article{Leith1996,
  title = {Stochastic Models of Chaotic Systems},
  author = {Leith, C.E.},
  year = {1996},
  month = nov,
  journal = {Physica D: Nonlinear Phenomena},
  volume = {98},
  number = {2-4},
  pages = {481--491},
  issn = {01672789},
  doi = {10.1016/0167-2789(96)00107-8},
  abstract = {Nonlinear dynamical systems, although strictly deterministic, often exhibit chaotic behavior which appears to be random. The determination of the probabilistic properties of such systems is, in general, an open problem. Closure approximations for moment expansion methods have been unsatisfactory. More successful has been approximation on the dynamics level by the use of linear stochastic models that attempt to generate the probabilistic properties of the original nonlinear chaotic system as closely as possible. Examples are reviewed of this approach to simple nonlinear systems, to turbulence, and to large-eddy simulation. A stochastic model that simulates the transient energy spectrum of the global atmosphere is developed.},
  language = {en}
}

@misc{Lentze2016,
  title = {Newsletter {{No}}. 146 - {{Winter}} 2015/16},
  author = {Lentze, Georg},
  year = {2016},
  number = {146},
  file = {/Users/milan/Zotero/storage/LKSE6CVD/15041-newsletter-no-146-winter-2015-16.html}
}

@article{Lenzen2018,
  title = {The Carbon Footprint of Global Tourism},
  author = {Lenzen, Manfred and Sun, Ya-Yen and Faturay, Futu and Ting, Yuan-Peng and Geschke, Arne and Malik, Arunima},
  year = {2018},
  month = jun,
  journal = {Nature Climate Change},
  volume = {8},
  number = {6},
  pages = {522--528},
  issn = {1758-678X, 1758-6798},
  doi = {10.1038/s41558-018-0141-x},
  language = {en},
  file = {/Users/milan/Zotero/storage/25MYX9SR/Lenzen et al. - 2018 - The carbon footprint of global tourism.pdf}
}

@misc{Leong2020,
  title = {{{SoftPosit}}},
  author = {Leong, S. H.},
  year = {2020},
  month = mar,
  doi = {10.5281/zenodo.3709035},
  howpublished = {Zenodo},
  language = {eng},
  keywords = {Posits,SoftPosit,UNUM},
  file = {/Users/milan/Zotero/storage/DA8WHA99/3709035.html}
}

@article{LeQuere2020,
  title = {Temporary Reduction in Daily Global {{CO}} 2 Emissions during the {{COVID}}-19 Forced Confinement},
  author = {Le Qu{\'e}r{\'e}, Corinne and Jackson, Robert B. and Jones, Matthew W. and Smith, Adam J. P. and Abernethy, Sam and Andrew, Robbie M. and {De-Gol}, Anthony J. and Willis, David R. and Shan, Yuli and Canadell, Josep G. and Friedlingstein, Pierre and Creutzig, Felix and Peters, Glen P.},
  year = {2020},
  month = jul,
  journal = {Nature Climate Change},
  volume = {10},
  number = {7},
  pages = {647--653},
  publisher = {{Nature Publishing Group}},
  issn = {1758-6798},
  doi = {10.1038/s41558-020-0797-x},
  abstract = {Government policies during the COVID-19 pandemic have drastically altered patterns of energy demand around the world. Many international borders were closed and populations were confined to their homes, which reduced transport and changed consumption patterns. Here we compile government policies and activity data to estimate the decrease in CO2 emissions during forced confinements. Daily global CO2 emissions decreased by \textendash 17\% (\textendash 11 to \textendash 25\% for {$\pm$}1{$\sigma$}) by early April 2020 compared with the mean 2019 levels, just under half from changes in surface transport. At their peak, emissions in individual countries decreased by \textendash 26\% on average. The impact on 2020 annual emissions depends on the duration of the confinement, with a low estimate of \textendash 4\% (\textendash 2 to \textendash 7\%) if prepandemic conditions return by mid-June, and a high estimate of \textendash 7\% (\textendash 3 to \textendash 13\%) if some restrictions remain worldwide until the end of 2020. Government actions and economic incentives postcrisis will likely influence the global CO2 emissions path for decades.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  language = {en},
  file = {/Users/milan/Zotero/storage/5QUZDQYA/Le Quéré et al. - 2020 - Temporary reduction in daily global CO 2 emissions.pdf;/Users/milan/Zotero/storage/S3PT2QU5/s41558-020-0797-x.html}
}

@article{LeSommer2009,
  title = {How Momentum Advection Schemes Influence Current-Topography Interactions at Eddy Permitting Resolution},
  author = {Le Sommer, Julien and Penduff, Thierry and Theetten, S{\'e}bastien and Madec, Gurvan and Barnier, Bernard},
  year = {2009},
  month = jan,
  journal = {Ocean Modelling},
  volume = {29},
  number = {1},
  pages = {1--14},
  issn = {14635003},
  doi = {10.1016/j.ocemod.2008.11.007},
  abstract = {Recent studies have shown that the use of an enstrophy-and-energy-conserving momentum advection scheme substantially reduces widespread biases of mean currents in the global 1/4\textdegree{} DRAKKAR model. This paper investigates the origin of these improvements. A series of sensitivity simulations with different momentum advection schemes is performed with the North Atlantic 1/4\textdegree{} DRAKKAR model. Three second order momentum advection schemes conserving, respectively, enstrophy (ens), energy (efx) and both quantities (een) are tested and their impact on the model solution are compared.},
  language = {en}
}

@article{Levitus2005,
  title = {Warming of the World Ocean, 1955\textendash 2003},
  author = {Levitus, S.},
  year = {2005},
  journal = {Geophysical Research Letters},
  volume = {32},
  number = {2},
  pages = {L02604},
  issn = {0094-8276},
  doi = {10.1029/2004GL021592},
  language = {en}
}

@inproceedings{Liang2018,
  title = {Error-{{Controlled Lossy Compression Optimized}} for {{High Compression Ratios}} of {{Scientific Datasets}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  author = {Liang, Xin and Di, Sheng and Tao, Dingwen and Li, Sihuan and Li, Shaomeng and Guo, Hanqi and Chen, Zizhong and Cappello, Franck},
  year = {2018},
  month = dec,
  pages = {438--447},
  doi = {10.1109/BigData.2018.8622520},
  abstract = {Today's scientific simulations require a significant reduction of the data size because of extremely large volumes of data they produce and the limitation of storage bandwidth and space. If the compression is set to reach a high compression ratio, however, the reconstructed data are often distorted too much to tolerate. In this paper, we explore a new compression strategy that can effectively control the data distortion when significantly reducing the data size. The contribution is threefold. (1) We propose an adaptive compression framework to select either our improved Lorenzo prediction method or our optimized linear regression method dynamically in different regions of the dataset. (2) We explore how to select them accurately based on the data features in each block to obtain the best compression quality. (3) We analyze the effectiveness of our solution in details using four real-world scientific datasets with 100+ fields. Evaluation results confirm that our new adaptive solution can significantly improve the rate distortion for the lossy compression with fairly high compression ratios. The compression ratio of our compressor is 1.5X 8X as high as that of two other leading lossy compressors (SZ and ZFP) with the same peak single-to-noise ratio (PSNR), in the high-compression cases. Parallel experiments with 8,192 cores and 24 TB of data shows that our solution obtains 1.86X dumping performance and 1.95X loading performance compared with the second-best lossy compressor, respectively.},
  keywords = {Bandwidth,Big Data,compression quality,data compression,data distortion,Data models,data reconstruction,Data visualization,Distortion,error-controlled lossy compression optimization,high compression ratio,linear regression method,Lorenzo prediction method,optimisation,peak single-to-noise ratio,Predictive models,PSNR,regression analysis,scientific datasets,SZ,ZFP},
  file = {/Users/milan/Zotero/storage/8VA744JH/Liang et al. - 2018 - Error-Controlled Lossy Compression Optimized for H.pdf;/Users/milan/Zotero/storage/28SZZN9G/8622520.html}
}

@article{Lindstrom2006,
  title = {Fast and {{Efficient Compression}} of {{Floating}}-{{Point Data}}},
  author = {Lindstrom, Peter and Isenburg, Martin},
  year = {2006},
  month = sep,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {12},
  number = {5},
  pages = {1245--1250},
  issn = {1941-0506},
  doi = {10.1109/TVCG.2006.143},
  abstract = {Large scale scientific simulation codes typically run on a cluster of CPUs that write/read time steps to/from a single file system. As data sets are constantly growing in size, this increasingly leads to I/O bottlenecks. When the rate at which data is produced exceeds the available I/O bandwidth, the simulation stalls and the CPUs are idle. Data compression can alleviate this problem by using some CPU cycles to reduce the amount of data needed to be transfered. Most compression schemes, however, are designed to operate offline and seek to maximize compression, not throughput. Furthermore, they often require quantizing floating-point values onto a uniform integer grid, which disqualifies their use in applications where exact values must be retained. We propose a simple scheme for lossless, online compression of floating-point data that transparently integrates into the I/O of many applications. A plug-in scheme for data-dependent prediction makes our scheme applicable to a wide variety of data used in visualization, such as unstructured meshes, point sets, images, and voxel grids. We achieve state-of-the-art compression rates and speeds, the latter in part due to an improved entropy coder. We demonstrate that this significantly accelerates I/O throughput in real simulation runs. Unlike previous schemes, our method also adapts well to variable-precision floating-point and integer data},
  keywords = {Analytical models,Bandwidth,data compression,Data compression,data visualisation,data visualization,Data visualization,data-dependent prediction,Entropy,fast entropy coding,file compaction for I/O efficiency,File systems,floating point arithmetic,High throughput,I/O bandwidth,Image coding,large scale simulation and visualization.,Large-scale systems,lossless compression,mathematics computing,online floating-point data compression,plug-in scheme,predictive coding,Predictive models,range coder,scientific simulation codes,Throughput},
  file = {/Users/milan/Zotero/storage/FG9CK3TW/4015488.html}
}

@article{Lindstrom2014,
  title = {Fixed-{{Rate Compressed Floating}}-{{Point Arrays}}},
  author = {Lindstrom, Peter},
  year = {2014},
  month = dec,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {20},
  number = {12},
  pages = {2674--2683},
  issn = {1941-0506},
  doi = {10.1109/TVCG.2014.2346458},
  abstract = {Current compression schemes for floating-point data commonly take fixed-precision values and compress them to a variable-length bit stream, complicating memory management and random access. We present a fixed-rate, near-lossless compression scheme that maps small blocks of 4d values in d dimensions to a fixed, user-specified number of bits per block, thereby allowing read and write random access to compressed floating-point data at block granularity. Our approach is inspired by fixed-rate texture compression methods widely adopted in graphics hardware, but has been tailored to the high dynamic range and precision demands of scientific applications. Our compressor is based on a new, lifted, orthogonal block transform and embedded coding, allowing each per-block bit stream to be truncated at any point if desired, thus facilitating bit rate selection using a single compression scheme. To avoid compression or decompression upon every data access, we employ a software write-back cache of uncompressed blocks. Our compressor has been designed with computational simplicity and speed in mind to allow for the possibility of a hardware implementation, and uses only a small number of fixed-point arithmetic operations per compressed value. We demonstrate the viability and benefits of lossy compression in several applications, including visualization, quantitative data analysis, and numerical simulation.},
  keywords = {Bandwidth allocation,bit rate selection,Computational modeling,computer graphics,data access,data compression,Data compression,data visualisation,data visualization,Data visualization,embedded coding,embedded systems,Encoding,fixed-point arithmetic operations,fixed-precision values,fixed-rate compressed floating-point arrays,fixed-rate texture compression methods,floating point arithmetic,Floating-point arithmetic,floating-point arrays,floating-point data,graphics hardware,graphics processing units,hardware implementation,Image coding,lossy compression,memory management,near-lossless compression scheme,numerical simulation,orthogonal block transform,per-block bit stream,quantitative data analysis,random-access storage,read and write random access,scientific applications,software write-back cache,storage management,variable-length bit stream},
  file = {/Users/milan/Zotero/storage/UFVB8GIE/6876024.html}
}

@article{Linnainmaa1974,
  title = {Analysis of Some Known Methods of Improving the Accuracy of Floating-Point Sums},
  author = {Linnainmaa, Seppo},
  year = {1974},
  month = jun,
  journal = {BIT Numerical Mathematics},
  volume = {14},
  number = {2},
  pages = {167--202},
  issn = {1572-9125},
  doi = {10.1007/BF01932946},
  abstract = {Some well-known methods for calculating the round-off error in floating-point addition are analyzed in this paper. The methods have been introduced by M\o ller [16], Kahan [11] and Knuth [12]. The necessary and sufficient conditions under which these methods produce the value of the round-off error, for rounding, truncating and parity arithmetic, are given. The computer-oriented parity arithmetic is not commonly known, but it has some desirable properties, as this paper will demonstrate. Some experimental results are also reported.},
  language = {en}
}

@article{Liu2000,
  title = {Simple Finite Element Method in Vorticity Formulation for Incompressible Flows},
  author = {Liu, Jian-Guo and E, Weinan},
  year = {2000},
  month = mar,
  journal = {Mathematics of Computation},
  volume = {70},
  number = {234},
  pages = {579--594},
  issn = {0025-5718},
  doi = {10.1090/S0025-5718-00-01239-4},
  abstract = {A very simple and efficient finite element method is introduced for two and three dimensional viscous incompressible flows using the vorticity formulation. This method relies on recasting the traditional finite element method in the spirit of the high order accurate finite difference methods introduced by the authors in another work. Optimal accuracy of arbitrary order can be achieved using standard finite element or spectral elements. The method is convectively stable and is particularly suited for moderate to high Reynolds number flows.},
  language = {en}
}

@article{Liu2012,
  title = {Adjoint-{{Based Estimation}} of {{Eddy}}-{{Induced Tracer Mixing Parameters}} in the {{Global Ocean}}},
  author = {Liu, Chuanyu and K{\"o}hl, Armin and Stammer, Detlef},
  year = {2012},
  month = jul,
  journal = {Journal of Physical Oceanography},
  volume = {42},
  number = {7},
  pages = {1186--1206},
  issn = {0022-3670, 1520-0485},
  doi = {10.1175/JPO-D-11-0162.1},
  abstract = {Using the German Estimating the Circulation and Climate of the Ocean (GECCO) synthesis framework, four separate eddy tracer mixing coefficients are adjusted jointly with external forcing fields, such as to reduce a global misfit between the model simulations and ocean observations over a single 10-yr period and weighted by uncertainties. The suite of the adjusted eddy tracer mixing coefficients includes the vertical diffusivity kz, the along-isopycnal surface diffusivity kredi, the isopycnal layer thickness diffusivity kgm, and the along-isothickness advection coefficient kgmskew. Large and geographically varying adjustments are found in all four parameters, which all together lead to an additional 10\% reduction of the total cost function, as compared to using only surface flux parameters. However, their relative contribution to the cost reduction varies from 1\% to 50\% among the four coefficients, with the adjusted kgm contributing most. Regionally, the estimated kgm ranges from less than 2800 to about 2500 m2 s21. Largest adjustments in kgm reside in the vicinity of large isopycnal slopes and support a mixing length hypothesis; they also likewise support the hypothesis of a critical layer enhancement and high potential density gradient suppression. In a few occasions, resulting negative net kgm values can be found in the core of main currents, suggesting the potential for an inverse energy cascade transfer there. Large adjustments of kredi and kgmskew are found in the vicinity of isopycnal slopes. The adjustments of kz in the tropical thermoclines suggest deficiencies of the mixed layer parameterization.},
  language = {en}
}

@article{Lorenz1963,
  title = {Deterministic {{Nonperiodic Flow}}},
  author = {Lorenz, Edward N.},
  year = {1963},
  month = mar,
  journal = {Journal of the Atmospheric Sciences},
  volume = {20},
  number = {2},
  pages = {130--141},
  publisher = {{American Meteorological Society}},
  issn = {0022-4928},
  doi = {10.1175/1520-0469(1963)020<0130:DNF>2.0.CO;2},
  abstract = {Finite systems of deterministic ordinary nonlinear differential equations may be designed to represent forced dissipative hydrodynamic flow. Solutions of these equations can be identified with trajectories in phase space. For those systems with bounded solutions, it is found that nonperiodic solutions are ordinarily unstable with respect to small modifications, so that slightly differing initial states can evolve into considerably different states. Systems with bounded solutions are shown to possess bounded numerical solutions. A simple system representing cellular convection is solved numerically. All of the solutions are found to be unstable, and almost all of them are nonperiodic. The feasibility of very-long-range weather prediction is examined in the light of these results.},
  file = {/Users/milan/Zotero/storage/LISF2756/Lorenz - 1963 - Deterministic Nonperiodic Flow.pdf;/Users/milan/Zotero/storage/RNFRXD2T/1520-0469(1963)0200130DNF2.0.html}
}

@article{Lorenz1998,
  title = {Optimal {{Sites}} for {{Supplementary Weather Observations}}: {{Simulation}} with a {{Small Model}}},
  shorttitle = {Optimal {{Sites}} for {{Supplementary Weather Observations}}},
  author = {Lorenz, Edward N. and Emanuel, Kerry A.},
  year = {1998},
  month = feb,
  journal = {Journal of the Atmospheric Sciences},
  volume = {55},
  number = {3},
  pages = {399--414},
  publisher = {{American Meteorological Society}},
  issn = {0022-4928},
  doi = {10.1175/1520-0469(1998)055<0399:OSFSWO>2.0.CO;2},
  abstract = {Anticipating the opportunity to make supplementary observations at locations that can depend upon the current weather situation, the question is posed as to what strategy should be adopted to select the locations, if the greatest improvement in analyses and forecasts is to be realized. To seek a preliminary answer, the authors introduce a model consisting of 40 ordinary differential equations, with the dependent variables representing values of some atmospheric quantity at 40 sites spaced equally about a latitude circle. The equations contain quadratic, linear, and constant terms representing advection, dissipation, and external forcing. Numerical integration indicates that small errors (differences between solutions) tend to double in about 2 days. Localized errors tend to spread eastward as they grow, encircling the globe after about 14 days. In the experiments presented, 20 consecutive sites lie over the ocean and 20 over land. A particular solution is chosen as the true weather. Every 6 h observations are made, consisting of the true weather plus small random errors, at every land site, and at one ocean site to be selected by the strategy being considered. An analysis is then made, consisting of observations where observations are made and previously made 6-h forecasts elsewhere. Forecasts are made for each site at ranges from 6 h to 10 days. In all forecasts, a slightly weakened external forcing is used to simulate the model error. This process continues for 5 years, and mean-square forecast errors at each site at each range are accumulated. Strategies that attempt to locate the site where the current analysis, as made without a supplementary observation, is most greatly in error are found to perform better than those that seek the oceanic site to which a chosen land site is most sensitive at a chosen range. Among the former are strategies based on the ``breeding'' method, a variant of singular vectors, and ensembles of ``replicated'' observations; the last of these outperforms the others. The authors speculate as to the applicability of these findings to models with more realistic dynamics or without extensive regions devoid of routine observations, and to the real world.},
  file = {/Users/milan/Zotero/storage/SPTVRZJI/Lorenz and Emanuel - 1998 - Optimal Sites for Supplementary Weather Observatio.pdf;/Users/milan/Zotero/storage/QMVVGJAI/1520-0469(1998)0550399OSFSWO2.0.html}
}

@article{Ma2016,
  title = {Western Boundary Currents Regulated by Interaction between Ocean Eddies and the Atmosphere},
  author = {Ma, Xiaohui and Jing, Zhao and Chang, Ping and Liu, Xue and Montuoro, Raffaele and Small, R. Justin and Bryan, Frank O. and Greatbatch, Richard J. and Brandt, Peter and Wu, Dexing and Lin, Xiaopei and Wu, Lixin},
  year = {2016},
  month = jul,
  journal = {Nature},
  volume = {535},
  number = {7613},
  pages = {533--537},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature18640},
  language = {en}
}

@book{MacKay2003,
  title = {Information {{Theory}}, {{Inference}} and {{Learning Algorithms}}},
  author = {MacKay, David J. C.},
  year = {2003},
  month = oct,
  edition = {First},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge, UK}},
  abstract = {Information theory and inference, often taught separately, are here united in one entertaining textbook. These topics lie at the heart of many exciting areas of contemporary science and engineering - communication, signal processing, data mining, machine learning, pattern recognition, computational neuroscience, bioinformatics, and cryptography. This textbook introduces theory in tandem with applications. Information theory is taught alongside practical communication systems, such as arithmetic coding for data compression and sparse-graph codes for error-correction. A toolbox of inference techniques, including message-passing algorithms, Monte Carlo methods, and variational approximations, are developed alongside applications of these tools to clustering, convolutional codes, independent component analysis, and neural networks. The final part of the book describes the state of the art in error-correcting codes, including low-density parity-check codes, turbo codes, and digital fountain codes -- the twenty-first century standards for satellite communications, disk drives, and data broadcast. Richly illustrated, filled with worked examples and over 400 exercises, some with detailed solutions, David MacKay's groundbreaking book is ideal for self-learning and for undergraduate or graduate courses. Interludes on crosswords, evolution, and sex provide entertainment along the way. In sum, this is a textbook on information, communication, and coding for a new generation of students, and an unparalleled entry point into these subjects for professionals in areas as diverse as computational biology, financial engineering, and machine learning.},
  isbn = {978-0-521-64298-9},
  language = {English}
}

@article{Madec,
  title = {{{NEMO}} Ocean Engine},
  author = {Madec, Gurvan},
  pages = {399},
  language = {en}
}

@article{Mak2016,
  title = {A New Gauge-Invariant Method for Diagnosing Eddy Diffusivities},
  author = {Mak, J. and Maddison, J.R. and Marshall, D.P.},
  year = {2016},
  month = aug,
  journal = {Ocean Modelling},
  volume = {104},
  pages = {252--268},
  issn = {14635003},
  doi = {10.1016/j.ocemod.2016.06.006},
  abstract = {Coarse resolution numerical ocean models must typically include a parameterisation for mesoscale turbulence. A common recipe for such parameterisations is to invoke mixing of some tracer quantity, such as potential vorticity or buoyancy. However, it is well known that eddy fluxes include large rotational components which necessarily do not lead to any mixing; eddy diffusivities diagnosed from unfiltered fluxes are thus contaminated by the presence of these rotational components. Here a new methodology is applied whereby eddy diffusivities are diagnosed directly from the eddy force function. The eddy force function depends only upon flux divergences, is independent of any rotational flux components, and is inherently non-local and smooth. A one-shot inversion procedure is applied, minimising the mis-match between parameterised force functions and force functions derived from eddy resolving calculations. This enables diffusivities associated with the eddy potential vorticity and Gent\textendash McWilliams coefficients associated with eddy buoyancy fluxes to be diagnosed. This methodology is applied to multi-layer quasigeostrophic ocean gyre simulations. It is found that: (i) a strictly down-gradient scheme for mixing potential vorticity and quasi-geostrophic buoyancy has limited success in reducing the mis-match compared to one with no sign constraint on the eddy diffusivity or Gent\textendash McWilliams coefficient, with prevalent negative signals around the time-mean jet; (ii) the diagnostic is successful away from the jet region and wind-forced top layer; (iii) the locations of closed mean stream lines correlate with signals of positive eddy potential vorticity diffusivity; (iv) there is indication that the magnitude of the eddy potential vorticity diffusivity correlates well with the eddy energy. Implications for parameterisation are discussed in light of these diagnostic results.},
  language = {en}
}

@article{Malardel,
  title = {A New Grid for the {{IFS}}},
  author = {Malardel, Sylvie}
}

@article{Malardel2016,
  title = {A New Grid for the {{IFS}}},
  author = {Malardel, Sylvie and Wedi, Nils and Deconinck, Nils and Diamantakis, Michail and Kuehnlein, Christian and Mozdzynski, George and Hamrud, Mats and Smolarkiewicz, Piotr},
  year = {2016},
  journal = {ECMWF Newsletter},
  number = {146},
  abstract = {ECMWF will implement a resolution upgrade for high-resolution forecasts (HRES) and ensemble forecasts (ENS) in spring 2016. HRES will then be run on a grid with a grid-point distance between neighbouring points of approximately 9 km instead of 16 km in the current configuration.},
  file = {/Users/milan/Zotero/storage/RTX8TJ4K/A-new-grid-for-the-IFS.pdf}
}

@inproceedings{Markidis2018,
  title = {{{NVIDIA Tensor Core Programmability}}, {{Performance Precision}}},
  booktitle = {2018 {{IEEE International Parallel}} and {{Distributed Processing Symposium Workshops}} ({{IPDPSW}})},
  author = {Markidis, Stefano and Chien, Steven Wei Der and Laure, Erwin and Peng, Ivy Bo and Vetter, Jeffrey S.},
  year = {2018},
  month = may,
  pages = {522--531},
  issn = {null},
  doi = {10.1109/IPDPSW.2018.00091},
  abstract = {The NVIDIA Volta GPU microarchitecture introduces a specialized unit, called Tensor Core that performs one matrix-multiply-and-accumulate on 4x4 matrices per clock cycle. The NVIDIA Tesla V100 accelerator, featuring the Volta microarchitecture, provides 640 Tensor Cores with a theoretical peak performance of 125 Tflops/s in mixed precision. In this paper, we investigate current approaches to program NVIDIA Tensor Cores, their performances and the precision loss due to computation in mixed precision. Currently, NVIDIA provides three different ways of programming matrix-multiply-and-accumulate on Tensor Cores: the CUDA Warp Matrix Multiply Accumulate (WMMA) API, CUTLASS, a templated library based on WMMA, and cuBLAS GEMM. After experimenting with different approaches, we found that NVIDIA Tensor Cores can deliver up to 83 Tflops/s in mixed precision on a Tesla V100 GPU, seven and three times the performance in single and half precision respectively. A WMMA implementation of batched GEMM reaches a performance of 4 Tflops/s. While precision loss due to matrix multiplication with half precision input might be critical in many HPC applications, it can be considerably reduced at the cost of increased computation. Our results indicate that HPC applications using matrix multiplications can strongly benefit from using of NVIDIA Tensor Cores.},
  keywords = {application program interfaces,Computer architecture,cuBLAS GEMM,CUDA Warp Matrix Multiply Accumulate API,CUDA WMMA API,CUTLASS,GEMM,GPU Programming,graphics processing units,Graphics processing units,Hardware,HPC applications,Instruction sets,matrix multiplication,Mixed Precision,Neural networks,NVIDIA Tensor Core programmability,NVIDIA Tensor Cores,NVIDIA Tesla V100 accelerator,NVIDIA Volta GPU microarchitecture,parallel architectures,parallel programming,Programming,programming matrix-multiply- accumulate,Tensile stress,tensors,Tesla V100 GPU,Volta microarchitecture},
  file = {/Users/milan/Zotero/storage/9GJDHGPW/Markidis et al. - 2018 - NVIDIA Tensor Core Programmability, Performance Pr.pdf;/Users/milan/Zotero/storage/D2WE2S8V/8425458.html}
}

@article{Marshall2012,
  title = {A {{Framework}} for {{Parameterizing Eddy Potential Vorticity Fluxes}}},
  author = {Marshall, David P. and Maddison, James R. and Berloff, Pavel S.},
  year = {2012},
  month = apr,
  journal = {Journal of Physical Oceanography},
  volume = {42},
  number = {4},
  pages = {539--557},
  issn = {0022-3670, 1520-0485},
  doi = {10.1175/JPO-D-11-048.1},
  abstract = {A framework for parameterizing eddy potential vorticity fluxes is developed that is consistent with conservation of energy and momentum while retaining the symmetries of the original eddy flux. The framework involves rewriting the residual-mean eddy force, or equivalently the eddy potential vorticity flux, as the divergence of an eddy stress tensor. A norm of this tensor is bounded by the eddy energy, allowing the components of the stress tensor to be rewritten in terms of the eddy energy and nondimensional parameters describing the mean shape and orientation of the eddies. If a prognostic equation is solved for the eddy energy, the remaining unknowns are nondimensional and bounded in magnitude by unity. Moreover, these nondimensional geometric parameters have strong connections with classical stability theory. When applied to the Eady problem, it is shown that the new framework preserves the functional form of the Eady growth rate for linear instability. Moreover, in the limit in which Reynolds stresses are neglected, the framework reduces to a Gent and McWilliams type of eddy closure where the eddy diffusivity can be interpreted as the form proposed by Visbeck et al. Simulations of three-layer wind-driven gyres are used to diagnose the eddy shape and orientations in fully developed geostrophic turbulence. These fields are found to have large-scale structure that appears related to the structure of the mean flow. The eddy energy sets the magnitude of the eddy stress tensor and hence the eddy potential vorticity fluxes. Possible extensions of the framework to ensure potential vorticity is mixed on average are discussed.},
  language = {en}
}

@article{Matheson1976,
  title = {Scoring {{Rules}} for {{Continuous Probability Distributions}}},
  author = {Matheson, James E. and Winkler, Robert L.},
  year = {1976},
  month = jun,
  journal = {Management Science},
  volume = {22},
  number = {10},
  pages = {1087--1096},
  publisher = {{INFORMS}},
  issn = {0025-1909},
  doi = {10.1287/mnsc.22.10.1087},
  abstract = {Personal, or subjective, probabilities are used as inputs to many inferential and decision-making models, and various procedures have been developed for the elicitation of such probabilities. Included among these elicitation procedures are scoring rules, which involve the computation of a score based on the assessor's stated probabilities and on the event that actually occurs. The development of scoring rules has, in general, been restricted to the elicitation of discrete probability distributions. In this paper, families of scoring rules for the elicitation of continuous probability distributions are developed and discussed.}
}

@article{Matsumoto1998,
  title = {Mersenne Twister: A 623-Dimensionally Equidistributed Uniform Pseudo-Random Number Generator},
  shorttitle = {Mersenne Twister},
  author = {Matsumoto, Makoto and Nishimura, Takuji},
  year = {1998},
  month = jan,
  journal = {ACM Transactions on Modeling and Computer Simulation},
  volume = {8},
  number = {1},
  pages = {3--30},
  issn = {1049-3301},
  doi = {10.1145/272991.272995},
  abstract = {A new algorithm called Mersenne Twister (MT) is proposed for generating uniform pseudorandom numbers. For a particular choice of parameters, the algorithm provides a super astronomical period of 219937 -1 and 623-dimensional equidistribution up to 32-bit accuracy, while using a working area of only 624 words. This is a new variant of the previously proposed generators, TGFSR, modified so as to admit a Mersenne-prime period. The characteristic polynomial has many terms. The distribution up to v bits accuracy for 1 {$\leq$} v {$\leq$} 32 is also shown to be good. An algorithm is also given that checks the primitivity of the characteristic polynomial of MT with computational complexity O(p2) where p is the degree of the polynomial. We implemented this generator in portable C-code. It passed several stringent statistical tests, including diehard. Its speed is comparable to other modern generators. Its merits are due to the efficient algorithms that are unique to polynomial calculations over the two-element field.},
  keywords = {finite fields,GFSR,incomplete array,inversive-decimation method,k-distribution,m-sequences,Mersenne primes,Mersenne twister,MT19937,multiple-recursive matrix method,primitive polynomials,random number generation,tempering,TGFSR},
  file = {/Users/milan/Zotero/storage/4UA3JGZ7/Matsumoto and Nishimura - 1998 - Mersenne twister a 623-dimensionally equidistribu.pdf}
}

@article{Matzarakis1999,
  title = {Applications of a Universal Thermal Index: Physiological Equivalent Temperature},
  shorttitle = {Applications of a Universal Thermal Index},
  author = {Matzarakis, Andreas and Mayer, H. and Iziomon, M.G.},
  year = {1999},
  month = oct,
  journal = {International Journal of Biometeorology},
  volume = {43},
  number = {2},
  pages = {76--84},
  issn = {1432-1254},
  doi = {10.1007/s004840050119},
  abstract = {The physiological equivalent temperature, PET, is a thermal index derived from the human energy balance. It is well suited to the evaluation of the thermal component of different climates. As well as having a detailed physiological basis, PET is preferable to other thermal indexes like the predicted mean vote because of its unit (\textdegree C), which makes results more comprehensible to urban or regional planners, for example, who are not so familiar with modern human-biometeorological terminology. PET results can be presented graphically or as bioclimatic maps. Graphs mostly display the temporal behaviour of PET, whereas spatial distribution is specified in bioclimatic maps. In this article, some applications of PET are discussed. They relate to the evaluation of the urban heat island in cities in both temperate climates and warm climates at high altitude. The thermal component of the microclimate in the trunk space of a deciduous forest is also evaluated by PET. As an example of the spatial distribution of PET, a bioclimatic map for Greece in July (Mediterranean climate) is presented.},
  language = {en},
  file = {/Users/milan/Zotero/storage/BTS2FB3P/Matzarakis et al. - 1999 - Applications of a universal thermal index physiol.pdf}
}

@article{McGUlNNESS1983,
  title = {{{THE FRACTAL DIMENSION OF THE LORENZ ATTRACTOR}}},
  author = {McGUlNNESS, Mark J},
  year = {1983},
  journal = {PHYSICS LETTERS},
  volume = {99},
  number = {1},
  pages = {5},
  language = {en}
}

@article{McWilliams2016,
  title = {Submesoscale Currents in the Ocean},
  author = {McWilliams, James C.},
  year = {2016},
  month = may,
  journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {472},
  number = {2189},
  pages = {20160117},
  issn = {1364-5021, 1471-2946},
  doi = {10.1098/rspa.2016.0117},
  language = {en}
}

@article{Mikaitis2020,
  title = {Stochastic {{Rounding}}: {{Algorithms}} and {{Hardware Accelerator}}},
  shorttitle = {Stochastic {{Rounding}}},
  author = {Mikaitis, Mantas},
  year = {2020},
  month = jan,
  journal = {arXiv:2001.01501 [cs, math]},
  eprint = {2001.01501},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {General algorithms and a hardware accelerator for performing stochastic rounding (SR) are presented. The main goal is to augment the ARM M4F based multi-core processor SpiNNaker 2 with a more flexible rounding functionality than is available in the ARM processor itself. The motivation of adding such an accelerator in hardware is based on our previous results showing improvements in numerical accuracy of ODE solvers in fixed-point arithmetic with SR, compared to standard round-to-nearest or bit truncation rounding modes. Furthermore, performing SR purely in software can be expensive, due to requirement of a pseudorandom number generator (PRNG), multiple masking and shifting instructions and an addition operation. Also, saturation of the rounded values is included, since rounding is usually followed by saturation, which is especially important in fixed-point arithmetic due to a narrow dynamic range of representable values. The main intended use of the accelerator is to round fixed-point multiplier outputs, which are returned unrounded by the ARM processor in a wider fixed-point format than the arguments.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {Computer Science - Hardware Architecture,Mathematics - Numerical Analysis},
  file = {/Users/milan/Zotero/storage/I5QS6YQ8/Mikaitis - 2020 - Stochastic Rounding Algorithms and Hardware Accel.pdf}
}

@article{Mikaitis2020a,
  title = {Stochastic {{Rounding}}: {{Algorithms}} and {{Hardware Accelerator}}},
  shorttitle = {Stochastic {{Rounding}}},
  author = {Mikaitis, Mantas},
  year = {2020},
  month = jan,
  journal = {arXiv:2001.01501 [cs, math]},
  eprint = {2001.01501},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {General algorithms and a hardware accelerator for performing stochastic rounding (SR) are presented. The main goal is to augment the ARM M4F based multi-core processor SpiNNaker 2 with a more flexible rounding functionality than is available in the ARM processor itself. The motivation of adding such an accelerator in hardware is based on our previous results showing improvements in numerical accuracy of ODE solvers in fixed-point arithmetic with SR, compared to standard round-to-nearest or bit truncation rounding modes. Furthermore, performing SR purely in software can be expensive, due to requirement of a pseudorandom number generator (PRNG), multiple masking and shifting instructions and an addition operation. Also, saturation of the rounded values is included, since rounding is usually followed by saturation, which is especially important in fixed-point arithmetic due to a narrow dynamic range of representable values. The main intended use of the accelerator is to round fixed-point multiplier outputs, which are returned unrounded by the ARM processor in a wider fixed-point format than the arguments.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {Computer Science - Hardware Architecture,Mathematics - Numerical Analysis},
  file = {/Users/milan/Zotero/storage/CFSEKR72/Mikaitis - 2020 - Stochastic Rounding Algorithms and Hardware Accel.pdf}
}

@misc{MilanK2020,
  title = {Milankl/{{ShallowWaters}}.Jl: V0.2.0},
  shorttitle = {Milankl/{{ShallowWaters}}.Jl},
  author = {Milan K and Julia TagBot and Erik Schnetter},
  year = {2020},
  month = feb,
  doi = {10.5281/zenodo.3673985},
  abstract = {ShallowWaters v0.2.0 Diff since v0.1.0 Merged pull requests: ?Parameter help included (\#125) (@milankl) surface forcing latitude adjustment (\#126) (@milankl) Ghost points type stable with Tcomm (\#127) (@milankl) Tcomm test (\#128) (@milankl) Tcomm set to Tprog by default (\#129) (@milankl) get it mode specific and tendency by u\_n+1-u\_n etc (\#130) (@milankl) Install TagBot as a GitHub Action (\#131) (@JuliaTagBot) Whatsnext (\#132) (@milankl) time dependent wind forcing (\#133) (@milankl)},
  howpublished = {Zenodo},
  file = {/Users/milan/Zotero/storage/U5RS52S5/3673985.html}
}

@article{Molemaker2005,
  title = {Baroclinic {{Instability}} and {{Loss}} of {{Balance}}},
  author = {Molemaker, M. Jeroen and McWilliams, James C. and Yavneh, Irad},
  year = {2005},
  month = sep,
  journal = {Journal of Physical Oceanography},
  volume = {35},
  number = {9},
  pages = {1505--1517},
  issn = {0022-3670, 1520-0485},
  doi = {10.1175/JPO2770.1},
  abstract = {Under the influences of stable density stratification and the earth's rotation, large-scale flows in the ocean and atmosphere have a mainly balanced dynamics\textemdash sometimes called the slow manifold\textemdash in the sense that there are diagnostic hydrostatic and gradient-wind momentum balances that constrain the fluid acceleration. The nonlinear balance equations are a widely successful, approximate model for this regime, and mathematically explicit limits of their time integrability have been identified. It is hypothesized that these limits are indicative, at least approximately, of the transition from the larger-scale regime of inverse energy cascades by anisotropic flows to the smaller-scale regime of forward energy cascade to dissipation by more nearly isotropic flows and intermittently breaking inertia\textendash gravity waves. This paper analyzes the particular example of an unbalanced instability of a balanced, horizontally uniform, vertically sheared current, as it occurs within the Boussinesq equations. This ageostrophic, anticyclonic, baroclinic instability is investigated with an emphasis on how it relates to the breakdown of balance in the neighborhood of loss of balanced integrability and on how its properties compare with other examples of ageostrophic anticyclonic instability of rotating, stratified, horizontally sheared currents. It is also compared with the more familiar types of instability for a vertically sheared current: balanced (geostrophic) baroclinic instability, centrifugal instability, and Kelvin\textendash Helmholtz instability.},
  language = {en}
}

@article{Molemaker2010,
  title = {Balanced and Unbalanced Routes to Dissipation in an Equilibrated {{Eady}} Flow},
  author = {Molemaker, M. Jeroen and Mcwilliams, James C. and Capet, Xavier},
  year = {2010},
  month = jul,
  journal = {Journal of Fluid Mechanics},
  volume = {654},
  pages = {35--63},
  issn = {0022-1120, 1469-7645},
  doi = {10.1017/S0022112009993272},
  abstract = {The oceanic general circulation is forced at large scales and is unstable to mesoscale eddies. Large-scale currents and eddy flows are approximately in geostrophic balance. Geostrophic dynamics is characterized by an inverse energy cascade except for dissipation near the boundaries. In this paper, we confront the dilemma of how the general circulation may achieve dynamical equilibrium in the presence of continuous large-scale forcing and the absence of boundary dissipation. We do this with a forced horizontal flow with spatially uniform rotation, vertical stratification and vertical shear in a horizontally periodic domain, i.e. a version of Eady's flow carried to turbulent equilibrium. A direct route to interior dissipation is presented that is essentially non-geostrophic in its dynamics, with significant submesoscale frontogenesis, frontal instability and breakdown, and forward kinetic energy cascade to dissipation. To support this conclusion, a series of simulations is made with both quasigeostrophic and Boussinesq models. The quasigeostrophic model is shown as increasingly inefficient in achieving equilibration through viscous dissipation at increasingly higher numerical resolution (hence Reynolds number), whereas the non-geostrophic Boussinesq model equilibrates with only weak dependence on resolution and Rossby number.},
  language = {en}
}

@article{Molemaker2010a,
  title = {Local Balance and Cross-Scale Flux of Available Potential Energy},
  author = {Molemaker, M. Jeroen and McWILLIAMS, James C.},
  year = {2010},
  month = feb,
  journal = {Journal of Fluid Mechanics},
  volume = {645},
  pages = {295--314},
  issn = {0022-1120, 1469-7645},
  doi = {10.1017/S0022112009992643},
  abstract = {Gravitational available potential energy is a central concept in an energy analysis of flows in which buoyancy effects are dynamically important. These include, but are not limited to, most geophysical flows with persistently stable density stratification. The volume-integrated available potential energy                                                                               ap                              is defined as the difference between the gravitational potential energy of the system and the potential energy of a reference state with the lowest potential energy that can be reached by adiabatic material rearrangement;                                                                               ap                              determines how much energy is available for conservative dynamical exchange with kinetic energy                                                                               k                              . In this paper we introduce new techniques for computing the local available potential energy density                                E                 ap                              in numerical simulations that allow for a more accurate and complete analysis of the available potential energy and its dynamical balances as part of the complete energy cycle of a flow. In particular, the definition of                                E                 ap                              and an associated gravitation disturbance field                                                              permit us to make a spectral decomposition of its dynamical balance and examine the cross-scale energy flux. Several examples illustrate the spatial structure of                                E                 ap                              and its evolutionary influences. The greatest attention is given to an analysis of a turbulent-equilibrium simulation Eady-like vertical-shear flow with rotation and stable stratification. In this regime                                E                 ap                              exhibits a vigorous forward energy cascade from the mesoscale through the submesoscale range \textendash{} first in a scale range dominated by frontogenesis and positive buoyancy-flux conversion from                                                                               ap                              to                                                                               k                              and then, after strong frontal instability and frontogenetic arrest, in a coupled kinetic-potential energy inertial-cascade range with negative buoyancy-flux conversion \textendash{}               en route               to fine-scale dissipation of both energy components.},
  language = {en}
}

@article{Moller1965,
  title = {Quasi Double-Precision in Floating Point Addition},
  author = {M{\o}ller, Ole},
  year = {1965},
  month = mar,
  journal = {BIT Numerical Mathematics},
  volume = {5},
  number = {1},
  pages = {37--50},
  issn = {1572-9125},
  doi = {10.1007/BF01975722},
  abstract = {The loss of accuracy incurred in adding a small, accurate quantity to a larger one, using floating point addition, can be avoided by keeping account of a small correction to the sum. This is particularly valuable in machines which perform truncation, but no proper round-off, following arithmetic operations. In the first part of the article the details of the method are discussed. In the second part the effectiveness of the method is shown in an application to the step-by-step integration of ordinary differential equations.},
  language = {en}
}

@article{Molteni1996,
  title = {The {{ECMWF Ensemble Prediction System}}: {{Methodology}} and Validation},
  shorttitle = {The {{ECMWF Ensemble Prediction System}}},
  author = {Molteni, F. and Buizza, R. and Palmer, T. N. and Petroliagis, T.},
  year = {1996},
  journal = {Quarterly Journal of the Royal Meteorological Society},
  volume = {122},
  number = {529},
  pages = {73--119},
  issn = {1477-870X},
  doi = {10.1002/qj.49712252905},
  abstract = {The European Centre for Medium-Range Weather Forecasts (ECMWF) Ensemble Prediction System (EPS) is described. In addition to an unperturbed (control) forecast, each ensemble comprises 32 10-day forecasts starting from initial conditions in which dynamically defined perturbations have been added to the operational analysis. The perturbations are constructed from singular vectors of a time-evolution operator linearized around the short-range-forecast trajectory. These singular vectors approximately determine the most unstable phase-space directions in the early part of the forecast period, and are estimated using a forward and adjoint linear version of the ECMWF numerical weather-prediction model. An appropriate norm is chosen, and relationships between the structures of these singular vectors at initial time and patterns showing the sensitivity of short-range forecast error to changes in the analysis are discussed. A methodology to perform a phase-space rotation of the singular vectors is described, which generates hemispheric-wide perturbations and renormalizes them according to analysis-error estimates from the data-assimilation system. The validation of the ensembles is given firstly in terms of scatter diagrams and contingency tables of ensemble spread and control-forecast skill. The contingency tables are compared with those from a perfect-model ensemble system; no significant differences are found in some cases. Brier scores for the probability of European flow clusters are presented, which indicate predictive skill up to forecast-day 8 with respect to climatological probabilities. The dependence of these scores on flow-dependent model errors is also discussed. Finally, ensemble-member skill-score distributions are presented, which confirm the overall satisfactory performance of the EPS, particularly in summer and autumn 1993. In winter, cases of poor performance over Europe were associated with the occurrence of a split westerly flow with a blocking high and/or a cut-off low in the verifying analysis. Two cases are studied in detail, one having large ensemble dispersion, the other corresponding to a more predictable situation. The case studies are used to illustrate the range of ensemble products routinely disseminated to ECMWF Member States. These products include clusters of flow types, and probability fields of weather elements.},
  copyright = {Copyright \textcopyright{} 1996 Royal Meteorological Society},
  language = {en},
  keywords = {Ensemble prediction,Forecasting skill,Medium-range forecasts,Singular vectors},
  annotation = {\_eprint: https://rmets.onlinelibrary.wiley.com/doi/pdf/10.1002/qj.49712252905},
  file = {/Users/milan/Zotero/storage/D7SWHJ3J/qj.html}
}

@article{Moore,
  title = {Lateral {{Boundary Conditions}} in {{Numerical Ocean Models}}},
  author = {Moore, Stuart Alex},
  pages = {205},
  abstract = {The large-scale wind-driven ocean circulation is found to be sensitive to the choice of both the lateral boundary condition and the method of implementation. Where model boundaries consist of irregular piecewise-constant steps, a classical free-slip like circulation is only maintained when the free-slip condition is implemented using the Vorticity-Divergence form of the viscous stress tensor. The Ghost-Point method is only capable of such a solution when the boundary located relative vorticity is explicitly set to zero. Free-slip solutions in particular, regardless of the method of implementation, are highly sensitive to the orientation of the piecewise-constant steps and we suggest two mechanisms, one physical and one numerical, by which this sensitivity can be understood.},
  language = {en}
}

@article{Moran2018,
  title = {Carbon Footprints of 13,000 Cities},
  author = {Moran, Daniel and Kanemoto, Keiichiro and Jiborn, Magnus and Wood, Richard and T{\"o}bben, Johannes and Seto, Karen C.},
  year = {2018},
  month = jun,
  journal = {Environmental Research Letters},
  volume = {13},
  number = {6},
  pages = {064041},
  publisher = {{IOP Publishing}},
  issn = {1748-9326},
  doi = {10.1088/1748-9326/aac72a},
  abstract = {While it is understood that cities generate the majority of carbon emissions, for most cities, towns, and rural areas around the world no carbon footprint (CF) has been estimated. The Gridded Global Model of City Footprints (GGMCF) presented here downscales national CFs into a 250 m gridded model using data on population, purchasing power, and existing subnational CF studies from the US, China, EU, and Japan. Studies have shown that CFs are highly concentrated by income, with the top decile of earners driving 30\%\textendash 45\% of emissions. Even allowing for significant modeling uncertainties, we find that emissions are similarly concentrated in a small number of cities. The highest emitting 100 urban areas (defined as contiguous population clusters) account for 18\% of the global carbon footprint. While many of the cities with the highest footprints are in countries with high carbon footprints, nearly one quarter of the top cities (41 of the top 200) are in countries with relatively low emissions. In these cities population and affluence combine to drive footprints at a scale similar to those of cities in high-income countries. We conclude that concerted action by a limited number of local governments can have a disproportionate impact on global emissions.},
  language = {en},
  file = {/Users/milan/Zotero/storage/7JQ29LH6/Moran et al. - 2018 - Carbon footprints of 13hspace0.167em000 cities.pdf}
}

@article{Morice2021,
  title = {An {{Updated Assessment}} of {{Near}}-{{Surface Temperature Change From}} 1850: {{The HadCRUT5 Data Set}}},
  shorttitle = {An {{Updated Assessment}} of {{Near}}-{{Surface Temperature Change From}} 1850},
  author = {Morice, C. P. and Kennedy, J. J. and Rayner, N. A. and Winn, J. P. and Hogan, E. and Killick, R. E. and Dunn, R. J. H. and Osborn, T. J. and Jones, P. D. and Simpson, I. R.},
  year = {2021},
  journal = {Journal of Geophysical Research: Atmospheres},
  volume = {126},
  number = {3},
  pages = {e2019JD032361},
  issn = {2169-8996},
  doi = {10.1029/2019JD032361},
  abstract = {We present a new version of the Met Office Hadley Centre/Climatic Research Unit global surface temperature data set, HadCRUT5. HadCRUT5 presents monthly average near-surface temperature anomalies, relative to the 1961\textendash 1990 period, on a regular 5\textdegree{} latitude by 5\textdegree{} longitude grid from 1850 to 2018. HadCRUT5 is a combination of sea-surface temperature (SST) measurements over the ocean from ships and buoys and near-surface air temperature measurements from weather stations over the land surface. These data have been sourced from updated compilations and the adjustments applied to mitigate the impact of changes in SST measurement methods have been revised. Two variants of HadCRUT5 have been produced for use in different applications. The first represents temperature anomaly data on a grid for locations where measurement data are available. The second, more spatially complete, variant uses a Gaussian process based statistical method to make better use of the available observations, extending temperature anomaly estimates into regions for which the underlying measurements are informative. Each is provided as a 200-member ensemble accompanied by additional uncertainty information. The combination of revised input data sets and statistical analysis results in greater warming of the global average over the course of the whole record. In recent years, increased warming results from an improved representation of Arctic warming and a better understanding of evolving biases in SST measurements from ships. These updates result in greater consistency with other independent global surface temperature data sets, despite their different approaches to data set construction, and further increase confidence in our understanding of changes seen.},
  copyright = {\textcopyright{} 2020 Crown Copyright. This article is published with the permission of the Controller of HMSO and the Queen's Printer for Scotland.},
  language = {en},
  annotation = {\_eprint: https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2019JD032361},
  file = {/Users/milan/Zotero/storage/TBJEPSS5/Morice et al. - 2021 - An Updated Assessment of Near-Surface Temperature .pdf;/Users/milan/Zotero/storage/NGQUS8J6/2019JD032361.html}
}

@article{Morris1971,
  title = {Tapered {{Floating Point}}: {{A New Floating}}-{{Point Representation}}},
  shorttitle = {Tapered {{Floating Point}}},
  author = {Morris, R.},
  year = {1971},
  month = dec,
  journal = {IEEE Transactions on Computers},
  volume = {C-20},
  number = {12},
  pages = {1578--1579},
  issn = {2326-3814},
  doi = {10.1109/T-C.1971.223174},
  abstract = {It is well known that there is a possible tradeoff in the binary representation of floating-point numbers in which one bit of accuracy can be gained at the cost of halving the exponent range, and vice versa. A way in which the exponent range can be greatly increased while preserving full accuracy for most computations is suggested.},
  keywords = {Acuracy; exponent range; floating point; number representation.},
  file = {/Users/milan/Zotero/storage/ILM24D4E/1671767.html}
}

@article{Mortensen2016,
  title = {High Performance {{Python}} for Direct Numerical Simulations of Turbulent Flows},
  author = {Mortensen, Mikael and Langtangen, Hans Petter},
  year = {2016},
  month = jun,
  journal = {Computer Physics Communications},
  volume = {203},
  eprint = {1602.03638},
  eprinttype = {arxiv},
  pages = {53--65},
  issn = {00104655},
  doi = {10.1016/j.cpc.2016.02.005},
  abstract = {Direct Numerical Simulations (DNS) of the Navier Stokes equations is an invaluable research tool in fluid dynamics. Still, there are few publicly available research codes and, due to the heavy number crunching implied, available codes are usually written in low-level languages such as C/C++ or Fortran. In this paper we describe a pure scientific Python pseudo-spectral DNS code that nearly matches the performance of C++ for thousands of processors and billions of unknowns. We also describe a version optimized through Cython, that is found to match the speed of C++. The solvers are written from scratch in Python, both the mesh, the MPI domain decomposition, and the temporal integrators. The solvers have been verified and benchmarked on the Shaheen supercomputer at the KAUST supercomputing laboratory, and we are able to show very good scaling up to several thousand cores.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Mathematical Software}
}

@techreport{Muller2019,
  type = {Preprint},
  title = {The {{ESCAPE}} Project: {{Energy}}-Efficient {{Scalable Algorithms}} for {{Weather Prediction}} at {{Exascale}}},
  shorttitle = {The {{ESCAPE}} Project},
  author = {M{\"u}ller, Andreas and Deconinck, Willem and K{\"u}hnlein, Christian and Mengaldo, Gianmarco and Lange, Michael and Wedi, Nils and Bauer, Peter and Smolarkiewicz, Piotr K. and Diamantakis, Michail and Lock, Sarah-Jane and Hamrud, Mats and Saarinen, Sami and Mozdzynski, George and Thiemert, Daniel and Glinton, Michael and B{\'e}nard, Pierre and Voitus, Fabrice and Colavolpe, Charles and Marguinaud, Philippe and Zheng, Yongjun and Van Bever, Joris and Degrauwe, Daan and Smet, Geert and Termonia, Piet and Nielsen, Kristian P. and Sass, Bent H. and Poulsen, Jacob W. and Berg, Per and Osuna, Carlos and Fuhrer, Oliver and Clement, Valentin and Baldauf, Michael and Gillard, Mike and Szmelter, Joanna and O\&amp;apos;Brien, Enda and McKinstry, Alastair and Robinson, Ois{\'i}n and Shukla, Parijat and Lysaght, Michael and Kulczewski, Micha{\l} and Ciznicki, Milosz and Pi{\c a}tek, Wojciech and Ciesielski, Sebastian and B{\l}a{\.z}ewicz, Marek and Kurowski, Krzysztof and Procyk, Marcin and Spychala, Pawel and Bosak, Bartosz and Piotrowski, Zbigniew and Wyszogrodzki, Andrzej and Raffin, Erwan and Mazauric, Cyril and Guibert, David and Douriez, Louis and Vigouroux, Xavier and Gray, Alan and Messmer, Peter and Macfaden, Alexander J. and New, Nick},
  year = {2019},
  month = jan,
  institution = {{Climate and Earth System Modeling}},
  doi = {10.5194/gmd-2018-304},
  abstract = {In the simulation of complex multi-scale flow problems, such as those arising in weather and climate modelling, one of the biggest challenges is to satisfy operational requirements in terms of time-to-solution and energy-to-solution yet without compromising the accuracy and stability of the calculation. These competing factors require the development of state-of-theart algorithms that can optimally exploit the targeted underlying hardware and efficiently deliver the extreme computational 5 capabilities typically required in operational forecast production. These algorithms should (i) minimise the energy footprint along with the time required to produce a solution, (ii) maintain a satisfying level of accuracy, (iii) be numerically stable and resilient, in case of hardware or software failure.},
  language = {en}
}

@article{Nadiga2008,
  title = {Orientation of Eddy Fluxes in Geostrophic Turbulence},
  author = {Nadiga, B.T},
  year = {2008},
  month = jul,
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {366},
  number = {1875},
  pages = {2489--2508},
  issn = {1364-503X, 1471-2962},
  doi = {10.1098/rsta.2008.0058},
  language = {en}
}

@article{Nadiga2010,
  title = {Alternating Zonal Jets and Energy Fluxes in Barotropic Wind-Driven Gyres},
  author = {Nadiga, B.T. and Straub, D.N.},
  year = {2010},
  month = jan,
  journal = {Ocean Modelling},
  volume = {33},
  number = {3-4},
  pages = {257--269},
  issn = {14635003},
  doi = {10.1016/j.ocemod.2010.02.007},
  abstract = {The barotropic b-plane vorticity equation is considered under steady large scale (double-gyre) and small scale (stochastic) forcing. For both forcings, regimes are found in which alternating zonal jets appear. For steady large scale forcing, this regime is characterized by weak forcing and weak dissipation. Attention is focused on energy cascades due to the nonlinear and b terms and the jets are found to be associated with to a near compensation in these cascades over a range of wavenumbers. Additionally, interaction between flow forced at large scale and flow forced at small scale is examined.},
  language = {en}
}

@article{Nadiga2014,
  title = {Nonlinear Evolution of a Baroclinic Wave and Imbalanced Dissipation},
  author = {Nadiga, Balasubramanya T.},
  year = {2014},
  month = oct,
  journal = {Journal of Fluid Mechanics},
  volume = {756},
  pages = {965--1006},
  issn = {0022-1120, 1469-7645},
  doi = {10.1017/jfm.2014.464},
  abstract = {Abstract                            We consider the nonlinear evolution of an unstable baroclinic wave in a regime of rotating stratified flow that is of relevance to interior circulation in the oceans and in the atmosphere: a regime characterized by small large-scale Rossby and Froude numbers, a small vertical to horizontal aspect ratio and no bounding horizontal surfaces. Using high-resolution simulations of the non-hydrostatic Boussinesq equations and companion integrations of the balanced quasi-geostrophic (QG) equations, we present evidence for a local route to dissipation of balanced energy directly through interior turbulent cascades. That is, analysis of simulations presented in this study suggest that a developing baroclinic instability can lead to secondary instabilities that can cascade a small fraction of the energy forward to unbalanced scales whereas the bulk of the energy is confined to large balanced scales. Mesoscale shear and strain resulting from the hydrostatic geostrophic baroclinic instability drive frontogenesis. The fronts in turn support ageostrophic secondary circulation and instabilities. These two processes acting together lead to a quick rise in dissipation rate which then reaches a peak and begins to fall slowly when frontogenesis slows down; eventually balanced and imbalanced modes decouple. A measurement of the dissipation of balanced energy by imbalanced processes reveals that it scales exponentially with Rossby number of the base flow. We expect that this scaling will hold more generally than for the specific set-up we consider given the fundamental nature of the dynamics involved. In other results, (               a               ) a break is seen in the total energy (TE) spectrum at small scales: while a steep                                                                      \$\textbackslash def \textbackslash xmlpi \#1\{\}\textbackslash def \textbackslash mathsfbi \#1\{\textbackslash boldsymbol \{\textbackslash mathsf \{\#1\}\}\}\textbackslash let \textbackslash le =\textbackslash leqslant \textbackslash let \textbackslash leq =\textbackslash leqslant \textbackslash let \textbackslash ge =\textbackslash geqslant \textbackslash let \textbackslash geq =\textbackslash geqslant \textbackslash def \textbackslash Pr \{\textbackslash mathit \{Pr\}\}\textbackslash def \textbackslash Fr \{\textbackslash mathit \{Fr\}\}\textbackslash def \textbackslash Rey \{\textbackslash mathit \{Re\}\}k\^\{-3\}\$                                               geostrophic scaling (where                                                                      \$k\$                                               is the three-dimensional wavenumber) is seen at intermediate scales, the smaller scales display a shallower                                                                      \$k\^\{-5/3\}\$                                               scaling, reminiscent of the atmospheric spectra of Nastrom \& Gage and (               b               ) at the higher of the Rossby numbers considered a minimum is seen in the vertical shear spectrum, reminiscent of similar spectra obtained using               in situ               measurements.},
  language = {en}
}

@article{Nakano2018,
  title = {Single {{Precision}} in the {{Dynamical Core}} of a {{Nonhydrostatic Global Atmospheric Model}}: {{Evaluation Using}} a {{Baroclinic Wave Test Case}}},
  shorttitle = {Single {{Precision}} in the {{Dynamical Core}} of a {{Nonhydrostatic Global Atmospheric Model}}},
  author = {Nakano, Masuo and Yashiro, Hisashi and Kodama, Chihiro and Tomita, Hirofumi},
  year = {2018},
  month = feb,
  journal = {Monthly Weather Review},
  volume = {146},
  number = {2},
  pages = {409--416},
  publisher = {{American Meteorological Society}},
  issn = {1520-0493, 0027-0644},
  doi = {10.1175/MWR-D-17-0257.1},
  abstract = {{$<$}section class="abstract"{$><$}h2 class="abstractTitle text-title my-1" id="d145702010e111"{$>$}Abstract{$<$}/h2{$><$}p{$>$}Reducing the computational cost of weather and climate simulations would lower electric energy consumption. From the standpoint of reducing costs, the use of reduced precision arithmetic has become an active area of research. Here the impact of using single-precision arithmetic on simulation accuracy is examined by conducting Jablonowski and Williamson's baroclinic wave tests using the dynamical core of a global fully compressible nonhydrostatic model. The model employs a finite-volume method discretized on an icosahedral grid system and its mesh size is set to 220, 56, 14, and 3.5 km. When double-precision arithmetic is fully replaced by single-precision arithmetic, a spurious wavenumber-5 structure becomes dominant in both hemispheres, rather than the expected baroclinic wave growth only in the Northern Hemisphere. It was found that this spurious wave growth comes from errors in the calculation of gridcell geometrics. Therefore, an additional simulation was conducted using double precision for calculations that only need to be performed for model setup, including calculation of gridcell geometrics, and single precision everywhere else, meaning that all calculations performed each time step used single precision. In this case, the model successfully simulated the growth of the baroclinic wave with only small errors and a 46\% reduction in runtime. These results suggest that the use of single-precision arithmetic will allow significant reduction of computational costs in next-generation weather and climate simulations using a fully compressible nonhydrostatic global model with the finite-volume method.{$<$}/p{$><$}/section{$>$}},
  chapter = {Monthly Weather Review},
  language = {EN},
  file = {/Users/milan/Zotero/storage/XE4CRLDK/Nakano et al. - 2018 - Single Precision in the Dynamical Core of a Nonhyd.pdf;/Users/milan/Zotero/storage/A966XWT3/mwr-d-17-0257.1.html}
}

@article{Necker2019,
  title = {Sampling {{Error Correction Evaluated Using}} a {{Convective}}-{{Scale}} 1000-{{Member Ensemble}}},
  author = {Necker, Tobias and Weissmann, Martin and Ruckstuhl, Yvonne and Anderson, Jeffrey and Miyoshi, Takemasa},
  year = {2019},
  month = dec,
  journal = {Monthly Weather Review},
  volume = {148},
  number = {3},
  pages = {1229--1249},
  publisher = {{American Meteorological Society}},
  issn = {0027-0644},
  doi = {10.1175/MWR-D-19-0154.1},
  abstract = {State-of-the-art ensemble prediction systems usually provide ensembles with only 20\textendash 250 members for estimating the uncertainty of the forecast and its spatial and spatiotemporal covariance. Given that the degrees of freedom of atmospheric models are several magnitudes higher, the estimates are therefore substantially affected by sampling errors. For error covariances, spurious correlations lead to random sampling errors, but also a systematic overestimation of the correlation. A common approach to mitigate the impact of sampling errors for data assimilation is to localize correlations. However, this is a challenging task given that physical correlations in the atmosphere can extend over long distances. Besides data assimilation, sampling errors pose an issue for the investigation of spatiotemporal correlations using ensemble sensitivity analysis. Our study evaluates a statistical approach for correcting sampling errors. The applied sampling error correction is a lookup table\textendash based approach and therefore computationally very efficient. We show that this approach substantially improves both the estimates of spatial correlations for data assimilation as well as spatiotemporal correlations for ensemble sensitivity analysis. The evaluation is performed using the first convective-scale 1000-member ensemble simulation for central Europe. Correlations of the 1000-member ensemble forecast serve as truth to assess the performance of the sampling error correction for smaller subsets of the full ensemble. The sampling error correction strongly reduced both random and systematic errors for all evaluated variables, ensemble sizes, and lead times.},
  file = {/Users/milan/Zotero/storage/R4DTXBXK/Necker et al. - 2019 - Sampling Error Correction Evaluated Using a Convec.pdf;/Users/milan/Zotero/storage/Z9KUXWT3/MWR-D-19-0154.html}
}

@inproceedings{Ni2020,
  title = {{{WrapNet}}: {{Neural Net Inference}} with {{Ultra}}-{{Low}}-{{Precision Arithmetic}}},
  shorttitle = {{{WrapNet}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Ni, Renkun and Chu, Hong-min and Castaneda, Oscar and Chiang, Ping-yeh and Studer, Christoph and Goldstein, Tom},
  year = {2020},
  month = sep,
  abstract = {Low-precision neural networks represent both weights and activations with few bits, drastically reducing the cost of multiplications. Meanwhile, these products are accumulated using high-precision...},
  language = {en},
  file = {/Users/milan/Zotero/storage/CPNF7K3S/Ni et al. - 2020 - WrapNet Neural Net Inference with Ultra-Low-Preci.pdf;/Users/milan/Zotero/storage/BGC9CHSC/forum.html}
}

@article{Nicolas2014,
  title = {New {{Reconstruction}} of {{Antarctic Near}}-{{Surface Temperatures}}: {{Multidecadal Trends}} and {{Reliability}} of {{Global Reanalyses}}},
  shorttitle = {New {{Reconstruction}} of {{Antarctic Near}}-{{Surface Temperatures}}},
  author = {Nicolas, Julien P. and Bromwich, David H.},
  year = {2014},
  month = nov,
  journal = {Journal of Climate},
  volume = {27},
  number = {21},
  pages = {8070--8093},
  issn = {0894-8755, 1520-0442},
  doi = {10.1175/JCLI-D-13-00733.1},
  abstract = {A reconstruction of Antarctic monthly mean near-surface temperatures spanning 1958\textendash 2012 is presented. Its primary goal is to take advantage of a recently revised key temperature record from West Antarctica (Byrd) to shed further light on multidecadal temperature changes in this region. The spatial interpolation relies on a kriging technique aided by spatiotemporal temperature covariances derived from three global reanalyses [the European Centre for Medium-Range Weather Forecasts (ECMWF) Interim Re-Analysis (ERAInterim), Modern-Era Retrospective Analysis for Research and Applications (MERRA), and Climate Forecast System Reanalysis (CFSR)]. For 1958\textendash 2012, the reconstruction yields statistically significant annual warming in the Antarctic Peninsula and virtually all of West Antarctica, but no significant temperature change in East Antarctica. Importantly, the warming is of comparable magnitude both in central West Antarctica and in most of the peninsula, rather than concentrated either in one or the other region as previous reconstructions have suggested. The Transantarctic Mountains act for the temperature trends, as a clear dividing line between East and West Antarctica, reflecting the topographic constraint on warm air advection from the Amundsen Sea basin. The reconstruction also serves to highlight spurious changes in the 1979\textendash 2009 time series of the three reanalyses that reduces the reliability of their trends, illustrating a longstanding issue in high southern latitudes. The study concludes with an examination of the influence of the southern annular mode (SAM) on Antarctic temperature trends. The results herein suggest that the trend of the SAM toward its positive phase in austral summer and fall since the 1950s has had a statistically significant cooling effect not only in East Antarctica (as already well documented) and but also (only in fall) in West Antarctica.},
  language = {en}
}

@inproceedings{Odajima2020,
  title = {Preliminary {{Performance Evaluation}} of the {{Fujitsu A64FX Using HPC Applications}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Cluster Computing}} ({{CLUSTER}})},
  author = {Odajima, Tetsuya and Kodama, Yuetsu and Tsuji, Miwako and Matsuda, Motohiko and Maruyama, Yutaka and Sato, Mitsuhisa},
  year = {2020},
  month = sep,
  pages = {523--530},
  issn = {2168-9253},
  doi = {10.1109/CLUSTER49012.2020.00075},
  abstract = {RIKEN Center for Computational Science has been installing the supercomputer Fugaku. The Fujitsu A64FX, based on the Armv8.2-A+SVE architecture, is used in the system. In this paper, we evaluated the seven HPC applications and benchmarks on the A64FX. In a performance comparison with Marvell (Cavium) ThunderX2 processor and Intel Xeon Skylake processor, the A64FX achieved higher performance in a memory bandwidth-intensive application thanks to its high memory bandwidth. However, we confirmed that the performance of the A64FX decreased from a lack of out-of-order resources. To mitigate this problem, the ``loop fission'' function of the Fujitsu compiler was used to improve the performance.},
  keywords = {A64FX,Arm,Bandwidth,Benchmark testing,Computer architecture,Out of order,Performance Evaluation,Scientific computing,Supercomputers,Throughput},
  file = {/Users/milan/Zotero/storage/R9ZYZ5TY/Odajima et al. - 2020 - Preliminary Performance Evaluation of the Fujitsu .pdf;/Users/milan/Zotero/storage/L9QRWFVA/9229635.html}
}

@article{Palmer2001,
  title = {A Nonlinear Dynamical Perspective on Model Error: {{A}} Proposal for Non-Local Stochastic-Dynamic Parametrization in Weather and Climate Prediction Models},
  shorttitle = {A Nonlinear Dynamical Perspective on Model Error},
  author = {Palmer, T. N.},
  year = {2001},
  month = jan,
  journal = {Quarterly Journal of the Royal Meteorological Society},
  volume = {127},
  number = {572},
  pages = {279--304},
  issn = {00359009, 1477870X},
  doi = {10.1002/qj.49712757202},
  language = {en}
}

@article{Palmer2007,
  title = {Isolating the Signal of Ocean Global Warming: {{ISOLATING THE SIGNAL OF OCEAN WARMING}}},
  shorttitle = {Isolating the Signal of Ocean Global Warming},
  author = {Palmer, M. D. and Haines, K. and Tett, S. F. B. and Ansell, T. J.},
  year = {2007},
  month = dec,
  journal = {Geophysical Research Letters},
  volume = {34},
  number = {23},
  pages = {n/a-n/a},
  issn = {00948276},
  doi = {10.1029/2007GL031712},
  language = {en}
}

@book{Palmer2010,
  title = {Stochastic Physics and Climate Modelling},
  editor = {Palmer, Tim and Williams, Paul D.},
  year = {2010},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge, UK ; New York}},
  isbn = {978-0-521-76105-5},
  language = {en},
  lccn = {QC874.5 .S76 2010},
  keywords = {Climatology,Mathematical models,Meteorology,Statistical methods,Weather forecasting},
  annotation = {OCLC: ocn319495602}
}

@article{Palmer2012,
  title = {Towards the Probabilistic {{Earth}}-System Simulator: A Vision for the Future of Climate and Weather Prediction},
  shorttitle = {Towards the Probabilistic {{Earth}}-System Simulator},
  author = {Palmer, T. N.},
  year = {2012},
  month = apr,
  journal = {Quarterly Journal of the Royal Meteorological Society},
  volume = {138},
  number = {665},
  pages = {841--861},
  issn = {00359009},
  doi = {10.1002/qj.1923},
  language = {en}
}

@article{Palmer2012a,
  title = {Climate and {{Earth}}'s {{Energy Flows}}},
  author = {Palmer, Matthew D.},
  year = {2012},
  month = jul,
  journal = {Surveys in Geophysics},
  volume = {33},
  number = {3-4},
  pages = {351--357},
  issn = {0169-3298, 1573-0956},
  doi = {10.1007/s10712-011-9165-8},
  abstract = {Under equilibrium conditions, climate can be viewed in simple terms as the average energy pathways that incoming solar radiation takes before exiting the system in order to maintain overall energy balance. Similarly, future climate change will ultimately be determined by how the Earth's energy balance and average energy pathways change in response to external radiative forcings, such as anthropogenic greenhouse gases, and internal redistributions. Here, we give an overview of climate research in the context of Earth's energy flows and make the case for improved observations of total energy as a more physically robust metric of climate change than the commonly used surface temperature record.},
  language = {en}
}

@article{Palmer2013,
  title = {Singular Vectors, Predictability and Ensemble Forecasting for Weather and Climate},
  author = {Palmer, T N and Zanna, Laure},
  year = {2013},
  month = jun,
  journal = {Journal of Physics A: Mathematical and Theoretical},
  volume = {46},
  number = {25},
  pages = {254018},
  issn = {1751-8113, 1751-8121},
  doi = {10.1088/1751-8113/46/25/254018},
  abstract = {The local instabilities of a nonlinear dynamical system can be characterised by the leading singular vectors of its linearized operator. The leading singular vectors are perturbations with the greatest linear growth and are therefore key in assessing the system's predictability. In this paper, the analysis of singular vectors for the predictability of weather and climate and ensemble forecasting is discussed. An overview of the role of singular vectors in informing about the error growth rate in numerical models of the atmosphere is given. This is followed by their use in the initialisation of ensemble weather forecasts. Singular vectors for the ocean and coupled ocean-atmosphere system in order to understand the predictability of climate phenomena such as ENSO and meridional overturning circulation are reviewed and their potential use to initialise seasonal and decadal forecasts is considered. As stochastic parameterisations are being implemented, some speculations are made about the future of singular vectors for the predictability of weather and climate for theoretical applications and at the operational level.},
  language = {en}
}

@article{Palmer2014,
  title = {Stochastic Modelling and Energy-Efficient Computing for Weather and Climate Prediction},
  author = {Palmer, Tim and D{\"u}ben, Peter and McNamara, Hugh},
  year = {2014},
  month = jun,
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {372},
  number = {2018},
  pages = {20140118},
  issn = {1364-503X, 1471-2962},
  doi = {10.1098/rsta.2014.0118},
  language = {en}
}

@article{Palmer2014a,
  title = {Climate Forecasting: {{Build}} High-Resolution Global Climate Models},
  shorttitle = {Climate Forecasting},
  author = {Palmer, Tim},
  year = {2014},
  month = nov,
  journal = {Nature News},
  volume = {515},
  number = {7527},
  pages = {338},
  doi = {10.1038/515338a},
  abstract = {International supercomputing centres dedicated to climate prediction are needed to reduce uncertainties in global warming, says Tim Palmer.},
  chapter = {Comment},
  language = {en},
  file = {/Users/milan/Zotero/storage/G3BB97WN/climate-forecasting-build-high-resolution-global-climate-models-1.html}
}

@article{Palmer2015,
  title = {Modelling: {{Build}} Imprecise Supercomputers},
  shorttitle = {Modelling},
  author = {Palmer, Tim},
  year = {2015},
  month = oct,
  journal = {Nature News},
  volume = {526},
  number = {7571},
  pages = {32},
  doi = {10.1038/526032a},
  abstract = {Energy-optimized hybrid computers with a range of processor accuracies will advance modelling in fields from climate change to neuroscience, says Tim Palmer.},
  chapter = {Comment},
  language = {en},
  file = {/Users/milan/Zotero/storage/QL9D2WFY/modelling-build-imprecise-supercomputers-1.html}
}

@article{Palmer2015a,
  title = {Solving Difficult Problems Creatively: A Role for Energy Optimised Deterministic/Stochastic Hybrid Computing},
  shorttitle = {Solving Difficult Problems Creatively},
  author = {Palmer, Tim N. and O'Shea, Michael},
  year = {2015},
  month = oct,
  journal = {Frontiers in Computational Neuroscience},
  volume = {9},
  issn = {1662-5188},
  doi = {10.3389/fncom.2015.00124},
  abstract = {How is the brain configured for creativity? What is the computational substrate for `eureka' moments of insight? Here we argue that creative thinking arises ultimately from a synergy between low-energy stochastic and energy-intensive deterministic processing, and is a by-product of a nervous system whose signal-processing capability per unit of available energy has become highly energy optimised. We suggest that the stochastic component has its origin in thermal (ultimately quantum decoherent) noise affecting the activity of neurons. Without this component, deterministic computational models of the brain are incomplete.},
  language = {en}
}

@article{Palmer2016,
  title = {A Personal Perspective on Modelling the Climate System},
  author = {Palmer, T. N.},
  year = {2016},
  month = apr,
  journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {472},
  number = {2188},
  pages = {20150772},
  issn = {1364-5021, 1471-2946},
  doi = {10.1098/rspa.2015.0772},
  language = {en}
}

@article{Palmer2019,
  title = {Stochastic Weather and Climate Models},
  author = {Palmer, T. N.},
  year = {2019},
  month = jul,
  journal = {Nature Reviews Physics},
  volume = {1},
  number = {7},
  pages = {463--471},
  publisher = {{Nature Publishing Group}},
  issn = {2522-5820},
  doi = {10.1038/s42254-019-0062-2},
  abstract = {Although the differential equations that describe the physical climate system are deterministic, there are reasons, both theoretical and practical, why computational representations of these equations should be stochastic. This Perspective surveys the benefits of stochastic modelling of weather and climate.},
  copyright = {2019 The Publisher},
  language = {en},
  file = {/Users/milan/Zotero/storage/87DNANFD/s42254-019-0062-2.html}
}

@article{Palmer2019a,
  title = {The {{ECMWF}} Ensemble Prediction System: {{Looking}} Back (More than) 25 Years and Projecting Forward 25 Years},
  shorttitle = {The {{ECMWF}} Ensemble Prediction System},
  author = {Palmer, Tim},
  year = {2019},
  journal = {Quarterly Journal of the Royal Meteorological Society},
  volume = {145},
  number = {S1},
  pages = {12--24},
  issn = {1477-870X},
  doi = {10.1002/qj.3383},
  abstract = {This paper has been written to mark 25 years of operational medium-range ensemble forecasting. The origins of the ECMWF Ensemble Prediction System are outlined, including the development of the precursor real-time Met Office monthly ensemble forecast system. In particular, the reasons for the development of singular vectors and stochastic physics \textendash{} particular features of the ECMWF Ensemble Prediction System - are discussed. The author speculates about the development and use of ensemble prediction in the next 25 years.},
  copyright = {\textcopyright{} 2018 The Authors. Quarterly Journal of the Royal Meteorological Society published by John Wiley \& Sons Ltd on behalf of the Royal Meteorological Society.},
  language = {en},
  keywords = {disaster relief reduction,ensemble prediction,singular vectors,stochastic physics,the butterfly effect},
  annotation = {\_eprint: https://rmets.onlinelibrary.wiley.com/doi/pdf/10.1002/qj.3383},
  file = {/Users/milan/Zotero/storage/IMKQCQAP/Palmer - 2019 - The ECMWF ensemble prediction system Looking back.pdf;/Users/milan/Zotero/storage/EW2DXD5I/qj.html}
}

@article{Pana2021,
  title = {Country-Level Determinants of the Severity of the First Global Wave of the {{COVID}}-19 Pandemic: An Ecological Study},
  shorttitle = {Country-Level Determinants of the Severity of the First Global Wave of the {{COVID}}-19 Pandemic},
  author = {Pana, Tiberiu A. and Bhattacharya, Sohinee and Gamble, David T. and Pasdar, Zahra and Szlachetka, Weronika A. and {Perdomo-Lampignano}, Jesus A. and Ewers, Kai D. and McLernon, David J. and Myint, Phyo K.},
  year = {2021},
  month = feb,
  journal = {BMJ Open},
  volume = {11},
  number = {2},
  pages = {e042034},
  publisher = {{British Medical Journal Publishing Group}},
  issn = {2044-6055, 2044-6055},
  doi = {10.1136/bmjopen-2020-042034},
  abstract = {Objective We aimed to identify the country-level determinants of the severity of the first wave of the COVID-19 pandemic. Design Ecological study of publicly available data. Countries reporting {$>$}25 COVID-19 related deaths until 8 June 2020 were included. The outcome was log mean mortality rate from COVID-19, an estimate of the country-level daily increase in reported deaths during the ascending phase of the epidemic curve. Potential determinants assessed were most recently published demographic parameters (population and population density, percentage population living in urban areas, population {$>$}65 years, average body mass index and smoking prevalence); economic parameters (gross domestic product per capita); environmental parameters (pollution levels and mean temperature (January\textendash May); comorbidities (prevalence of diabetes, hypertension and cancer); health system parameters (WHO Health Index and hospital beds per 10 000 population); international arrivals; the stringency index, as a measure of country-level response to COVID-19; BCG vaccination coverage; UV radiation exposure; and testing capacity. Multivariable linear regression was used to analyse the data. Primary outcome Country-level mean mortality rate: the mean slope of the COVID-19 mortality curve during its ascending phase. Participants Thirty-seven countries were included: Algeria, Argentina, Austria, Belgium, Brazil, Canada, Chile, Colombia, the Dominican Republic, Ecuador, Egypt, Finland, France, Germany, Hungary, India, Indonesia, Ireland, Italy, Japan, Mexico, the Netherlands, Peru, the Philippines, Poland, Portugal, Romania, the Russian Federation, Saudi Arabia, South Africa, Spain, Sweden, Switzerland, Turkey, Ukraine, the UK and the USA. Results Of all country-level determinants included in the multivariable model, total number of international arrivals (beta 0.033 (95\% CI 0.012 to 0.054)) and BCG vaccination coverage (-0.018 (95\% CI -0.034 to \textendash 0.002)), were significantly associated with the natural logarithm of the mean death rate. Conclusions International travel was directly associated with the mortality slope and thus potentially the spread of COVID-19. Very early restrictions on international travel should be considered to control COVID-19 outbreaks and prevent related deaths.},
  chapter = {Public health},
  copyright = {\textcopyright{} Author(s) (or their employer(s)) 2021. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ.. http://creativecommons.org/licenses/by-nc/4.0/This is an open access article distributed in accordance with the Creative Commons Attribution Non Commercial (CC BY-NC 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited, appropriate credit is given, any changes made indicated, and the use is non-commercial. See:~http://creativecommons.org/licenses/by-nc/4.0/.},
  language = {en},
  pmid = {33536319},
  keywords = {epidemiology,infectious diseases,public health},
  file = {/Users/milan/Zotero/storage/A6582ZF8/Pana et al. - 2021 - Country-level determinants of the severity of the .pdf;/Users/milan/Zotero/storage/WS3TVA8E/e042034.html}
}

@article{Parry1960,
  title = {On the {$\beta$}-Expansions of Real Numbers},
  author = {Parry, W.},
  year = {1960},
  month = sep,
  journal = {Acta Mathematica Academiae Scientiarum Hungarica},
  volume = {11},
  number = {3},
  pages = {401--416},
  issn = {1588-2632},
  doi = {10.1007/BF02020954},
  language = {en}
}

@article{Pathak2017,
  title = {Using Machine Learning to Replicate Chaotic Attractors and Calculate {{Lyapunov}} Exponents from Data},
  author = {Pathak, Jaideep and Lu, Zhixin and Hunt, Brian R. and Girvan, Michelle and Ott, Edward},
  year = {2017},
  month = dec,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {27},
  number = {12},
  pages = {121102},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.5010300},
  language = {en}
}

@article{Paxton2021,
  title = {Climate {{Modelling}} in {{Low}}-{{Precision}}: {{Effects}} of Both {{Deterministic}} \& {{Stochastic Rounding}}},
  shorttitle = {Climate {{Modelling}} in {{Low}}-{{Precision}}},
  author = {Paxton, E. Adam and Chantry, Matthew and Kl{\"o}wer, Milan and Saffin, Leo and Palmer, Tim},
  year = {2021},
  month = apr,
  journal = {arXiv},
  eprint = {2104.15076},
  eprinttype = {arxiv},
  abstract = {Motivated by recent advances in operational weather forecasting, we study the efficacy of low-precision arithmetic for climate simulations. We develop a framework to measure rounding error in a climate model which provides a stress-test for a low-precision version of the model, and we apply our method to a variety of models including the Lorenz system; a shallow water approximation for flow over a ridge; and a coarse resolution global atmospheric model with simplified parameterisations (SPEEDY). Although double precision (52 significant bits) is standard across operational climate models, in our experiments we find that single precision (23 sbits) is more than enough and that as low as half precision (10 sbits) is often sufficient. For example, SPEEDY can be run with 12 sbits across the entire code with negligible rounding error and this can be lowered to 10 sbits if very minor errors are accepted, amounting to less than 0.1 mm/6hr for the average grid-point precipitation, for example. Our test is based on the Wasserstein metric and this provides stringent non-parametric bounds on rounding error accounting for annual means as well as extreme weather events. In addition, by testing models using both round-to-nearest (RN) and stochastic rounding (SR) we find that SR can mitigate rounding error across a range of applications. Thus our results also provide evidence that SR could be relevant to next-generation climate models. While many studies have shown that low-precision arithmetic can be suitable on short-term weather forecasting timescales, our results give the first evidence that a similar low precision level can be suitable for climate.},
  archiveprefix = {arXiv},
  keywords = {Physics - Atmospheric and Oceanic Physics},
  file = {/Users/milan/Zotero/storage/ZNY557HS/Paxton et al. - 2021 - Climate Modelling in Low-Precision Effects of bot.pdf;/Users/milan/Zotero/storage/X5P9F68M/2104.html}
}

@article{Pelkonen2015,
  title = {Gorilla: A Fast, Scalable, in-Memory Time Series Database},
  shorttitle = {Gorilla},
  author = {Pelkonen, Tuomas and Franklin, Scott and Teller, Justin and Cavallaro, Paul and Huang, Qi and Meza, Justin and Veeraraghavan, Kaushik},
  year = {2015},
  month = aug,
  journal = {Proceedings of the VLDB Endowment},
  volume = {8},
  number = {12},
  pages = {1816--1827},
  issn = {2150-8097},
  doi = {10.14778/2824032.2824078},
  abstract = {Large-scale internet services aim to remain highly available and responsive in the presence of unexpected failures. Providing this service often requires monitoring and analyzing tens of millions of measurements per second across a large number of systems, and one particularly effective solution is to store and query such measurements in a time series database (TSDB). A key challenge in the design of TSDBs is how to strike the right balance between efficiency, scalability, and reliability. In this paper we introduce Gorilla, Facebook's in-memory TSDB. Our insight is that users of monitoring systems do not place much emphasis on individual data points but rather on aggregate analysis, and recent data points are of much higher value than older points to quickly detect and diagnose the root cause of an ongoing problem. Gorilla optimizes for remaining highly available for writes and reads, even in the face of failures, at the expense of possibly dropping small amounts of data on the write path. To improve query efficiency, we aggressively leverage compression techniques such as delta-of-delta timestamps and XOR'd floating point values to reduce Gorilla's storage footprint by 10x. This allows us to store Gorilla's data in memory, reducing query latency by 73x and improving query throughput by 14x when compared to a traditional database (HBase)-backed time series data. This performance improvement has unlocked new monitoring and debugging tools, such as time series correlation search and more dense visualization tools. Gorilla also gracefully handles failures from a single-node to entire regions with little to no operational overhead.},
  file = {/Users/milan/Zotero/storage/TQ9E9RLG/Pelkonen et al. - 2015 - Gorilla a fast, scalable, in-memory time series d.pdf}
}

@article{Peterson2001,
  title = {Vorticity Fluxes in Shallow Water Ocean Models},
  author = {Peterson, K. Andrew and Greatbatch, Richard J.},
  year = {2001},
  month = mar,
  journal = {Atmosphere-Ocean},
  volume = {39},
  number = {1},
  pages = {1--14},
  issn = {0705-5900, 1480-9214},
  doi = {10.1080/07055900.2001.9649662},
  abstract = {We investigate some of the claims concerning new eddy parametrization schemes for coarse resolution ocean models by analyzing the role of eddies in eddy-resolving 11{$\fracslash$}2 and 21{$\fracslash$}2 layer models with double-gyre wind forcing. We find that the divergent part of the eddy flux of potential vorticity is directed down the mean thickness-weighted potential vorticity gradient. The relationship between the eddy flux of thickness and either the mean thickness or mean thickness-weighted potential vorticity gradients is less clear. The analysis is complicated by the presence of large rotational fluxes of both thickness and potential vorticity and by the importance of the Reynolds stress terms. We argue that the rotational fluxes have a fundamental role to play and need to be parametrized in a comprehensive parametrization scheme.},
  language = {en}
}

@inproceedings{Pinard2020,
  title = {Assessing {{Differences}} in {{Large Spatio}}-Temporal {{Climate Datasets}} with a {{New Python}} Package},
  booktitle = {2020 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  author = {Pinard, Alexander and Hammerling, Dorit M. and Baker, Allison H.},
  year = {2020},
  month = dec,
  pages = {2699--2707},
  publisher = {{IEEE}},
  address = {{Atlanta, GA, USA}},
  doi = {10.1109/BigData50022.2020.9378100},
  abstract = {Output data from modern Earth system model simulations are consuming increasingly massive amounts of storage resources, and storing these climate model data is not economically sustainable. Previous works have motivated lossy compression as a potential solution, which achieves greater compression ratios than lossless compression. This further reduction comes at the cost of a loss of information, and therefore, care must be taken to avoid introducing artifacts in the data that could affect scientific conclusions. In this paper we introduce a Python package designed to aid in the analysis of differences in large spatio-temporal datasets, such as those produced by global climate models. While the new package is agnostic to the source of the differences, our motivation is to enable climate scientists to more easily assess the effects of lossy data compression by visualizing and computing derived spatial-temporal quantities that compare lossily compressed datasets to the original dataset. Because Python is quickly becoming the tool of choice for scientific data analysis in the geoscience community, this new package makes use of the Python software stack in Pangeo (an active NSF-funded community platform for Big Data geoscience). Interoperability with other Pangeo software tools means that the new package easily integrates into climate scientists' postprocessing and analysis workflows, which we hope will facilitate the adoption of lossy compression into the climate modeling community.},
  isbn = {978-1-72816-251-5},
  language = {en},
  file = {/Users/milan/Zotero/storage/2MB8S5XX/Pinard et al. - 2020 - Assessing Differences in Large Spatio-temporal Cli.pdf}
}

@article{Poppick2020,
  title = {A Statistical Analysis of Lossily Compressed Climate Model Data},
  author = {Poppick, Andrew and Nardi, Joseph and Feldman, Noah and Baker, Allison H. and Pinard, Alexander and Hammerling, Dorit M.},
  year = {2020},
  month = dec,
  journal = {Computers \& Geosciences},
  volume = {145},
  pages = {104599},
  issn = {0098-3004},
  doi = {10.1016/j.cageo.2020.104599},
  abstract = {The data storage burden resulting from large climate model simulations continues to grow. While lossy data compression methods can alleviate this burden, they introduce the possibility that key climate variables could be altered to the point of affecting scientific conclusions. Therefore, developing a detailed understanding of how compressed model output differs from the original is important. Here, we evaluate the effects of two leading compression algorithms, sz and zfp, on daily surface temperature and precipitation rate data from a widely used climate model. While both algorithms show promising fidelity with the original output, detectable artifacts are introduced even at relatively tight error tolerances. This study highlights the need for evaluation methods that are sensitive to errors at different spatiotemporal scales and specific to the particular climate variable of interest.},
  language = {en},
  keywords = {CESM,Climate variability,Earth system models,Lossy compression},
  file = {/Users/milan/Zotero/storage/CLYMPZHI/Poppick et al. - 2020 - A statistical analysis of lossily compressed clima.pdf;/Users/milan/Zotero/storage/6H369FMH/S009830042030580X.html}
}

@article{PortaMana2014,
  title = {Toward a Stochastic Parameterization of Ocean Mesoscale Eddies},
  author = {Porta Mana, PierGianLuca and Zanna, Laure},
  year = {2014},
  month = jul,
  journal = {Ocean Modelling},
  volume = {79},
  pages = {1--20},
  issn = {14635003},
  doi = {10.1016/j.ocemod.2014.04.002},
  abstract = {A stochastic parameterization of ocean mesoscale eddies is constructed in order to account for the fluctuations in subgrid transport and to represent upscale turbulent cascades. Eddy-resolving simulations to derive the parameterization are performed in a quasi-geostrophic (QG) model in a double-gyre configuration. The coarse-graining of the high-resolution model is giving rise to an eddy source term which represents the turbulent Reynolds stresses. The eddy source term, its mean and fluctuations are analyzed as function of the resolved scales and external parameters.},
  language = {en}
}

@article{Pothapakula2019,
  title = {Quantification of {{Information Exchange}} in {{Idealized}} and {{Climate System Applications}}},
  author = {Pothapakula, Praveen Kumar and Primo, Cristina and Ahrens, Bodo},
  year = {2019},
  month = nov,
  journal = {Entropy},
  volume = {21},
  number = {11},
  pages = {1094},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/e21111094},
  abstract = {Often in climate system studies, linear and symmetric statistical measures are applied to quantify interactions among subsystems or variables. However, they do not allow identification of the driving and responding subsystems. Therefore, in this study, we aimed to apply asymmetric measures from information theory: the axiomatically proposed transfer entropy and the first principle-based information flow to detect and quantify climate interactions. As their estimations are challenging, we initially tested nonparametric estimators like transfer entropy (TE)-binning, TE-kernel, and TE k-nearest neighbor and parametric estimators like TE-linear and information flow (IF)-linear with idealized two-dimensional test cases along with their sensitivity on sample size. Thereafter, we experimentally applied these methods to the Lorenz-96 model and to two real climate phenomena, i.e., (1) the Indo-Pacific Ocean coupling and (2) North Atlantic Oscillation (NAO)\&ndash;European air temperature coupling. As expected, the linear estimators work for linear systems but fail for strongly nonlinear systems. The TE-kernel and TE k-nearest neighbor estimators are reliable for linear and nonlinear systems. Nevertheless, the nonparametric methods are sensitive to parameter selection and sample size. Thus, this work proposes a composite use of the TE-kernel and TE k-nearest neighbor estimators along with parameter testing for consistent results. The revealed information exchange in Lorenz-96 is dominated by the slow subsystem component. For real climate phenomena, expected bidirectional information exchange between the Indian and Pacific SSTs was detected. Furthermore, expected information exchange from NAO to European air temperature was detected, but also unexpected reversal information exchange. The latter might hint to a hidden process driving both the NAO and European temperatures. Hence, the limitations, availability of time series length and the system at hand must be taken into account before drawing any conclusions from TE and IF-linear estimations.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  language = {en},
  keywords = {climate informatics,El Niño,Indian Ocean dipole,information flow,transfer entropy},
  file = {/Users/milan/Zotero/storage/3IEMFYE9/Pothapakula et al. - 2019 - Quantification of Information Exchange in Idealize.pdf;/Users/milan/Zotero/storage/P5LUVND7/1094.html}
}

@article{Pothapakula2019a,
  title = {Quantification of {{Information Exchange}} in {{Idealized}} and {{Climate System Applications}}},
  author = {Pothapakula, Praveen Kumar and Primo, Cristina and Ahrens, Bodo},
  year = {2019},
  month = nov,
  journal = {Entropy},
  volume = {21},
  number = {11},
  pages = {1094},
  issn = {1099-4300},
  doi = {10.3390/e21111094},
  abstract = {Often in climate system studies, linear and symmetric statistical measures are applied to quantify interactions among subsystems or variables. However, they do not allow identification of the driving and responding subsystems. Therefore, in this study, we aimed to apply asymmetric measures from information theory: the axiomatically proposed transfer entropy and the first principle-based information flow to detect and quantify climate interactions. As their estimations are challenging, we initially tested nonparametric estimators like transfer entropy (TE)-binning, TE-kernel, and TE k-nearest neighbor and parametric estimators like TE-linear and information flow (IF)-linear with idealized two-dimensional test cases along with their sensitivity on sample size. Thereafter, we experimentally applied these methods to the Lorenz-96 model and to two real climate phenomena, i.e., (1) the Indo-Pacific Ocean coupling and (2) North Atlantic Oscillation (NAO)\textendash European air temperature coupling. As expected, the linear estimators work for linear systems but fail for strongly nonlinear systems. The TE-kernel and TE k-nearest neighbor estimators are reliable for linear and nonlinear systems. Nevertheless, the nonparametric methods are sensitive to parameter selection and sample size. Thus, this work proposes a composite use of the TE-kernel and TE k-nearest neighbor estimators along with parameter testing for consistent results. The revealed information exchange in Lorenz-96 is dominated by the slow subsystem component. For real climate phenomena, expected bidirectional information exchange between the Indian and Pacific SSTs was detected. Furthermore, expected information exchange from NAO to European air temperature was detected, but also unexpected reversal information exchange. The latter might hint to a hidden process driving both the NAO and European temperatures. Hence, the limitations, availability of time series length and the system at hand must be taken into account before drawing any conclusions from TE and IF-linear estimations.},
  language = {en},
  file = {/Users/milan/Zotero/storage/8M3MAQBI/Pothapakula et al. - 2019 - Quantification of Information Exchange in Idealize.pdf}
}

@inproceedings{Preist2019,
  title = {Evaluating {{Sustainable Interaction Design}} of {{Digital Services}}: {{The Case}} of {{YouTube}}},
  shorttitle = {Evaluating {{Sustainable Interaction Design}} of {{Digital Services}}},
  booktitle = {Proceedings of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Preist, Chris and Schien, Daniel and Shabajee, Paul},
  year = {2019},
  month = may,
  series = {{{CHI}} '19},
  pages = {1--12},
  publisher = {{Association for Computing Machinery}},
  address = {{Glasgow, Scotland Uk}},
  doi = {10.1145/3290605.3300627},
  abstract = {Recent research has advocated for a broader conception of evaluation for Sustainable HCI (SHCI), using interdisciplinary insights and methods. In this paper, we put this into practice to conduct an evaluation of Sustainable Interaction Design (SID) of digital services. We explore how SID can contribute to corporate greenhouse gas (GHG) reduction strategies. We show how a Digital Service Provider (DSP) might incorporate SID into their design process and quantitatively evaluate a specific SID intervention by combining user analytics data with environmental life cycle assessment. We illustrate this by considering YouTube. Replacing user analytics data with aggregate estimates from publicly available sources, we estimate emissions associated with the deployment of YouTube to be approximately 10MtCO2e p.a. We estimate emissions reductions enabled through the use of an SID intervention from prior literature to be approximately 300KtCO2e p.a., and demonstrate that this is significant when considered alongside other emissions reduction interventions used by DSPs.},
  isbn = {978-1-4503-5970-2},
  keywords = {digital service,evaluation,interaction design,sustainability,sustainable hci},
  file = {/Users/milan/Zotero/storage/3P6QZ67E/Preist et al. - 2019 - Evaluating Sustainable Interaction Design of Digit.pdf}
}

@article{Proud2020,
  title = {Go-{{Around Detection Using Crowd}}-{{Sourced ADS}}-{{B Position Data}}},
  author = {Proud, Simon Richard},
  year = {2020},
  month = feb,
  journal = {Aerospace},
  volume = {7},
  number = {2},
  pages = {16},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/aerospace7020016},
  abstract = {The decision of a flight crew to undertake a go-around, aborting a landing attempt, is primarily to ensure the safe conduct of a flight. Although go-arounds are rare, they do cause air traffic disruption, especially in busy airspace, due to the need to accommodate an aircraft in an unusual position, and a go-around can also result in knock-on delays due to the time taken for the aircraft to re-position, fit into the landing sequence and execute a successful landing. Therefore, it is important to understand and alleviate the factors that can result in a go-around. In this paper, I present a new method for automatically detecting go-around events in aircraft position data, such as that sent via the ADS-B system, and apply the method to one year of approach data for Chhatrapati Shivaji Maharaj International Airport (VABB) in Mumbai, India. I show that the method is significantly more accurate than other methods, detecting go-arounds with very few false positives or negatives. Finally, I use the new method to reveal that while there is no one cause for go-arounds at this airport, the majority can be attributed to weather and/or an unstable approach. I also show that one runway (14/32) has a significantly higher proportion of go-arounds than the other (09/27).},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  language = {en},
  keywords = {ADS-B,airports,approach,go-around,safety},
  file = {/Users/milan/Zotero/storage/CTKKNB3B/Proud - 2020 - Go-Around Detection Using Crowd-Sourced ADS-B Posi.pdf;/Users/milan/Zotero/storage/KUIQTXGN/16.html}
}

@article{Randall,
  title = {Climate {{Models}} and {{Their Evaluation}}},
  author = {Randall, David A and Wood, Richard A and Bony, Sandrine and Colman, Robert and Fichefet, Thierry and Fyfe, John and Kattsov, Vladimir and Pitman, Andrew and Shukla, Jagadish and Srinivasan, Jayaraman and Stouffer, Ronald J and Sumi, Akimasa and Taylor, Karl E and AchutaRao, K and Allan, R and Berger, A and Blatter, H and Bonfils, C and Boone, A and Bretherton, C and Broccoli, A and Brovkin, V and Dirmeyer, P and Doutriaux, C and Drange, H and Frei, A and Ganopolski, A and Gent, P and Gleckler, P and Goosse, H and Graham, R and Gregory, J M and Gudgel, R and Hall, A and Hallegatte, S and Hasumi, H and {Henderson-Sellers}, A and Hendon, H and Hodges, K and Holland, M and Holtslag, A A M and Hunke, E and Huybrechts, P and Ingram, W and Joos, F and Kirtman, B and Klein, S and Koster, R and Kushner, P and Lanzante, J and Latif, M and Pavlova, T and Federationi, Russian and Petoukhov, V and Phillips, T and Power, S and Rahmstorf, S and Raper, S C B and Renssen, H and Rind, D and Roberts, M and Rosati, A and Sch{\"a}r, C and Schmittner, A and Scinocca, J and Seidov, D and Slater, A G and Slingo, J and Smith, D and Soden, B and Stern, W and Stone, D A and Sudo, K and Takemura, T and Tselioudis, G and Webb, M and Wild, M and Manzini, Elisa and Matsuno, Taroh and McAvaney, Bryant},
  pages = {74},
  language = {en}
}

@article{Rasp2018,
  title = {Deep Learning to Represent Subgrid Processes in Climate Models},
  author = {Rasp, Stephan and Pritchard, Michael S. and Gentine, Pierre},
  year = {2018},
  month = sep,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {39},
  pages = {9684--9689},
  file = {/Users/milan/Zotero/storage/4FW52IRQ/9684.html}
}

@article{Rasp2020,
  title = {{{WeatherBench}}: {{A Benchmark Data Set}} for {{Data}}-{{Driven Weather Forecasting}}},
  shorttitle = {{{WeatherBench}}},
  author = {Rasp, Stephan and Dueben, Peter D. and Scher, Sebastian and Weyn, Jonathan A. and Mouatadid, Soukayna and Thuerey, Nils},
  year = {2020},
  journal = {Journal of Advances in Modeling Earth Systems},
  volume = {12},
  number = {11},
  pages = {e2020MS002203},
  issn = {1942-2466},
  doi = {10.1029/2020MS002203},
  abstract = {Data-driven approaches, most prominently deep learning, have become powerful tools for prediction in many domains. A natural question to ask is whether data-driven methods could also be used to predict global weather patterns days in advance. First studies show promise but the lack of a common data set and evaluation metrics make intercomparison between studies difficult. Here we present a benchmark data set for data-driven medium-range weather forecasting (specifically 3\textendash 5 days), a topic of high scientific interest for atmospheric and computer scientists alike. We provide data derived from the ERA5 archive that has been processed to facilitate the use in machine learning models. We propose simple and clear evaluation metrics which will enable a direct comparison between different methods. Further, we provide baseline scores from simple linear regression techniques, deep learning models, as well as purely physical forecasting models. The data set is publicly available at https://github.com/pangeo-data/WeatherBench and the companion code is reproducible with tutorials for getting started. We hope that this data set will accelerate research in data-driven weather forecasting.},
  copyright = {\textcopyright 2020. The Authors.},
  language = {en},
  keywords = {artificial intelligence,benchmark,machine learning,NWP},
  annotation = {\_eprint: https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2020MS002203},
  file = {/Users/milan/Zotero/storage/MMWH56EY/Rasp et al. - 2020 - WeatherBench A Benchmark Data Set for Data-Driven.pdf;/Users/milan/Zotero/storage/B8FFKVC3/2020MS002203.html}
}

@article{Rauscher2013,
  title = {Exploring a {{Global Multiresolution Modeling Approach Using Aquaplanet Simulations}}},
  author = {Rauscher, Sara A. and Ringler, Todd D. and Skamarock, William C. and Mirin, Arthur A.},
  year = {2013},
  month = apr,
  journal = {Journal of Climate},
  volume = {26},
  number = {8},
  pages = {2432--2452},
  issn = {0894-8755, 1520-0442},
  doi = {10.1175/JCLI-D-12-00154.1},
  abstract = {Results from aquaplanet experiments performed using the Model for Prediction across Scales (MPAS) hydrostatic dynamical core implemented within the Department of Energy (DOE)\textendash NCAR Community Atmosphere Model (CAM) are presented. MPAS is an unstructured-grid approach to climate system modeling that supports both quasi-uniform and variable-resolution meshing of the sphere based on conforming grids. Using quasi-uniform simulations at resolutions of 30, 60, 120, and 240 km, the authors evaluate the performance of CAM-MPAS via its kinetic energy spectra, general circulation, and precipitation characteristics. By analyzing an additional variable-resolution simulation with grid spacing that varies from 30 km in a spherical, continental-sized equatorial region to 240 km elsewhere, the CAM-MPAS's potential for use as a regional climate simulation tool is explored.},
  language = {en}
}

@article{Rayner2003,
  title = {Global Analyses of Sea Surface Temperature, Sea Ice, and Night Marine Air Temperature since the Late Nineteenth Century},
  author = {Rayner, N. A.},
  year = {2003},
  journal = {Journal of Geophysical Research},
  volume = {108},
  number = {D14},
  pages = {4407},
  issn = {0148-0227},
  doi = {10.1029/2002JD002670},
  language = {en}
}

@article{Rhines,
  title = {Geostrophic {{Turbulence}}},
  author = {Rhines, P B},
  pages = {41},
  language = {en}
}

@article{Rudisuhli2013,
  title = {{{COSMO}} in Single Precision},
  author = {R{\"u}dis{\"u}hli, Stefan and Walser, Andr{\'e} and Fuhrer, Oliver},
  year = {2013},
  journal = {Cosmo Newsletter},
  number = {14},
  pages = {5--1},
  file = {/Users/milan/Zotero/storage/ZJHTAGB2/Rüdisühli et al. - 2013 - COSMO in single precision.pdf}
}

@article{Ruelle2002,
  title = {Dynamical {{Zeta Functions}} and {{Transfer Operators}}},
  author = {Ruelle, David},
  year = {2002},
  volume = {49},
  number = {8},
  pages = {9},
  language = {en},
  file = {/Users/milan/Zotero/storage/P9SDJUFH/Ruelle - 2002 - Dynamical Zeta Functions and Transfer Operators.pdf}
}

@article{Russell2017,
  title = {Exploiting the Chaotic Behaviour of Atmospheric Models with Reconfigurable Architectures},
  author = {Russell, Francis P. and D{\"u}ben, Peter D. and Niu, Xinyu and Luk, Wayne and Palmer, T.N.},
  year = {2017},
  month = dec,
  journal = {Computer Physics Communications},
  volume = {221},
  pages = {160--173},
  issn = {00104655},
  doi = {10.1016/j.cpc.2017.08.011},
  abstract = {Reconfigurable architectures are becoming mainstream: Amazon, Microsoft and IBM are supporting such architectures in their data centres. The computationally intensive nature of atmospheric modelling is an attractive target for hardware acceleration using reconfigurable computing. Performance of hardware designs can be improved through the use of reduced-precision arithmetic, but maintaining appropriate accuracy is essential. We explore reduced-precision optimisation for simulating chaotic systems, targeting atmospheric modelling, in which even minor changes in arithmetic behaviour will cause simulations to diverge quickly. The possibility of equally valid simulations having differing outcomes means that standard techniques for comparing numerical accuracy are inappropriate. We use the Hellinger distance to compare statistical behaviour between reduced-precision CPU implementations to guide reconfigurable designs of a chaotic system, then analyse accuracy, performance and power efficiency of the resulting implementations. Our results show that with only a limited loss in accuracy corresponding to less than 10\% uncertainty in input parameters, the throughput and energy efficiency of a single-precision chaotic system implemented on a Xilinx Virtex-6 SX475T Field Programmable Gate Array (FPGA) can be more than doubled.},
  language = {en}
}

@article{Sabherwal,
  title = {The {{Greta Thunberg Effect}}: {{Familiarity}} with {{Greta Thunberg}} Predicts Intentions to Engage in Climate Activism in the {{United States}}},
  shorttitle = {The {{Greta Thunberg Effect}}},
  author = {Sabherwal, Anandita and Ballew, Matthew T. and van der Linden, Sander and Gustafson, Abel and Goldberg, Matthew H. and Maibach, Edward W. and Kotcher, John E. and Swim, Janet K. and Rosenthal, Seth A. and Leiserowitz, Anthony},
  journal = {Journal of Applied Social Psychology},
  volume = {n/a},
  number = {n/a},
  issn = {1559-1816},
  doi = {10.1111/jasp.12737},
  abstract = {Despite Greta Thunberg's popularity, research has yet to investigate her impact on the public's willingness to take collective action on climate change. Using cross-sectional data from a nationally representative survey of U.S. adults (N = 1,303), we investigate the ``Greta Thunberg Effect,'' or whether exposure to Greta Thunberg predicts collective efficacy and intentions to engage in collective action. We find that those who are more familiar with Greta Thunberg have higher intentions of taking collective actions to reduce global warming and that stronger collective efficacy beliefs mediate this relationship. This association between familiarity with Greta Thunberg, collective efficacy beliefs, and collective action intentions is present even after accounting for respondents' overall support for climate activism. Moderated mediation models testing age and political ideology as moderators of the ``Greta Thunberg Effect'' indicate that although the indirect effect of familiarity with Greta Thunberg via collective efficacy is present across all age-groups, and across the political spectrum, it may be stronger among those who identify as more liberal (than conservative). Our findings suggest that young public figures like Greta Thunberg may motivate collective action across the U.S. public, but their effect may be stronger among those with a shared political ideology. Implications for future research and for broadening climate activists' appeals across the political spectrum are discussed.},
  language = {en},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/jasp.12737},
  file = {/Users/milan/Zotero/storage/5SFKNWIJ/Sabherwal et al. - The Greta Thunberg Effect Familiarity with Greta .pdf;/Users/milan/Zotero/storage/S6MX77T6/jasp.html}
}

@article{Saffin2020,
  title = {Reduced-Precision Parametrization: Lessons from an Intermediate-Complexity Atmospheric Model},
  shorttitle = {Reduced-Precision Parametrization},
  author = {Saffin, Leo and Hatfield, Sam and D{\"u}ben, Peter and Palmer, Tim},
  year = {2020},
  journal = {Quarterly Journal of the Royal Meteorological Society},
  volume = {146},
  number = {729},
  pages = {1590--1607},
  issn = {1477-870X},
  doi = {10.1002/qj.3754},
  abstract = {Reducing numerical precision can save computational costs which can then be reinvested for more useful purposes. This study considers the effects of reducing precision in the parametrizations of an intermediate complexity atmospheric model (SPEEDY). We find that the difference between double-precision and reduced-precision parametrization tendencies is proportional to the expected machine rounding error if individual timesteps are considered. However, if reduced precision is used in simulations that are compared to double-precision simulations, a range of precision is found where differences are approximately the same for all simulations. Here, rounding errors are small enough to not directly perturb the model dynamics, but can perturb conditional statements in the parametrizations (such as convection active/inactive) leading to a similar error growth for all runs. For lower precision, simulations are perturbed significantly. Precision cannot be constrained without some quantification of the uncertainty. The inherent uncertainty in numerical weather and climate models is often explicitly considered in simulations by stochastic schemes that will randomly perturb the parametrizations. A commonly used scheme is stochastic perturbation of parametrization tendencies (SPPT). A strong test on whether a precision is acceptable is whether a low-precision ensemble produces the same probability distribution as a double-precision ensemble where the only difference between ensemble members is the model uncertainty (i.e., the random seed in SPPT). Tests with SPEEDY suggest a precision as low as 3.5 decimal places (equivalent to half precision) could be acceptable, which is surprisingly close to the lowest precision that produces similar error growth in the experiments without SPPT mentioned above. Minor changes to model code to express variables as anomalies rather than absolute values reduce rounding errors and low-precision biases, allowing even lower precision to be used. These results provide a pathway for implementing reduced-precision parametrizations in more complex weather and climate models.},
  copyright = {\textcopyright{} 2020 Royal Meteorological Society},
  language = {en},
  keywords = {model error,parametrization,reduced precision,stochastic physics},
  annotation = {\_eprint: https://rmets.onlinelibrary.wiley.com/doi/pdf/10.1002/qj.3754},
  file = {/Users/milan/Zotero/storage/BV9PZWK8/qj.html}
}

@article{Salmon2004,
  title = {Poisson-{{Bracket Approach}} to the {{Construction}} of {{Energy}}- and {{Potential}}-{{Enstrophy}}- {{Conserving Algorithms}} for the {{Shallow}}-{{Water Equations}}},
  author = {Salmon, Rick},
  year = {2004},
  journal = {JOURNAL OF THE ATMOSPHERIC SCIENCES},
  volume = {61},
  pages = {21},
  abstract = {Arakawa and Lamb discovered a finite-difference approximation to the shallow-water equations that exactly conserves finite-difference approximations to the energy and potential enstrophy of the fluid. The Arakawa\textendash Lamb (AL) algorithm is a stunning and important achievement\textemdash stunning, because in the shallow-water case, neither energy nor potential enstrophy is a simple quadratic, and important because the simultaneous conservation of energy and potential enstrophy is known to prevent the spurious cascade of energy to high wavenumbers. However, the method followed by AL is somewhat ad hoc, and it is difficult to see how it might be generalized to other systems.},
  language = {en}
}

@article{Salmon2007,
  title = {A {{General Method}} for {{Conserving Energy}} and {{Potential Enstrophy}} in {{Shallow}}-{{Water Models}}},
  author = {Salmon, Rick},
  year = {2007},
  month = feb,
  journal = {Journal of the Atmospheric Sciences},
  volume = {64},
  number = {2},
  pages = {515--531},
  issn = {0022-4928, 1520-0469},
  doi = {10.1175/JAS3837.1},
  abstract = {The shallow-water equations may be posed in the form dF/dt ϭ \{F, H, Z\}, where H is the energy, Z is the potential enstrophy, and the Nambu bracket \{F, H, Z\} is completely antisymmetric in its three arguments. This makes it very easy to construct numerical models that conserve analogs of the energy and potential enstrophy; one need only discretize the Nambu bracket in such a way that the antisymmetry property is maintained. Using this strategy, this paper derives explicit finite-difference approximations to the shallowwater equations that conserve mass, circulation, energy, and potential enstrophy on a regular square grid and on an unstructured triangular mesh. The latter includes the regular hexagonal grid as a special case.},
  language = {en}
}

@article{Salmon2009,
  title = {A Shallow Water Model Conserving Energy and Potential Enstrophy in the Presence of Boundaries},
  author = {Salmon, Rick},
  year = {2009},
  month = nov,
  journal = {Journal of Marine Research},
  volume = {67},
  number = {6},
  pages = {779--814},
  issn = {00222402, 15439542},
  doi = {10.1357/002224009792006160},
  abstract = {We extend a previously developed method for constructing shallow water models that conserve energy and potential enstrophy to the case of flow bounded by rigid walls. This allows the method to be applied to ocean models. Our procedure splits the dynamics into a set of prognostic equations for variables (vorticity, divergence, and depth) chosen for their relation to the Casimir invariants of mass, circulation and potential enstrophy, and a set of diagnostic equations for variables that are the functional derivatives of the Hamiltonian with respect to the chosen prognostic variables. The form of the energy determines the form of the diagnostic equations. Our emphasis on conservation laws produces a novel form of the boundary conditions, but numerical test cases demonstrate the accuracy of our model and its extreme robustness, even in the case of vanishing viscosity.},
  language = {en}
}

@book{Salomon2006,
  title = {Data {{Compression}}: {{The Complete Reference}}},
  shorttitle = {Data {{Compression}}},
  author = {Salomon, David and Motta, G. and Bryant, D.},
  year = {2006},
  month = dec,
  edition = {4th Edition},
  publisher = {{Springer}},
  address = {{London}},
  abstract = {"A wonderful treasure chest of information; spanning a wide range of data compression methods, from simple test compression methods to the use of wavelets in image compression. It is unusual for a text on compression to cover the field so completely." \textendash{} ACM Computing Reviews "Salomon's book is the most complete and up-to-date reference on the subject. The style, rigorous yet easy to read, makes this book the preferred choice \ldots{} [and] the encyclopedic nature of the text makes it an obligatory acquisition by our library." \textendash{} Dr Martin Cohn, Brandeis University Data compression is one of the most important~tools in modern computing, and there has been tremendous progress~in all areas of the field.~This fourth edition of Data Compression provides an all-inclusive, thoroughly updated, and user-friendly reference for the many different types and methods of compression (especially audio compression, an area in which many new topics covered in this revised edition appear). Among the important features of the book are a detailed and helpful taxonomy, a detailed description of the most common methods, and discussions on the use and comparative benefits of different methods. The book's logical, clear and lively presentation is organized around the main branches of data compression. Topics and features: \textbullet highly inclusive, yet well-balanced coverage for specialists and nonspecialists \textbullet thorough coverage of wavelets methods, including SPIHT, EZW, DjVu, WSQ, and JPEG 2000 \textbullet comprehensive updates on all material from previous editions And these NEW topics: \textbullet RAR, a proprietary algorithm \textbullet FLAC, a free, lossless audio compression method \textbullet WavPack, an open, multiplatform audio-compression algorithm \textbullet LZMA, a sophisticated dictionary-based compression method \textbullet Differential compression \textbullet ALS, the audio lossless coding algorithm used in MPEG-4 \textbullet H.264, an advanced video codec, part of the huge MPEG-4 project \textbullet AC-3, Dolby's third-generation audio codec \textbullet Hyperspectral compression of 3D data sets This meticulously enhanced reference is an essential resource and companion for all computer scientists; computer, electrical and signal/image processing engineers; and scientists needing a comprehensive compilation of compression methods. It requires only a minimum of mathematics and is well-suited to nonspecialists and general readers who need to know and use this valuable content. David Salomon is a professor emeritus of computer Science at California State University, Northridge. He has authored numerous articles and books, including Coding for Data and Computer Communications, Guide to Data Compression Methods, Data Privacy and Security, Computer Graphics and Geometric Modeling, Foundations of Computer Security and Transformations and Projections in Computer Graphics.},
  isbn = {978-1-84628-602-5},
  language = {English}
}

@article{Samelson2014,
  title = {Randomness, {{Symmetry}}, and {{Scaling}} of {{Mesoscale Eddy Life Cycles}}},
  author = {Samelson, R. M. and Schlax, M. G. and Chelton, D. B.},
  year = {2014},
  month = mar,
  journal = {Journal of Physical Oceanography},
  volume = {44},
  number = {3},
  pages = {1012--1029},
  issn = {0022-3670, 1520-0485},
  doi = {10.1175/JPO-D-13-0161.1},
  abstract = {It is shown that the life cycles of nonlinear mesoscale eddies, a major component of low-frequency ocean physical variability, have a characteristic structure that differs fundamentally from that which would be expected on the basis of classical interpretations of ocean eddy evolution in terms of mean flow instability and equilibration followed by frictional, radiative, or barotropic decay, or of vortex merger dynamics in quasigeostrophic turbulent cascades. Further, it is found that these life cycles can be accurately modeled in terms of the large-amplitude excursions of a stochastic process. These conclusions, which apply in the corresponding global-mean context, follow from the examination of ensemble-mean and standard deviation time series of normalized eddy amplitude from an automated eddy identification and tracking analysis of a nearly two decade\textendash merged satellite altimeter record of global sea surface height (SSH). The resulting series are found to have several striking and unexpected characteristics, including time-reversal symmetry and approximate self-similarity. Consistent results are obtained from a similar analysis of a 7-yr record of global SSH from a numerical ocean circulation model. The basic qualitative and quantitative statistical properties of these series can be remarkably well reproduced with an extremely simple stochastic model, in which the SSH increments between successive time points are random numbers, and the eddy life cycles are represented by excursions exceeding a given threshold. The stochastic model is found also to predict accurately the empirical autocorrelation structure of the underlying observed SSH field itself, when the autocorrelations are computed along long planetary (Rossby) wave characteristics.},
  language = {en}
}

@article{Santer2005,
  title = {Amplification of {{Surface Temperature Trends}} and {{Variability}} in the {{Tropical Atmosphere}}},
  author = {Santer, B. D.},
  year = {2005},
  month = sep,
  journal = {Science},
  volume = {309},
  number = {5740},
  pages = {1551--1556},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1114867},
  language = {en}
}

@article{Sasaki2012,
  title = {{{SSH Wavenumber Spectra}} in the {{North Pacific}} from a {{High}}-{{Resolution Realistic Simulation}}},
  author = {Sasaki, Hideharu and Klein, Patrice},
  year = {2012},
  month = jul,
  journal = {Journal of Physical Oceanography},
  volume = {42},
  number = {7},
  pages = {1233--1241},
  issn = {0022-3670, 1520-0485},
  doi = {10.1175/JPO-D-11-0180.1},
  abstract = {Following recent studies based on altimetric data, this paper analyses the spectral characteristics of the sea surface height (SSH) using a new realistic simulation of the North Pacific Ocean with high resolution (1/308 in the horizontal and 100 vertical levels). This simulation resolves smaller scales (down to '10 km) than altimetric data (limited to 70 km because of the noise level). In high eddy kinetic energy (EKE) regions (as in the western part), SSH spectral slope almost follows a k24 (with k the wavenumber) or slightly steeper law in agreement with altimeter studies. The new result is that, unlike altimeter studies, such a k24 slope is also observed in low EKE regions (as in the eastern part). In these regions, this slope mostly concerns scales not well resolved by altimetric data. Such k24 SSH spectral slopes are weaker from what is expected from quasigeostrophic turbulence theory but closer to surface quasigeostrophic (SQG) turbulence theory. The consequence is that the small scales concerned by these spectral slopes, in particular in low EKE regions, may significantly affect the larger ones because of the inverse kinetic energy cascade. These results need to be confirmed using a longer numerical integration. They also need to be corroborated by high-resolution observations.},
  language = {en}
}

@inproceedings{Sato2020,
  title = {Co-{{Design}} for {{A64FX Manycore Processor}} and ''{{Fugaku}}''},
  booktitle = {{{SC20}}: {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  author = {Sato, Mitsuhisa and Ishikawa, Yutaka and Tomita, Hirofumi and Kodama, Yuetsu and Odajima, Tetsuya and Tsuji, Miwako and Yashiro, Hisashi and Aoki, Masaki and Shida, Naoyuki and Miyoshi, Ikuo and Hirai, Kouichi and Furuya, Atsushi and Asato, Akira and Morita, Kuniki and Shimizu, Toshiyuki},
  year = {2020},
  month = nov,
  pages = {1--15},
  doi = {10.1109/SC41405.2020.00051},
  abstract = {We have been carrying out the FLAGSHIP 2020 Project to develop the Japanese next-generation flagship supercomputer, the Post-K, recently named ``Fugaku''. We have designed an original many core processor based on Armv8 instruction sets with the Scalable Vector Extension (SVE), an A64FX processor, as well as a system including interconnect and a storage subsystem with the industry partner, Fujitsu. The ``co-design'' of the system and applications is a key to making it power efficient and high performance. We determined many architectural parameters by reflecting an analysis of a set of target applications provided by applications teams. In this paper, we present the pragmatic practice of our co-design effort for ``Fugaku''. As a result, the system has been proven to be a very power-efficient system, and it is confirmed that the performance of some target applications using the whole system is more than 100 times the performance of the K computer.},
  keywords = {co-design,exascale computing,High performance computing,high-performance computing,Industries,Instruction sets,many core processor,Manycore processors,Next generation networking,Pragmatics,Supercomputers}
}

@article{Sauer2002,
  title = {Shadowing Breakdown and Large Errors in Dynamical Simulations of Physical Systems},
  author = {Sauer, Timothy D.},
  year = {2002},
  month = feb,
  journal = {Physical Review E},
  volume = {65},
  number = {3},
  pages = {036220},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevE.65.036220},
  abstract = {Simulations play a crucial role in the modern study of physical systems. A major open question for long dynamical simulations of physical processes is the role of discretization and truncation errors in the outcome. A general mechanism is described that can cause extremely small noise inputs to result in errors in simulation statistics that are several orders of magnitude larger. A scaling law for the size of such errors in terms of the noise level and properties of the dynamics is given. This result brings into question trajectory averages that are computed for systems with particular dynamical behaviors, in particular, systems that exhibit fluctuating Lyapunov exponents or unstable dimension variability.},
  file = {/Users/milan/Zotero/storage/V6WNKB7C/Sauer - 2002 - Shadowing breakdown and large errors in dynamical .pdf;/Users/milan/Zotero/storage/3DSTMKRS/PhysRevE.65.html}
}

@article{Sausen2000,
  title = {Estimates of the {{Climate Response}} to {{Aircraft CO}} 2 and {{NO}} x {{Emissions Scenarios}}},
  author = {Sausen, Robert and Schumann, Ulrich},
  year = {2000},
  month = jan,
  journal = {Climatic Change},
  volume = {44},
  number = {1},
  pages = {27--58},
  issn = {1573-1480},
  doi = {10.1023/A:1005579306109},
  abstract = {A combination of linear response models is used to estimate the transient changes in the global means of carbon dioxide (CO2) concentration, surface temperature, and sea level due to aviation. Apart from CO2, the forcing caused by ozone (O3) changes due to nitrogen oxide (NOx) emissions from aircraft is also considered. The model is applied to aviation using several CO2 emissions scenarios, based on reported fuel consumption in the past and scenarios for the future, and corresponding NOx emissions. Aviation CO2 emissions from the past until 1995 enlarged the atmospheric CO2 concentration by 1.4 ppmv (1.7\% of the anthropogenic CO2 increase since 1800). By 1995, the global mean surface temperature had increased by about 0.004 K, and the sea level had risen by 0.045 cm. In one scenario (Fa1), which assumes a threefold increase in aviation fuel consumption until 2050 and an annual increase rate of 1\% thereafter until 2100, the model predicts a CO2 concentration change of 13 ppmv by 2100, causing temperature increases of 0.01, 0.025, 0.05 K and sea level increases of 0.1, 0.3, and 0.5 cm in the years 2015, 2050, and 2100, respectively. For other recently published scenarios, the results range from 5 to 17 ppmv for CO2 concentration increase in the year 2050, and 0.02 to 0.05 K for temperature increase. Under the assumption that present-day aircraft-induced O3 changes cause an equilibrium surface warming of 0.05 K, the transient responses amount to 0.03 K in surface temperature for scenario Fa1 in 1995. The radiative forcing due to an aircraft-induced O3 increase causes a larger temperature change than aircraft CO2 forcing. Also, climate reacts more promptly to changes in O3 than to changes in CO2 emissions from aviation. Finally, even under the assumption of a rather small equilibrium temperature change from aircraft-induced O3 (0.01 K for the 1992 NOx emissions), a proposed new combustor technology which reduces specific NOx emissions will cause a smaller temperature change during the next century than the standard technology does, despite a slightly enhanced fuel consumption. Regional effects are not considered here, but may be larger than the global mean responses.},
  language = {en},
  file = {/Users/milan/Zotero/storage/KXUZTW9L/Sausen and Schumann - 2000 - Estimates of the Climate Response to Aircraft CO 2.pdf}
}

@inproceedings{Schafer2014,
  title = {Bringing up {{OpenSky}}: {{A}} Large-Scale {{ADS}}-{{B}} Sensor Network for Research},
  shorttitle = {Bringing up {{OpenSky}}},
  booktitle = {{{IPSN}}-14 {{Proceedings}} of the 13th {{International Symposium}} on {{Information Processing}} in {{Sensor Networks}}},
  author = {Sch{\"a}fer, M. and Strohmeier, M. and Lenders, V. and Martinovic, I. and Wilhelm, M.},
  year = {2014},
  month = apr,
  pages = {83--94},
  doi = {10.1109/IPSN.2014.6846743},
  abstract = {Automatic Dependent Surveillance-Broadcast (ADS-B) is one of the key components of the next generation air transportation system. Since ADS-B will become mandatory by 2020 for most airspaces, it is important that aspects such as capacity, applications, and security are investigated by an independent research community. However, large-scale real-world data was previously only accessible to a few closed industrial and governmental groups because it required specialized and expensive equipment. To enable researchers to conduct experimental studies based on real data, we developed OpenSky, a sensor network based on low-cost hardware connected over the Internet. OpenSky is based on off-the-shelf ADS-B sensors distributed to volunteers throughout Central Europe. It covers 720,000 km2, is able to capture more than 30\% of the commercial air traffic in Europe, and enables researchers to analyze billions of ADS-B messages. In this paper, we report on the challenges we faced during the development and deployment of this participatory network and the insights we gained over the last two years of operations as a service to academic research groups. We go on to provide real-world insights about the possibilities and limitations of such low-cost sensor networks concerning air traffic surveillance and further applications such as multilateration.},
  keywords = {academic research groups,Accuracy,ADS-B,ADS-B messages,ADS-B sensor network,air traffic control,Air Traffic Control,air traffic surveillance,Aircraft,automatic dependent surveillance broadcast,central Europe,commercial air traffic,Databases,Europe,Global Positioning System,governmental groups,industrial groups,Internet,low-cost sensor networks,multilateration,next generation air transportation system,NextGen,OpenSky,real-world insights,Receivers,Sensor Networks,surveillance,Surveillance,transportation,wireless sensor networks},
  file = {/Users/milan/Zotero/storage/RAT2LLD5/6846743.html}
}

@article{Schar2020,
  title = {Kilometer-{{Scale Climate Models}}: {{Prospects}} and {{Challenges}}},
  shorttitle = {Kilometer-{{Scale Climate Models}}},
  author = {Sch{\"a}r, Christoph and Fuhrer, Oliver and Arteaga, Andrea and Ban, Nikolina and Charpilloz, Christophe and Girolamo, Salvatore Di and Hentgen, Laureline and Hoefler, Torsten and Lapillonne, Xavier and Leutwyler, David and Osterried, Katherine and Panosetti, Davide and R{\"u}dis{\"u}hli, Stefan and Schlemmer, Linda and Schulthess, Thomas C. and Sprenger, Michael and Ubbiali, Stefano and Wernli, Heini},
  year = {2020},
  month = may,
  journal = {Bulletin of the American Meteorological Society},
  volume = {101},
  number = {5},
  pages = {E567-E587},
  publisher = {{American Meteorological Society}},
  issn = {0003-0007, 1520-0477},
  doi = {10.1175/BAMS-D-18-0167.1},
  abstract = {{$<$}section class="abstract"{$><$}h2 class="abstractTitle text-title my-1" id="d359e2"{$>$}Abstract{$<$}/h2{$><$}p{$>$}Currently major efforts are underway toward refining the horizontal resolution (or grid spacing) of climate models to about 1 km, using both global and regional climate models (GCMs and RCMs). Several groups have succeeded in conducting kilometer-scale multiweek GCM simulations and decadelong continental-scale RCM simulations. There is the well-founded hope that this increase in resolution represents a quantum jump in climate modeling, as it enables replacing the parameterization of moist convection by an explicit treatment. It is expected that this will improve the simulation of the water cycle and extreme events and reduce uncertainties in climate change projections. While kilometer-scale resolution is commonly employed in limited-area numerical weather prediction, enabling it on global scales for extended climate simulations requires a concerted effort. In this paper, we exploit an RCM that runs entirely on graphics processing units (GPUs) and show examples that highlight the prospects of this approach. A particular challenge addressed in this paper relates to the growth in output volumes. It is argued that the data avalanche of high-resolution simulations will make it impractical or impossible to store the data. Rather, repeating the simulation and conducting online analysis will become more efficient. A prototype of this methodology is presented. It makes use of a bit-reproducible model version that ensures reproducible simulations across hardware architectures, in conjunction with a data virtualization layer as a common interface for output analyses. An assessment of the potential of these novel approaches will be provided.{$<$}/p{$><$}/section{$>$}},
  chapter = {Bulletin of the American Meteorological Society},
  language = {EN},
  file = {/Users/milan/Zotero/storage/BCW2LB3A/Schär et al. - 2020 - Kilometer-Scale Climate Models Prospects and Chal.pdf;/Users/milan/Zotero/storage/HPZDIR8L/bams-d-18-0167.1.html}
}

@article{Schneider2019,
  title = {Possible Climate Transitions from Breakup of Stratocumulus Decks under Greenhouse Warming},
  author = {Schneider, Tapio and Kaul, Colleen M. and Pressel, Kyle G.},
  year = {2019},
  month = mar,
  journal = {Nature Geoscience},
  volume = {12},
  number = {3},
  pages = {163--167},
  issn = {1752-0894, 1752-0908},
  doi = {10.1038/s41561-019-0310-1},
  language = {en}
}

@article{Schreiber2000,
  title = {Measuring {{Information Transfer}}},
  author = {Schreiber, Thomas},
  year = {2000},
  month = jul,
  journal = {Physical Review Letters},
  volume = {85},
  number = {2},
  pages = {461--464},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevLett.85.461},
  abstract = {An information theoretic measure is derived that quantifies the statistical coherence between systems evolving in time. The standard time delayed mutual information fails to distinguish information that is actually exchanged from shared information due to common history and input signals. In our new approach, these influences are excluded by appropriate conditioning of transition probabilities. The resulting transfer entropy is able to distinguish effectively driving and responding elements and to detect asymmetry in the interaction of subsystems.},
  file = {/Users/milan/Zotero/storage/C3ZSKCYP/Schreiber - 2000 - Measuring Information Transfer.pdf;/Users/milan/Zotero/storage/XE4PMAFH/PhysRevLett.85.html}
}

@article{Schumann2000,
  title = {Influence of Propulsion Efficiency on Contrail Formation},
  author = {Schumann, Ulrich},
  year = {2000},
  month = sep,
  journal = {Aerospace Science and Technology},
  volume = {4},
  number = {6},
  pages = {391--401},
  issn = {1270-9638},
  doi = {10.1016/S1270-9638(00)01062-2},
  abstract = {Aircraft cause contrails when flying in an atmosphere colder than a threshold temperature which depends on the overall efficiency {$\eta$} of propulsion of the aircraft/engine combination. Higher {$\eta$} causes contrails at higher ambient temperatures and over a larger range of flight altitudes. The ratio of temperature increase relative to moisture increase in engine plumes is lower for engines with higher {$\eta$} . Thermodynamic arguments are given for this fact and measurements and observations are reported which support the validity of the given criterion. The measurements include contrail observations for identified aircraft flying at ambient temperature and humidity conditions measured with high precision in-situ instruments, measurements of the temperature and humidity increases in an aircraft exhaust plume, and an observation of contrail formation behind two different four-engine jet aircraft with different engines flying wing by wing. The observations show that an altitude range exists in which the aircraft with high efficiency causes contrails while the other aircraft with lower efficiency causes none. Aircraft with more efficient propulsion cause contrails more frequently. The climatic impact depends on the relative importance of increased contrail frequency and reduced carbon dioxide emissions for increased efficiency, and on other parameters, and has not yet been quantified. Zusammenfassung Einfluss des Antriebswirkungsgrades auf die Bildung von Kondensstreifen. Kondensstreifen bilden sich hinter Flugzeugen, die in einer Atmosph\"are fliegen, die k\"alter ist als eine Grenztemperatur, deren Wert vom Antriebs-Gesamt-Wirkungsgrad {$\eta$} der Flugzeug/Triebwerks-Kombination abh\"angt. F\"ur gr\"o\ss eres {$\eta$} entstehen Kondensstreifen bei h\"oheren Umgebungstemperaturen und \"uber einen gr\"o\ss eren H\"ohenbereich. Die Zunahme der Temperatur im Vergleich zur Feuchte im Abgas ist um so kleiner je gr\"o\ss er {$\eta$} ist. Die thermodynamischen Gr\"unde hierf\"ur werden erkl\"art und es werden Messungen und Beobachtungen berichtet, die diese Zusammenh\"ange best\"atigen. Die Messungen umfassen Beobachtungen von Kondensstreifen hinter bekannten Flugzeugen mit genauen lokalen Messungen der Temperatur und Feuchte der Atmosph\"are im Flugniveau, Messungen der Temperatur- und Feuchtedifferenz zwischen Abgasfahne und Umgebung, und Beobachtungen des Einsetzens von Kondensstreifen hinter zwei dicht nebeneinander fliegenden vierstrahligen Strahlflugzeugen mit verschiedenen Triebwerken. Die Beobachtungen belegen, dass es einen H\"ohenbereich gibt, in dem nur das Flugzeug mit hohem Wirkungsgrad einen Kondensstreifen bildet. Verbesserungen des Antriebs-Gesamt-Wirkungsgrades f\"uhren zu mehr Kondensstreifen. Der damit verbundene Klimaeinfluss h\"angt au\ss er von den vermehrten Kondensstreifen auch von den verminderten Beitr\"agen zu Kohlendioxid in der Atmosph\"are bei effektiveren Flugzeugen ab und von weiteren Parametern und ist bisher nicht quantifiziert.},
  language = {en},
  keywords = {aircraft propulsion,Atmosphäre,atmosphere,contrail,efficiency,Flugzeug-Antrieb,Kondensstreifen,Wirkungsgrad},
  file = {/Users/milan/Zotero/storage/23XSK3T6/Schumann - 2000 - Influence of propulsion efficiency on contrail for.pdf;/Users/milan/Zotero/storage/JZCN7FQF/S1270963800010622.html}
}

@article{Scott2005,
  title = {Direct {{Evidence}} of an {{Oceanic Inverse Kinetic Energy Cascade}} from {{Satellite Altimetry}}},
  author = {Scott, Robert B. and Wang, Faming},
  year = {2005},
  month = sep,
  journal = {Journal of Physical Oceanography},
  volume = {35},
  number = {9},
  pages = {1650--1666},
  issn = {0022-3670, 1520-0485},
  doi = {10.1175/JPO2771.1},
  abstract = {Sea surface height measurements from satellites reveal the turbulent properties of the South Pacific Ocean surface geostrophic circulation, both supporting and challenging different aspects of geostrophic turbulence theory. A near-universal shape of the spectral kinetic energy flux is found and provides direct evidence of a source of kinetic energy near to or smaller than the deformation radius, consistent with linear instability theory. The spectral kinetic energy flux also reveals a net inverse cascade (i.e., a cascade to larger spatial scale), consistent with two-dimensional turbulence phenomenology. However, stratified geostrophic turbulence theory predicts an inverse cascade for the barotropic mode only; energy in the large-scale baroclinic modes undergoes a direct cascade toward the first-mode deformation scale. Thus if the surface geostrophic flow is predominately the first baroclinic mode, as expected for oceanic stratification profiles, then the observed inverse cascade contradicts geostrophic turbulence theory. The latter interpretation is argued for. Furthermore, and consistent with this interpretation, the inverse cascade arrest scale does not follow the Rhines arrest scale, as one would expect for the barotropic mode. A tentative revision of theory is proposed that would resolve the conflicts; however, further observations and idealized modeling experiments are needed to confirm, or refute, the revision. It is noted that no inertial range was found for the inverse cascade range of the spectrum, implying inertial range scaling, such as the established KϪ5/3 slope in the spectral kinetic energy density plot, is not applicable to the surface geostrophic flow.},
  language = {en}
}

@article{Shannon1948,
  ids = {Shannon},
  title = {A Mathematical Theory of Communication},
  author = {Shannon, C. E.},
  year = {1948},
  month = oct,
  journal = {The Bell System Technical Journal},
  volume = {27},
  number = {4},
  pages = {623--656},
  issn = {0005-8580},
  doi = {10.1002/j.1538-7305.1948.tb00917.x},
  abstract = {In this final installment of the paper we consider the case where the signals or the messages or both are continuously variable, in contrast with the discrete nature assumed until now. To a considerable extent the continuous case can be obtained through a limiting process from the discrete case by dividing the continuum of messages and signals into a large but finite number of small regions and calculating the various parameters involved on a discrete basis. As the size of the regions is decreased these parameters in general approach as limits the proper values for the continuous case. There are, however, a few new effects that appear and also a general change of emphasis in the direction of specialization of the general results to particular cases.},
  file = {/Users/milan/Zotero/storage/QM36PZIS/Shannon - 1948 - A mathematical theory of communication.pdf;/Users/milan/Zotero/storage/Y8CSSWGA/Shannon - A Mathematical Theory of Communication.pdf;/Users/milan/Zotero/storage/TZQ7RHJZ/6773067.html}
}

@article{Shchepetkin2005,
  title = {The Regional Oceanic Modeling System ({{ROMS}}): A Split-Explicit, Free-Surface, Topography-Following-Coordinate Oceanic Model},
  shorttitle = {The Regional Oceanic Modeling System ({{ROMS}})},
  author = {Shchepetkin, Alexander F. and McWilliams, James C.},
  year = {2005},
  month = jan,
  journal = {Ocean Modelling},
  volume = {9},
  number = {4},
  pages = {347--404},
  issn = {14635003},
  doi = {10.1016/j.ocemod.2004.08.002},
  language = {en}
}

@article{Shutts2015,
  title = {A Stochastic Convective Backscatter Scheme for Use in Ensemble Prediction Systems: {{Convective Backscatter}}},
  shorttitle = {A Stochastic Convective Backscatter Scheme for Use in Ensemble Prediction Systems},
  author = {Shutts, Glenn},
  year = {2015},
  month = oct,
  journal = {Quarterly Journal of the Royal Meteorological Society},
  volume = {141},
  number = {692},
  pages = {2602--2616},
  issn = {00359009},
  doi = {10.1002/qj.2547},
  language = {en}
}

@article{Sigmond2013,
  title = {Enhanced Seasonal Forecast Skill Following Stratospheric Sudden Warmings},
  author = {Sigmond, M. and Scinocca, J. F. and Kharin, V. V. and Shepherd, T. G.},
  year = {2013},
  month = feb,
  journal = {Nature Geoscience},
  volume = {6},
  number = {2},
  pages = {98--102},
  issn = {1752-0894, 1752-0908},
  doi = {10.1038/ngeo1698},
  language = {en}
}

@article{Silver2017,
  title = {The Compression\textendash Error Trade-off for Large Gridded Data Sets},
  author = {Silver, Jeremy D. and Zender, Charles S.},
  year = {2017},
  month = jan,
  journal = {Geoscientific Model Development},
  volume = {10},
  number = {1},
  pages = {413--423},
  issn = {1991-9603},
  doi = {10.5194/gmd-10-413-2017},
  abstract = {The netCDF-4 format is widely used for large gridded scientific data sets and includes several compression methods: lossy linear scaling and the non-lossy deflate and shuffle algorithms. Many multidimensional geoscientific data sets exhibit considerable variation over one or several spatial dimensions (e.g., vertically) with less variation in the remaining dimensions (e.g., horizontally). On such data sets, linear scaling with a single pair of scale and offset parameters often entails considerable loss of precision. We introduce an alternative compression method called ``layer-packing'' that simultaneously exploits lossy linear scaling and lossless compression. Layer-packing stores arrays (instead of a scalar pair) of scale and offset parameters. An implementation of this method is compared with lossless compression, storing data at fixed relative precision (bit-grooming) and scalar linear packing in terms of compression ratio, accuracy and speed.},
  language = {en},
  file = {/Users/milan/Zotero/storage/EYA2WME8/Silver and Zender - 2017 - The compression–error trade-off for large gridded .pdf}
}

@misc{Skibinski2020,
  title = {Inikep/Lzbench},
  author = {Skibinski, Przemyslaw},
  year = {2020},
  month = sep,
  abstract = {lzbench is an in-memory benchmark of open-source LZ77/LZSS/LZMA compressors},
  keywords = {benchmark,benchmarking,brotli,compression,compressor,lz4,zlib,zstd}
}

@article{Skowron2021,
  title = {Greater Fuel Efficiency Is Potentially Preferable to Reducing {{NO}} x Emissions for Aviation's Climate Impacts},
  author = {Skowron, Agnieszka and Lee, David S. and Le{\'o}n, Rub{\'e}n Rodr{\'i}guez De and Lim, Ling L. and Owen, Bethan},
  year = {2021},
  month = jan,
  journal = {Nature Communications},
  volume = {12},
  number = {1},
  pages = {1--8},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-20771-3},
  abstract = {The regulation of aircraft engine NOx emissions was introduced to improve local air quality and reduce NOx emissions at altitude. Here, the authors find that greater fuel efficiency of aircrafts, and therefore lower CO2 emissions, could be preferable to reducing NOx emissions in terms of the aviation industries future climate impacts.},
  copyright = {2021 The Author(s)},
  language = {en},
  file = {/Users/milan/Zotero/storage/Y3IWFI4K/s41467-020-20771-3.html}
}

@article{Smith,
  title = {University of {{Reading Department}} of {{Mathematics}}},
  author = {Smith, Christopher J},
  pages = {179},
  language = {en}
}

@article{Smith2004,
  title = {Improved {{Extended Reconstruction}} of {{SST}} (1854\textendash 1997)},
  author = {Smith, Thomas M and Reynolds, Richard W},
  year = {2004},
  journal = {JOURNAL OF CLIMATE},
  volume = {17},
  pages = {12},
  abstract = {An improved SST reconstruction for the 1854\textendash 1997 period is developed. Compared to the version 1 analysis, in the western tropical Pacific, the tropical Atlantic, and Indian Oceans, more variance is resolved in the new analysis. This improved analysis also uses sea ice concentrations to improve the high-latitude SST analysis and a modified historical bias correction for the 1939\textendash 41 period. In addition, the new analysis includes an improved error estimate. Analysis uncertainty is largest in the nineteenth century and during the two world wars due to sparse sampling. The near-global average SST in the new analysis is consistent with the version 1 reconstruction. The 95\% confidence uncertainty for the near-global average is 0.4\cyrchar\CYRNJE C or more in the nineteenth century, near 0.2\cyrchar\CYRNJE C for the first half of the twentieth century, and 0.1\cyrchar\CYRNJE C or less after 1950.},
  language = {en}
}

@article{Smith2008,
  title = {Improvements to {{NOAA}}'s {{Historical Merged Land}}\textendash{{Ocean Surface Temperature Analysis}} (1880\textendash 2006)},
  author = {Smith, Thomas M. and Reynolds, Richard W. and Peterson, Thomas C. and Lawrimore, Jay},
  year = {2008},
  month = may,
  journal = {Journal of Climate},
  volume = {21},
  number = {10},
  pages = {2283--2296},
  issn = {0894-8755, 1520-0442},
  doi = {10.1175/2007JCLI2100.1},
  abstract = {Observations of sea surface and land\textendash near-surface merged temperature anomalies are used to monitor climate variations and to evaluate climate simulations; therefore, it is important to make analyses of these data as accurate as possible. Analysis uncertainty occurs because of data errors and incomplete sampling over the historical period. This manuscript documents recent improvements in NOAA's merged global surface temperature anomaly analysis, monthly, in spatial 5\textdegree{} grid boxes. These improvements allow better analysis of temperatures throughout the record, with the greatest improvements in the late nineteenth century and since 1985. Improvements in the late nineteenth century are due to improved tuning of the analysis methods. Beginning in 1985, improvements are due to the inclusion of bias-adjusted satellite data. The old analysis (version 2) was documented in 2005, and this improved analysis is called version 3.},
  language = {en}
}

@article{Smith2018,
  title = {{{FAIR}} v1.3: A Simple Emissions-Based Impulse Response and Carbon Cycle Model},
  shorttitle = {{{FAIR}} v1.3},
  author = {Smith, Christopher J. and Forster, Piers M. and Allen, Myles and Leach, Nicholas and Millar, Richard J. and Passerello, Giovanni A. and Regayre, Leighton A.},
  year = {2018},
  month = jun,
  journal = {Geoscientific Model Development},
  volume = {11},
  number = {6},
  pages = {2273--2297},
  publisher = {{Copernicus GmbH}},
  issn = {1991-959X},
  doi = {10.5194/gmd-11-2273-2018},
  abstract = {{$<$}p{$><$}strong class="journal-contentHeaderColor"{$>$}Abstract.{$<$}/strong{$>$} Simple climate models can be valuable if they are able to replicate aspects of complex fully coupled earth system models. Larger ensembles can be produced, enabling a probabilistic view of future climate change. A simple emissions-based climate model, FAIR, is presented, which calculates atmospheric concentrations of greenhouse gases and effective radiative forcing (ERF) from greenhouse gases, aerosols, ozone and other agents. Model runs are constrained to observed temperature change from 1880 to 2016 and produce a range of future projections under the Representative Concentration Pathway (RCP) scenarios. The constrained estimates of equilibrium climate sensitivity (ECS), transient climate response (TCR) and transient climate response to cumulative CO\textsubscript{2} emissions (TCRE) are 2.86 (2.01 to 4.22)\&thinsp;K, 1.53 (1.05 to 2.41)\&thinsp;K and 1.40 (0.96 to 2.23)\&thinsp;K (1000\&thinsp;GtC)\textsuperscript{-1} (median and 5\textendash 95\&thinsp;\% credible intervals). These are in good agreement with the likely Intergovernmental Panel on Climate Change (IPCC) Fifth Assessment Report (AR5) range, noting that AR5 estimates were derived from a combination of climate models, observations and expert judgement. The ranges of future projections of temperature and ranges of estimates of ECS, TCR and TCRE are somewhat sensitive to the prior distributions of ECS/TCR parameters but less sensitive to the ERF from a doubling of CO\textsubscript{2} or the observational temperature dataset used to constrain the ensemble. Taking these sensitivities into account, there is no evidence to suggest that the median and credible range of observationally constrained TCR or ECS differ from climate model-derived estimates. The range of temperature projections under RCP8.5 for 2081\textendash 2100 in the constrained FAIR model ensemble is lower than the emissions-based estimate reported in AR5 by half a degree, owing to differences in forcing assumptions and ECS/TCR distributions.{$<$}/p{$>$}},
  language = {English},
  file = {/Users/milan/Zotero/storage/WY6BGZZ5/Smith et al. - 2018 - FAIR v1.3 a simple emissions-based impulse respon.pdf;/Users/milan/Zotero/storage/6F9NFZVU/2018.html}
}

@article{Smith2021,
  title = {Further Improvement of Warming-Equivalent Emissions Calculation},
  author = {Smith, Matthew and Cain, Michelle and Allen, Myles R.},
  year = {2021},
  month = jan,
  publisher = {{EarthArXiv}},
  abstract = {GWP* was recently proposed1 as a simple metric for calculating warming-equivalent emissions by equating a change in the rate of emission of a short-lived climate pollutant (SLCP) to a pulse emission of carbon-dioxide. Other metrics aiming to account for the time-dependent impact of SLCP emissions, such as CGWP, have also been proposed2. In 2019 an improvement to GWP* was proposed by Cain et al3, hereafter CLA, combining both the rate and change in rate of SLCP emission, justified by the rate of forcing decline required to stabilise temperatures following a recent multi-decade emissions increase. Here we provide a more direct justification of the coefficients used in this definition of GWP*, with a small revision to their absolute values, by equating CO2 and SLCP forcing directly, without reference to the temperature response. This provides a more direct link to the impulse-response model used to calculate GWP values and improves consistency with CGWP values.},
  copyright = {CC BY Attribution 4.0 International},
  language = {en},
  file = {/Users/milan/Zotero/storage/WGCXHAKA/Smith et al. - 2021 - Further improvement of warming-equivalent emission.pdf;/Users/milan/Zotero/storage/7LVJ3HI7/1989.html}
}

@article{Smolarkiewicz1992,
  title = {A {{Class}} of {{Semi}}-{{Lagrangian Approximations}} for {{Fluids}}},
  author = {Smolarkiewicz, Piotr K. and Pudykiewicz, Janusz A.},
  year = {1992},
  month = nov,
  journal = {Journal of the Atmospheric Sciences},
  volume = {49},
  number = {22},
  pages = {2082--2096},
  publisher = {{American Meteorological Society}},
  issn = {0022-4928},
  doi = {10.1175/1520-0469(1992)049<2082:ACOSLA>2.0.CO;2},
  abstract = {This paper discusses a class of finite-difference approximations to the evolution equations of fluid dynamics. These approximations derive from elementary properties of differential forms. Values of a fluid variable {$\psi$} at any two points of a space-time continuum are related through the integral of the space-time gradient of {$\psi$} along an arbitrary contour connecting these two points (Stokes' theorem). Noting that spatial and temporal components of the gradient are related through the fluid equations, and selecting the contour composed of a parcel trajectory and an appropriate residual, leads to the integral form of the fluid equations, which is particularly convenient for finite-difference approximations. In these equations, the inertial and forcing terms are separated such that forces are integrated along a parcel trajectory (the Lagrangian aspect), whereas advection of the variable is evaluated along the residual contour (the Eulerian aspect). The virtue of this method is an extreme simplicity of the resulting solver; the entire model for a fluid may be essentially built upon a single one-dimensional Eulerian advection scheme while retaining the formal accuracy of its constant-coefficient limit. The Lagrangian aspect of the approach allows for large-Courant-number ({$>$}1) computations in a broad spectrum of dynamic applications. Theoretical considerations are illustrated with examples of applications to selected classical problems of atmospheric fluid dynamics. Since the theoretical arguments adopted in this paper assume differentiability of fluid variables, fluid systems admitting truly discontinuous solutions (e.g., shock waves, hydraulic jumps) are formally excluded from our considerations.},
  file = {/Users/milan/Zotero/storage/3ZKFM5ZF/Smolarkiewicz and Pudykiewicz - 1992 - A Class of Semi-Lagrangian Approximations for Flui.pdf;/Users/milan/Zotero/storage/DH4Y6H5B/1520-0469(1992)0492082ACOSLA2.0.html}
}

@article{Stammer2002,
  title = {Global Ocean Circulation during 1992\textendash 1997, Estimated from Ocean Observations and a General Circulation Model},
  author = {Stammer, D.},
  year = {2002},
  journal = {Journal of Geophysical Research},
  volume = {107},
  number = {C9},
  pages = {3118},
  issn = {0148-0227},
  doi = {10.1029/2001JC000888},
  language = {en}
}

@inproceedings{Steinkraus2005,
  title = {Using {{GPUs}} for Machine Learning Algorithms},
  booktitle = {Eighth {{International Conference}} on {{Document Analysis}} and {{Recognition}} ({{ICDAR}}'05)},
  author = {Steinkraus, D. and Buck, I. and Simard, P.Y.},
  year = {2005},
  month = aug,
  pages = {1115-1120 Vol. 2},
  issn = {2379-2140},
  doi = {10.1109/ICDAR.2005.251},
  abstract = {Using dedicated hardware to do machine learning typically ends up in disaster because of cost, obsolescence, and poor software. The popularization of graphic processing units (GPUs), which are now available on every PC, provides an attractive alternative. We propose a generic 2-layer fully connected neural network GPU implementation which yields over 3/spl times/ speedup for both training and testing with respect to a 3 GHz P4 CPU.},
  keywords = {Bandwidth,Central Processing Unit,Computer graphics,Hardware,Machine learning,Machine learning algorithms,Neural networks,Optical character recognition software,Rendering (computer graphics),Testing},
  file = {/Users/milan/Zotero/storage/XXU9KPZR/Steinkraus et al. - 2005 - Using GPUs for machine learning algorithms.pdf}
}

@article{Stevens2019,
  title = {{{DYAMOND}}: The {{DYnamics}} of the {{Atmospheric}} General Circulation {{Modeled On Non}}-Hydrostatic {{Domains}}},
  shorttitle = {{{DYAMOND}}},
  author = {Stevens, Bjorn and Satoh, Masaki and Auger, Ludovic and Biercamp, Joachim and Bretherton, Christopher S. and Chen, Xi and D{\"u}ben, Peter and Judt, Falko and Khairoutdinov, Marat and Klocke, Daniel and Kodama, Chihiro and Kornblueh, Luis and Lin, Shian-Jiann and Neumann, Philipp and Putman, William M. and R{\"o}ber, Niklas and Shibuya, Ryosuke and Vanniere, Benoit and Vidale, Pier Luigi and Wedi, Nils and Zhou, Linjiong},
  year = {2019},
  month = sep,
  journal = {Progress in Earth and Planetary Science},
  volume = {6},
  number = {1},
  pages = {61},
  issn = {2197-4284},
  doi = {10.1186/s40645-019-0304-z},
  abstract = {A review of the experimental protocol and motivation for DYAMOND, the first intercomparison project of global storm-resolving models, is presented. Nine models submitted simulation output for a 40-day (1 August\textendash 10 September 2016) intercomparison period. Eight of these employed a tiling of the sphere that was uniformly less than 5 km. By resolving the transient dynamics of convective storms in the tropics, global storm-resolving models remove the need to parameterize tropical deep convection, providing a fundamentally more sound representation of the climate system and a more natural link to commensurately high-resolution data from satellite-borne sensors. The models and some basic characteristics of their output are described in more detail, as is the availability and planned use of this output for future scientific study. Tropically and zonally averaged energy budgets, precipitable water distributions, and precipitation from the model ensemble are evaluated, as is their representation of tropical cyclones and the predictability of column water vapor, the latter being important for tropical weather.},
  keywords = {Climate modeling,Convective parameterization,Model intercomparison project,Tropical convection},
  file = {/Users/milan/Zotero/storage/9FHHJETZ/Stevens et al. - 2019 - DYAMOND the DYnamics of the Atmospheric general c.pdf;/Users/milan/Zotero/storage/J22L6WB9/s40645-019-0304-z.html}
}

@article{Stevens2020,
  title = {The Imperative to Reduce Carbon Emissions in Astronomy},
  author = {Stevens, Adam R. H. and Bellstedt, Sabine and Elahi, Pascal J. and Murphy, Michael T.},
  year = {2020},
  month = sep,
  journal = {Nature Astronomy},
  volume = {4},
  number = {9},
  pages = {843--851},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3366},
  doi = {10.1038/s41550-020-1169-1},
  abstract = {For astronomers to make a significant contribution to the reduction of climate change-inducing greenhouse gas emissions, we first must quantify the sources of our emissions and review the most effective approaches for reducing them. Here we estimate that Australian astronomers' total greenhouse gas emissions from their regular work activities are {$\greaterequivlnt$}25 ktCO2e yr\textendash 1 (equivalent kilotonnes of carbon dioxide per year). This can be broken into \textasciitilde 15 ktCO2e yr\textendash 1 from supercomputer usage, \textasciitilde 4.2 ktCO2e yr\textendash 1 from flights (where individuals' flight emissions correlate with seniority), {$>$}3.3 ktCO2e yr\textendash 1 from the operation of observatories, and 2.6 {$\pm$} 0.4 ktCO2e yr\textendash 1 from powering office buildings. Split across faculty scientists, postdoctoral researchers and PhD students, this averages to {$\greaterequivlnt$}37 tCO2e yr\textendash 1 per astronomer, more than 40\% greater than the average Australian non-dependant's emissions in total, and equivalent to around five times the global average. To combat these environmentally unsustainable practices, we suggest that astronomers should strongly preference the use of supercomputers, observatories and office spaces that are predominantly powered by renewable energy sources. Where current facilities do not meet this requirement, their funders should be lobbied to invest in renewables, such as solar or wind farms. Air travel should also be reduced wherever possible, replaced primarily by video conferencing, which should also promote inclusivity.},
  copyright = {2020 Springer Nature Limited},
  language = {en},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Reviews Subject\_term: Astronomy and planetary science;Business and industry;Climate change;Energy and behaviour;Ethics Subject\_term\_id: astronomy-and-planetary-science;business-and-industry;climate-change;energy-and-behaviour;ethics},
  file = {/Users/milan/Zotero/storage/CLMIK75E/Stevens et al. - 2020 - The imperative to reduce carbon emissions in astro.pdf;/Users/milan/Zotero/storage/QKJQABIU/s41550-020-1169-1.html}
}

@article{Stewart2017,
  title = {Vertical Resolution of Baroclinic Modes in Global Ocean Models},
  author = {Stewart, K.D. and Hogg, A.McC. and Griffies, S.M. and Heerdegen, A.P. and Ward, M.L. and Spence, P. and England, M.H.},
  year = {2017},
  month = may,
  journal = {Ocean Modelling},
  volume = {113},
  pages = {50--65},
  issn = {14635003},
  doi = {10.1016/j.ocemod.2017.03.012},
  abstract = {Improvements in the horizontal resolution of global ocean models, motivated by the horizontal resolution requirements for specific flow features, has advanced modelling capabilities into the dynamical regime dominated by mesoscale variability. In contrast, the choice of the vertical grid remains a subjective choice, and it is not clear that efforts to improve vertical resolution adequately support their horizontal counterparts. Indeed, considering that the bulk of the vertical ocean dynamics (including convection) are parameterized, it is not immediately obvious what the vertical grid is supposed to resolve. Here, we propose that the primary purpose of the vertical grid in a hydrostatic ocean model is to resolve the vertical structure of horizontal flows, rather than to resolve vertical motion. With this principle we construct vertical grids based on their abilities to represent baroclinic modal structures commensurate with the theoretical capabilities of a given horizontal grid. This approach is designed to ensure that the vertical grids of global ocean models complement (and, importantly, to not undermine) the resolution capabilities of the horizontal grid. We find that for z-coordinate global ocean models, at least 50 wellpositioned vertical levels are required to resolve the first baroclinic mode, with an additional 25 levels per subsequent mode. High-resolution ocean-sea ice simulations are used to illustrate some of the dynamical enhancements gained by improving the vertical resolution of a 1/10\textdegree{} global ocean model. These enhancements include substantial increases in the sea surface height variance ({$\sim$}30\% increase south of 40\textdegree S), the barotropic and baroclinic eddy kinetic energies (up to 200\% increase on and surrounding the Antarctic continental shelf and slopes), and the overturning streamfunction in potential density space (near-tripling of the Antarctic Bottom Water cell at 65\textdegree S).},
  language = {en}
}

@article{Storch2012,
  title = {An {{Estimate}} of the {{Lorenz Energy Cycle}} for the {{World Ocean Based}} on the {{STORM}}/{{NCEP Simulation}}},
  author = {von Storch, Jin-Song and Eden, Carsten and Fast, Irina and Haak, Helmuth and {Hern{\'a}ndez-Deckers}, Daniel and {Maier-Reimer}, Ernst and Marotzke, Jochem and Stammer, Detlef},
  year = {2012},
  month = dec,
  journal = {Journal of Physical Oceanography},
  volume = {42},
  number = {12},
  pages = {2185--2205},
  issn = {0022-3670, 1520-0485},
  doi = {10.1175/JPO-D-12-079.1},
  abstract = {This paper presents an estimate of the oceanic Lorenz energy cycle derived from a 1/10\textdegree{} simulation forced by 6-hourly fluxes obtained from NCEP\textendash NCAR reanalysis-1. The total rate of energy generation amounts to 6.6 TW, of which 1.9 TW is generated by the time-mean winds and 2.2 TW by the time-varying winds. The dissipation of kinetic energy amounts to 4.4 TW, of which 3 TW originate from the dissipation of eddy kinetic energy. The energy exchange between reservoirs is dominated by the baroclinic pathway and the pathway that distributes the energy generated by the time-mean winds. The former converts 0.7 to 0.8 TW mean available potential energy to eddy available potential energy and finally to eddy kinetic energy, whereas the latter converts 0.5 TW mean kinetic energy to mean available potential energy.},
  language = {en}
}

@article{Stramma2010,
  title = {Ocean Oxygen Minima Expansions and Their Biological Impacts},
  author = {Stramma, Lothar and Schmidtko, Sunke and Levin, Lisa A. and Johnson, Gregory C.},
  year = {2010},
  month = apr,
  journal = {Deep Sea Research Part I: Oceanographic Research Papers},
  volume = {57},
  number = {4},
  pages = {587--595},
  issn = {09670637},
  doi = {10.1016/j.dsr.2010.01.005},
  abstract = {Climate models with biogeochemical components predict declines in oceanic dissolved oxygen with global warming. In coastal regimes oxygen deficits represent acute ecosystem perturbations. Here, we estimate dissolved oxygen differences across the global tropical and subtropical oceans within the oxygen minimum zone (200\textendash 700-dbar depth) between 1960\textendash 1974 (an early period with reliable data) and 1990\textendash 2008 (a recent period capturing ocean response to planetary warming). In most regions of the tropical Pacific, Atlantic, and Indian Oceans the oxygen content in the 200\textendash 700-dbar layer has declined. Furthermore, at 200 dbar, the area with O2 o 70 mmol kg\`A1, where some large mobile macro-organisms are unable to abide, has increased by 4.5 million km2. The tropical low oxygen zones have expanded horizontally and vertically. Subsurface oxygen has decreased adjacent to most continental shelves. However, oxygen has increased in some regions in the subtropical gyres at the depths analyzed. According to literature discussed below, fishing pressure is strong in the open ocean, which may make it difficult to isolate the impact of declining oxygen on fisheries. At shallower depths we predict habitat compression will occur for hypoxia-intolerant taxa, with eventual loss of biodiversity. Should past trends in observed oxygen differences continue into the future, shifts in animal distributions and changes in ecosystem structure could accelerate.},
  language = {en}
}

@article{Strohmeier2021,
  title = {Crowdsourced Air Traffic Data from the {{OpenSky Network}} 2019\textendash 2020},
  author = {Strohmeier, Martin and Olive, Xavier and L{\"u}bbe, Jannis and Sch{\"a}fer, Matthias and Lenders, Vincent},
  year = {2021},
  month = feb,
  journal = {Earth System Science Data},
  volume = {13},
  number = {2},
  pages = {357--366},
  publisher = {{Copernicus GmbH}},
  issn = {1866-3508},
  doi = {10.5194/essd-13-357-2021},
  abstract = {{$<$}p{$><$}strong class="journal-contentHeaderColor"{$>$}Abstract.{$<$}/strong{$>$} The OpenSky Network is a non-profit association that crowdsources the global collection of live air traffic control data broadcast by aircraft and makes them available to researchers.{$<$}/p{$>$} {$<$}p{$>$}OpenSky's data have been used by over 100 academic groups in the past 5 years, with popular research applications ranging from improved weather forecasting to climate analysis. With the COVID-19 outbreak, the demand for live and historic aircraft flight data has surged. Researchers around the world use air traffic data to comprehend the spread of the pandemic and analyse the effects of the global containment measures on economies, climate and other systems.{$<$}/p{$>$} {$<$}p{$>$}With this work, we present a comprehensive air traffic dataset, derived and enriched from the full OpenSky data and made publicly available for the first time ({$<$}a href="\#bib1.bibx12"{$>$}Olive et al.{$<$}/a{$>$}, {$<$}a href="\#bib1.bibx12"{$>$}2020{$<$}/a{$>$}; https://doi.org/10.5281/zenodo.3931948, last access: 9 February 2021). It spans all flights seen by the network's more than 3500 members between 1 January 2019 and 1 July 2020. The archive is being updated every month and for the first 18 months includes 41 900 660 flights, from 160 737 aircraft, which were seen to frequent 13 934 airports in 127 countries.{$<$}/p{$>$}},
  language = {English},
  file = {/Users/milan/Zotero/storage/WXHZ4NEM/Strohmeier et al. - 2021 - Crowdsourced air traffic data from the OpenSky Net.pdf;/Users/milan/Zotero/storage/4AFZPL7Y/2021.html}
}

@article{Sukoriansky2007,
  title = {On the {{Arrest}} of {{Inverse Energy Cascade}} and the {{Rhines Scale}}},
  author = {Sukoriansky, Semion and Dikovskaya, Nadejda and Galperin, Boris},
  year = {2007},
  month = sep,
  journal = {Journal of the Atmospheric Sciences},
  volume = {64},
  number = {9},
  pages = {3312--3327},
  issn = {0022-4928, 1520-0469},
  doi = {10.1175/JAS4013.1},
  abstract = {The notion of the cascade arrest in a NL-plane turbulence in the context of continuously forced flows is revised in this paper using both theoretical analysis and numerical simulations. It is demonstrated that the upscale energy propagation cannot be stopped by a NL effect and can only be absorbed by friction. A fundamental dimensional parameter in flows with a NL effect, the Rhines scale, LR , has traditionally been associated with the cascade arrest or with the scale that separates turbulence and Rossby wave\textendash dominated spectral ranges. It is shown that rather than being a measure of the inverse cascade arrest, LR is a characteristic of different processes in different flow regimes. In unsteady flows, LR can be identified with the moving energy front propagating toward the decreasing wavenumbers. When large-scale energy sink is present, NL-plane turbulence may attain several steady-state regimes. Two of these regimes are highlighted: friction-dominated and zonostrophic. In the former, LR does not have any particular significance, while in the latter, the Rhines scale nearly coincides with the characteristic length associated with the large-scale friction. Spectral analysis in the frequency domain demonstrates that Rossby waves coexist with turbulence on scales smaller than LR thus indicating that the Rhines scale cannot be viewed as a crossover between turbulence and Rossby wave ranges.},
  language = {en}
}

@inproceedings{Sun2020,
  title = {Ultra-{{Low Precision}} 4-Bit {{Training}} of {{Deep Neural Networks}}},
  booktitle = {{{NeurIPS}}},
  author = {Sun, Xiao and Wang, Naigang and Chen, Chia-yu and Ni, Jia-min},
  year = {2020},
  pages = {12},
  abstract = {In this paper, we propose a number of novel techniques and numerical representation formats that enable, for the very first time, the precision of training systems to be aggressively scaled from 8-bits to 4-bits. To enable this advance, we explore a novel adaptive Gradient Scaling technique (GradScale) that addresses the challenges of insufficient range and resolution in quantized gradients as well as explores the impact of quantization errors observed during model training. We theoretically analyze the role of bias in gradient quantization and propose solutions that mitigate the impact of this bias on model convergence. Finally, we examine our techniques on a spectrum of deep learning models in computer vision, speech and NLP. In combination with previously proposed solutions for 4-bit quantization of weight and activation tensors, 4-bit training shows non-significant loss in accuracy across application domains while enabling significant hardware acceleration ({$>$}7\texttimes{} over state of the art FP16 systems).},
  language = {en},
  file = {/Users/milan/Zotero/storage/DIE8JQJX/Sun et al. - Ultra-Low Precision 4-bit Training of Deep Neural .pdf}
}

@article{Taguchi2014,
  title = {Predictability of {{Major Stratospheric Sudden Warmings}} of the {{Vortex Split Type}}: {{Case Study}} of the 2002 {{Southern Event}} and the 2009 and 1989 {{Northern Events}}},
  shorttitle = {Predictability of {{Major Stratospheric Sudden Warmings}} of the {{Vortex Split Type}}},
  author = {Taguchi, Masakazu},
  year = {2014},
  month = aug,
  journal = {Journal of the Atmospheric Sciences},
  volume = {71},
  number = {8},
  pages = {2886--2904},
  issn = {0022-4928, 1520-0469},
  doi = {10.1175/JAS-D-13-078.1},
  abstract = {This study investigates the predictability of three major stratospheric sudden warmings (MSSWs) of the vortex split type: the Southern Hemisphere case in September 2002 and two Northern Hemisphere cases in January 2009 and February 1989. The author examines changes in the predictability of the MSSWs with lead time, as well as the connection of the predictability to lower-atmospheric features for pre- and post-MSSW periods. The Japan Meteorological Agency (JMA)'s 1-month ensemble hindcast (HC) experiment data are compared to the Japanese 25-year Reanalysis Project (JRA-25)/JMA Climate Data Assimilation System (JCDAS) data.},
  language = {en}
}

@techreport{Takano1982,
  title = {A {{Fourth Order Energy}} and {{Potential Enstrophy Conserving Difference Scheme}}:},
  shorttitle = {A {{Fourth Order Energy}} and {{Potential Enstrophy Conserving Difference Scheme}}},
  author = {Takano, Kenji and Wurtele, M. G.},
  year = {1982},
  month = jun,
  address = {{Fort Belvoir, VA}},
  institution = {{Defense Technical Information Center}},
  doi = {10.21236/ADA126626},
  language = {en}
}

@article{Tantet2018,
  title = {Resonances in a {{Chaotic Attractor Crisis}} of the {{Lorenz Flow}}},
  author = {Tantet, Alexis and Lucarini, Valerio and Dijkstra, Henk A.},
  year = {2018},
  month = feb,
  journal = {Journal of Statistical Physics},
  volume = {170},
  number = {3},
  pages = {584--616},
  issn = {0022-4715, 1572-9613},
  doi = {10.1007/s10955-017-1938-0},
  abstract = {Local bifurcations of stationary points and limit cycles have successfully been characterized in terms of the critical exponents of these solutions. Lyapunov exponents and their associated covariant Lyapunov vectors have been proposed as tools for supporting the understanding of critical transitions in chaotic dynamical systems. However, it is in general not clear how the statistical properties of dynamical systems change across a boundary crisis during which a chaotic attractor collides with a saddle. This behavior is investigated here for a boundary crisis in the Lorenz flow, for which neither the Lyapunov exponents nor the covariant Lyapunov vectors provide a criterion for the crisis. Instead, the convergence of the time evolution of probability densities to the invariant measure, governed by the semigroup of transfer operators, is expected to slow down at the approach of the crisis. Such convergence is described by the eigenvalues of the generator of this semigroup, which can be divided into two families, referred to as the stable and unstable Ruelle\textendash Pollicott resonances, respectively. The former describes the convergence of densities to the attractor (or escape from a repeller) and is estimated from many short time series sampling the state space. The latter is responsible for the decay of correlations, or mixing, and can be estimated from a long times series, invoking ergodicity. It is found numerically for the Lorenz flow that the stable resonances do approach the imaginary axis during the crisis, as is indicative of the loss of global stability of the attractor. On the other hand, the unstable resonances, and a fortiori the decay of correlations, do not flag the proximity of the crisis, thus questioning the usual design of early warning indicators of boundary crises of chaotic attractors and the applicability of response theory close to such crises.},
  language = {en}
}

@inproceedings{Targett2015,
  title = {Lower Precision for Higher Accuracy: {{Precision}} and Resolution Exploration for Shallow Water Equations},
  shorttitle = {Lower Precision for Higher Accuracy},
  booktitle = {2015 {{International Conference}} on {{Field Programmable Technology}} ({{FPT}})},
  author = {Targett, James Stanley and Niu, Xinyu and Russell, Francis and Luk, Wayne and Jeffress, Stephen and Duben, Peter},
  year = {2015},
  month = dec,
  pages = {208--211},
  publisher = {{IEEE}},
  address = {{Queenstown, New Zealand}},
  doi = {10.1109/FPT.2015.7393152},
  abstract = {Accurate forecasts of future climate with numerical models of atmosphere and ocean are of vital importance. However, forecast quality is often limited by the available computational power. This paper investigates the acceleration of a C-grid shallow water model through the use of reduced precision targeting FPGA technology. Using a double-gyre scenario, we show that the mantissa length of variables can be reduced to 14 bits without affecting the accuracy beyond the error inherent in the model. Our reduced precision FPGA implementation runs 5.4 times faster than a double precision FPGA implementation, and 12 times faster than a multi-threaded CPU implementation. Moreover, our reduced precision FPGA implementation uses 39 times less energy than the CPU implementation and can compute a 100x100 grid for the same energy that the CPU implementation would take for a 29x29 grid.},
  isbn = {978-1-4673-9091-0},
  language = {en}
}

@article{Teoh2020,
  title = {Mitigating the {{Climate Forcing}} of {{Aircraft Contrails}} by {{Small}}-{{Scale Diversions}} and {{Technology Adoption}}},
  author = {Teoh, Roger and Schumann, Ulrich and Majumdar, Arnab and Stettler, Marc E. J.},
  year = {2020},
  month = mar,
  journal = {Environmental Science \& Technology},
  volume = {54},
  number = {5},
  pages = {2941--2950},
  publisher = {{American Chemical Society}},
  issn = {0013-936X},
  doi = {10.1021/acs.est.9b05608},
  abstract = {The climate forcing of contrails and induced-cirrus cloudiness is thought to be comparable to the cumulative impacts of aviation CO2 emissions. This paper estimates the impact of aviation contrails on climate forcing for flight track data in Japanese airspace and propagates uncertainties arising from meteorology and aircraft black carbon (BC) particle number emissions. Uncertainties in the contrail age, coverage, optical properties, radiative forcing, and energy forcing (EF) from individual flights can be 2 orders of magnitude larger than the fleet-average values. Only 2.2\% [2.0, 2.5\%] of flights contribute to 80\% of the contrail EF in this region. A small-scale strategy of selectively diverting 1.7\% of the fleet could reduce the contrail EF by up to 59.3\% [52.4, 65.6\%], with only a 0.014\% [0.010, 0.017\%] increase in total fuel consumption and CO2 emissions. A low-risk strategy of diverting flights only if there is no fuel penalty, thereby avoiding additional long-lived CO2 emissions, would reduce contrail EF by 20.0\% [17.4, 23.0\%]. In the longer term, widespread use of new engine combustor technology, which reduces BC particle emissions, could achieve a 68.8\% [45.2, 82.1\%] reduction in the contrail EF. A combination of both interventions could reduce the contrail EF by 91.8\% [88.6, 95.8\%].}
}

@article{Thornes2017,
  title = {On the Use of Scale-Dependent Precision in {{Earth System}} Modelling: {{Scale}}-{{Dependent Precision}} in {{Earth System Modelling}}},
  shorttitle = {On the Use of Scale-Dependent Precision in {{Earth System}} Modelling},
  author = {Thornes, Tobias and D{\"u}ben, Peter and Palmer, Tim},
  year = {2017},
  month = jan,
  journal = {Quarterly Journal of the Royal Meteorological Society},
  volume = {143},
  number = {703},
  pages = {897--908},
  issn = {00359009},
  doi = {10.1002/qj.2974},
  language = {en}
}

@article{Thyng2016,
  title = {True {{Colors}} of {{Oceanography}}: {{Guidelines}} for {{Effective}} and {{Accurate Colormap Selection}}},
  shorttitle = {True {{Colors}} of {{Oceanography}}},
  author = {Thyng, Kristen and Greene, Chad and Hetland, Robert and Zimmerle, Heather and DiMarco, Steven},
  year = {2016},
  month = sep,
  journal = {Oceanography},
  volume = {29},
  number = {3},
  pages = {9--13},
  issn = {10428275},
  doi = {10.5670/oceanog.2016.66},
  language = {en}
}

@article{TintoPrims2019,
  title = {How to Use Mixed Precision in Ocean Models: Exploring a Potential Reduction of Numerical Precision in {{NEMO}} 4.0 and {{ROMS}} 3.6},
  shorttitle = {How to Use Mixed Precision in Ocean Models},
  author = {Tint{\'o} Prims, Oriol and Acosta, Mario C. and Moore, Andrew M. and Castrillo, Miguel and Serradell, Kim and Cort{\'e}s, Ana and {Doblas-Reyes}, Francisco J.},
  year = {2019},
  month = jul,
  journal = {Geoscientific Model Development},
  volume = {12},
  number = {7},
  pages = {3135--3148},
  publisher = {{Copernicus GmbH}},
  issn = {1991-959X},
  doi = {10.5194/gmd-12-3135-2019},
  abstract = {{$<$}p{$><$}strong{$>$}Abstract.{$<$}/strong{$>$} Mixed-precision approaches can provide substantial speed-ups for both computing- and memory-bound codes with little effort. Most scientific codes have overengineered the numerical precision, leading to a situation in which models are using more resources than required without knowing where they are required and where they are not. Consequently, it is possible to improve computational performance by establishing a more appropriate choice of precision. The only input that is needed is a method to determine which real variables can be represented with fewer bits without affecting the accuracy of the results. This paper presents a novel method that enables modern and legacy codes to benefit from a reduction of the precision of certain variables without sacrificing accuracy. It consists of a simple idea: we reduce the precision of a group of variables and measure how it affects the outputs. Then we can evaluate the level of precision that they truly need. Modifying and recompiling the code for each case that has to be evaluated would require a prohibitive amount of effort. Instead, the method presented in this paper relies on the use of a tool called a reduced-precision emulator (RPE) that can significantly streamline the process. Using the RPE and a list of parameters containing the precisions that will be used for each real variable in the code, it is possible within a single binary to emulate the effect on the outputs of a specific choice of precision. When we are able to emulate the effects of reduced precision, we can proceed with the design of the tests that will give us knowledge of the sensitivity of the model variables regarding their numerical precision. The number of possible combinations is prohibitively large and therefore impossible to explore. The alternative of performing a screening of the variables individually can provide certain insight about the required precision of variables, but, on the other hand, other complex interactions that involve several variables may remain hidden. Instead, we use a divide-and-conquer algorithm that identifies the parts that require high precision and establishes a set of variables that can handle reduced precision. This method has been tested using two state-of-the-art ocean models, the Nucleus for European Modelling of the Ocean (NEMO) and the Regional Ocean Modeling System (ROMS), with very promising results. Obtaining this information is crucial to build an actual mixed-precision version of the code in the next phase that will bring the promised performance benefits.{$<$}/p{$>$}},
  language = {English},
  file = {/Users/milan/Zotero/storage/ZLMJCFIP/Tintó Prims et al. - 2019 - How to use mixed precision in ocean models explor.pdf;/Users/milan/Zotero/storage/3XNIE5KW/2019.html}
}

@article{Treguier1997,
  title = {Parameterization of {{Quasigeostrophic Eddies}} in {{Primitive Equation Ocean Models}}},
  author = {Treguier, A M},
  year = {1997},
  journal = {JOURNAL OF PHYSICAL OCEANOGRAPHY},
  volume = {27},
  pages = {14},
  abstract = {A parameterization of mesoscale eddy fluxes in the ocean should be consistent with the fact that the ocean interior is nearly adiabatic. Gent and McWilliams have described a framework in which this can be approximated in z-coordinate primitive equation models by incorporating the effects of eddies on the buoyancy field through an eddy-induced velocity. It is also natural to base a parameterization on the simple picture of the mixing of potential vorticity in the interior and the mixing of buoyancy at the surface. The authors discuss the various constraints imposed by these two requirements and attempt to clarify the appropriate boundary conditions on the eddy-induced velocities at the surface. Quasigeostrophic theory is used as a guide to the simplest way of satisfying these constraints.},
  language = {en}
}

@article{Tripathi2015,
  title = {The Predictability of the Extratropical Stratosphere on Monthly Time-Scales and Its Impact on the Skill of Tropospheric Forecasts: {{Stratospheric Predictability}} and {{Tropospheric Forecasts}}},
  shorttitle = {The Predictability of the Extratropical Stratosphere on Monthly Time-Scales and Its Impact on the Skill of Tropospheric Forecasts},
  author = {Tripathi, Om P. and Baldwin, Mark and {Charlton-Perez}, Andrew and Charron, Martin and Eckermann, Stephen D. and Gerber, Edwin and Harrison, R. Giles and Jackson, David R. and Kim, Baek-Min and Kuroda, Yuhji and Lang, Andrea and Mahmood, Sana and Mizuta, Ryo and Roff, Greg and Sigmond, Michael and Son, Seok-Woo},
  year = {2015},
  month = apr,
  journal = {Quarterly Journal of the Royal Meteorological Society},
  volume = {141},
  number = {689},
  pages = {987--1003},
  issn = {00359009},
  doi = {10.1002/qj.2432},
  language = {en}
}

@article{Turing1950,
  title = {I.\textemdash{{COMPUTING MACHINERY AND INTELLIGENCE}}},
  author = {Turing, A. M.},
  year = {1950},
  month = oct,
  journal = {Mind},
  volume = {LIX},
  number = {236},
  pages = {433--460},
  issn = {0026-4423},
  doi = {10.1093/mind/LIX.236.433},
  file = {/Users/milan/Zotero/storage/66WKQ3HA/TURING - 1950 - I.—COMPUTING MACHINERY AND INTELLIGENCE.pdf;/Users/milan/Zotero/storage/ZUNNX9RZ/986238.html}
}

@techreport{UKClimateChangeCommittee2020,
  title = {Sixth {{Carbon Budget}}},
  author = {UK Climate Change Committee},
  year = {2020},
  month = dec,
  abstract = {The Sixth Carbon Budget, provides ministers with advice on the volume of greenhouse gases the UK can emit during the period 2033-2037.},
  language = {en-US},
  file = {/Users/milan/Zotero/storage/KKJW5UZD/sixth-carbon-budget.html}
}

@article{Urminsky2010,
  title = {Shadowing Unstable Orbits of the {{Sitnikov}} Elliptic Three-Body Problem},
  author = {Urminsky, D. J.},
  year = {2010},
  month = sep,
  journal = {Monthly Notices of the Royal Astronomical Society},
  volume = {407},
  number = {2},
  pages = {804--811},
  issn = {0035-8711},
  doi = {10.1111/j.1365-2966.2010.16974.x},
  abstract = {Errors in numerical simulations of gravitating systems can be magnified exponentially over short periods of time. Numerical shadowing provides a way of demonstrating that the dynamics represented by numerical simulations are representative of true dynamics. Using the Sitnikov problem as an example, it is demonstrated that unstable orbits of the three-body problem can be shadowed for long periods of time. In addition, it is shown that the stretching of phase space near escape and capture regions is a cause for the failure of the shadowing refinement procedure.},
  file = {/Users/milan/Zotero/storage/GI27IV5K/Urminsky - 2010 - Shadowing unstable orbits of the Sitnikov elliptic.pdf;/Users/milan/Zotero/storage/M6UBNSL3/1116480.html}
}

@book{Vallis2006,
  title = {Atmospheric and {{Oceanic Fluid Dynamics}}},
  author = {Vallis, Geoffrey K.},
  year = {2006},
  publisher = {{Cambridge University Press}}
}

@article{Vallis2016,
  title = {Geophysical Fluid Dynamics: Whence, Whither and Why?},
  shorttitle = {Geophysical Fluid Dynamics},
  author = {Vallis, Geoffrey K.},
  year = {2016},
  month = aug,
  journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {472},
  number = {2192},
  pages = {20160140},
  issn = {1364-5021, 1471-2946},
  doi = {10.1098/rspa.2016.0140},
  language = {en}
}

@article{Vana2017,
  title = {Single {{Precision}} in {{Weather Forecasting Models}}: {{An Evaluation}} with the {{IFS}}},
  shorttitle = {Single {{Precision}} in {{Weather Forecasting Models}}},
  author = {V{\'a}{\v n}a, Filip and D{\"u}ben, Peter and Lang, Simon and Palmer, Tim and Leutbecher, Martin and Salmond, Deborah and Carver, Glenn},
  year = {2017},
  month = feb,
  journal = {Monthly Weather Review},
  volume = {145},
  number = {2},
  pages = {495--502},
  issn = {0027-0644, 1520-0493},
  doi = {10.1175/MWR-D-16-0228.1},
  abstract = {Earth's climate is a nonlinear dynamical system with scale-dependent Lyapunov exponents. As such, an important theoretical question for modeling weather and climate is how much real information is carried in a model's physical variables as a function of scale and variable type. Answering this question is of crucial practical importance given that the development of weather and climate models is strongly constrained by available supercomputer power. As a starting point for answering this question, the impact of limiting almost all real-number variables in the forecasting mode of ECMWF Integrated Forecast System (IFS) from 64 to 32 bits is investigated. Results for annual integrations and medium-range ensemble forecasts indicate no noticeable reduction in accuracy, and an average gain in computational efficiency by approximately 40\%. This study provides the motivation for more scale-selective reductions in numerical precision.},
  language = {en}
}

@inproceedings{vanDam2019,
  title = {An {{Accelerator}} for {{Posit Arithmetic Targeting Posit Level}} 1 {{BLAS Routines}} and {{Pair}}-{{HMM}}},
  booktitle = {Proceedings of the {{Conference}} for {{Next Generation Arithmetic}} 2019},
  author = {{van Dam}, Laurens and Peltenburg, Johan and {Al-Ars}, Zaid and Hofstee, H. Peter},
  year = {2019},
  month = mar,
  series = {{{CoNGA}}'19},
  pages = {1--10},
  publisher = {{Association for Computing Machinery}},
  address = {{Singapore, Singapore}},
  doi = {10.1145/3316279.3316284},
  abstract = {The newly proposed posit number format uses a significantly different approach to represent floating point numbers. This paper introduces a framework for posit arithmetic in reconfigurable logic that maintains full precision in intermediate results. We present the design and implementation of a L1 BLAS arithmetic accelerator on posit vectors leveraging Apache Arrow. For a vector dot product with an input vector length of 106 elements, a hardware speedup of approximately 104 is achieved as compared to posit software emulation. For 32-bit numbers, the decimal accuracy of the posit dot product results improve by one decimal of accuracy on average compared to a software implementation, and two extra decimals compared to the IEEE754 format. We also present a posit-based implementation of pair-HMM. In this case, the hardware speedup vs. a posit-based software implementation ranges from 105 to 106. With appropriate initial scaling constants, accuracy improves on an implementation based on IEEE 754.},
  isbn = {978-1-4503-7139-1},
  keywords = {accelerator,arithmetic,BLAS,decimal accuracy,FPGA,pair-HMM,posit,unum,unum-III},
  file = {/Users/milan/Zotero/storage/RR2FCBD4/van Dam et al. - 2019 - An Accelerator for Posit Arithmetic Targeting Posi.pdf}
}

@article{Vecchi2006,
  title = {Weakening of Tropical {{Pacific}} Atmospheric Circulation Due to Anthropogenic Forcing},
  author = {Vecchi, Gabriel A. and Soden, Brian J. and Wittenberg, Andrew T. and Held, Isaac M. and Leetmaa, Ants and Harrison, Matthew J.},
  year = {2006},
  month = may,
  journal = {Nature},
  volume = {441},
  number = {7089},
  pages = {73--76},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature04744},
  language = {en}
}

@article{Vecchi2007,
  title = {Global {{Warming}} and the {{Weakening}} of the {{Tropical Circulation}}},
  author = {Vecchi, Gabriel A. and Soden, Brian J.},
  year = {2007},
  month = sep,
  journal = {Journal of Climate},
  volume = {20},
  number = {17},
  pages = {4316--4340},
  issn = {0894-8755, 1520-0442},
  doi = {10.1175/JCLI4258.1},
  abstract = {This study examines the response of the tropical atmospheric and oceanic circulation to increasing greenhouse gases using a coordinated set of twenty-first-century climate model experiments performed for the Intergovernmental Panel on Climate Change (IPCC) Fourth Assessment Report (AR4). The strength of the atmospheric overturning circulation decreases as the climate warms in all IPCC AR4 models, in a manner consistent with the thermodynamic scaling arguments of Held and Soden. The weakening occurs preferentially in the zonally asymmetric (i.e., Walker) rather than zonal-mean (i.e., Hadley) component of the tropical circulation and is shown to induce substantial changes to the thermal structure and circulation of the tropical oceans. Evidence suggests that the overall circulation weakens by decreasing the frequency of strong updrafts and increasing the frequency of weak updrafts, although the robustness of this behavior across all models cannot be confirmed because of the lack of data. As the climate warms, changes in both the atmospheric and ocean circulation over the tropical Pacific Ocean resemble ``El Ni\~no\textendash like'' conditions; however, the mechanisms are shown to be distinct from those of El Ni\~no and are reproduced in both mixed layer and full ocean dynamics coupled climate models. The character of the Indian Ocean response to global warming resembles that of Indian Ocean dipole mode events. The consensus of model results presented here is also consistent with recently detected changes in sea level pressure since the mid\textendash nineteenth century.},
  language = {en}
}

@article{Vecchi2007a,
  title = {Increased Tropical {{Atlantic}} Wind Shear in Model Projections of Global Warming: {{ATLANTIC WIND SHEAR AND GLOBAL WARMING}}},
  shorttitle = {Increased Tropical {{Atlantic}} Wind Shear in Model Projections of Global Warming},
  author = {Vecchi, Gabriel A. and Soden, Brian J.},
  year = {2007},
  month = apr,
  journal = {Geophysical Research Letters},
  volume = {34},
  number = {8},
  issn = {00948276},
  doi = {10.1029/2006GL028905},
  language = {en}
}

@book{Villani2003,
  title = {Topics in {{Optimal Transportation}}},
  author = {Villani, C{\'e}dric},
  year = {2003},
  publisher = {{American Mathematical Soc.}},
  abstract = {Cedric Villani's book is a lucid and very readable documentation of the tremendous recent analytic progress in ``optimal mass transportation'' theory and of its diverse and unexpected applications in optimization, nonlinear PDE, geometry, and mathematical physics. --Lawrence C. Evans, University of California at Berkeley In 1781, Gaspard Monge defined the problem of ``optimal transportation'', or the transferring of mass with the least possible amount of work, with applications to engineering in mind. In 1942, Leonid Kantorovich applied the newborn machinery of linear programming to Monge's problem, with applications to economics in mind. In 1987, Yann Brenier used optimal transportation to prove a new projection theorem on the set of measure preserving maps, with applications to fluid mechanics in mind. Each of these contributions marked the beginning of a whole mathematical theory, with many unexpected ramifications. Nowadays, the Monge-Kantorovich problem is used and studied by researchers from extremely diverse horizons, including probability theory, functional analysis, isoperimetry, partial differential equations, and even meteorology. Originating from a graduate course, the present volume is at once an introduction to the field of optimal transportation and a survey of the research on the topic over the last 15 years. The book is intended for graduate students and researchers, and it covers both theory and applications. Readers are only assumed to be familiar with the basics of measure theory and functional analysis.},
  googlebooks = {idyFAwAAQBAJ},
  isbn = {978-0-8218-3312-4},
  language = {en},
  keywords = {Mathematics / General}
}

@article{Visbeck,
  title = {As the Complex Interplay of Forces in the Ocean Responds to Climate Change, the Dynamics of Global Ocean Circulation Are Shifting.},
  author = {Visbeck, Martin},
  pages = {1},
  language = {en}
}

@inproceedings{Vitasek1969,
  title = {The Numerical Stability in Solution of Differential Equations},
  booktitle = {Conference on the {{Numerical Solution}} of {{Differential Equations}}},
  author = {Vitasek, Emil},
  editor = {Morris, J. Li.},
  year = {1969},
  series = {Lecture {{Notes}} in {{Mathematics}}},
  pages = {87--111},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/BFb0060017},
  isbn = {978-3-540-36158-9},
  language = {en},
  keywords = {Arithmetic Operation,Discretization Error,Lipschitz Condition,Numerical Process,Numerical Stability}
}

@article{Voigt2021,
  title = {Cleaner Burning Aviation Fuels Can Reduce Contrail Cloudiness},
  author = {Voigt, Christiane and Kleine, Jonas and Sauer, Daniel and Moore, Richard H. and Br{\"a}uer, Tiziana and Le Clercq, Patrick and Kaufmann, Stefan and Scheibe, Monika and {Jurkat-Witschas}, Tina and Aigner, Manfred and Bauder, Uwe and Boose, Yvonne and Borrmann, Stephan and Crosbie, Ewan and Diskin, Glenn S. and DiGangi, Joshua and Hahn, Valerian and Heckl, Christopher and Huber, Felix and Nowak, John B. and Rapp, Markus and Rauch, Bastian and Robinson, Claire and Schripp, Tobias and Shook, Michael and Winstead, Edward and Ziemba, Luke and Schlager, Hans and Anderson, Bruce E.},
  year = {2021},
  month = jun,
  journal = {Communications Earth \& Environment},
  volume = {2},
  number = {1},
  pages = {1--10},
  publisher = {{Nature Publishing Group}},
  issn = {2662-4435},
  doi = {10.1038/s43247-021-00174-y},
  abstract = {Contrail cirrus account for the major share of aviation's climate impact. Yet, the links between jet fuel composition, contrail microphysics and climate impact remain unresolved. Here we present unique observations from two DLR-NASA aircraft campaigns that measured exhaust and contrail characteristics of an Airbus A320 burning either standard jet fuels or low aromatic sustainable aviation fuel blends. Our results show that soot particles can regulate the number of contrail cirrus ice crystals for current emission levels. We provide experimental evidence that burning low aromatic sustainable aviation fuel can result in a 50 to 70\% reduction in soot and ice number concentrations and an increase in ice crystal size. Reduced contrail ice numbers cause less energy deposition in the atmosphere and less warming. Meaningful reductions in aviation's climate impact could therefore be obtained from the widespread adoptation of low aromatic fuels, and from regulations to lower the maximum aromatic fuel content.},
  copyright = {2021 The Author(s)},
  language = {en},
  file = {/Users/milan/Zotero/storage/KJ4GK7UU/Voigt et al. - 2021 - Cleaner burning aviation fuels can reduce contrail.pdf;/Users/milan/Zotero/storage/NKTTTIWB/s43247-021-00174-y.html}
}

@article{vonLarcher2019,
  title = {On Identification of Self-Similar Characteristics Using the {{Tensor Train}} Decomposition Method with Application to Channel Turbulence Flow},
  author = {{von Larcher}, Thomas and Klein, Rupert},
  year = {2019},
  month = apr,
  journal = {Theoretical and Computational Fluid Dynamics},
  volume = {33},
  number = {2},
  pages = {141--159},
  issn = {1432-2250},
  doi = {10.1007/s00162-019-00485-z},
  abstract = {A study on the application of the Tensor Train decomposition method to 3D direct numerical simulation data of channel turbulence flow is presented. The approach is validated with respect to compression rate and storage requirement. In tests with synthetic data, it is found that grid-aligned self-similar patterns are well captured and also the application to non-grid-aligned self-similarity yields satisfying results. It is observed that the shape of the input Tensor significantly affects the compression rate. Applied to data of channel turbulent flow, the Tensor Train format allows for surprisingly high compression rates whilst ensuring low relative errors. However, the results indicate that representation of highly irregular flows at low ranks cannot be expected.},
  language = {en},
  file = {/Users/milan/Zotero/storage/UBP5CY95/von Larcher and Klein - 2019 - On identification of self-similar characteristics .pdf}
}

@article{vonLarcher2019a,
  title = {On Identification of Self-Similar Characteristics Using the {{Tensor Train}} Decomposition Method with Application to Channel Turbulence Flow},
  author = {{von Larcher}, Thomas and Klein, Rupert},
  year = {2019},
  month = apr,
  journal = {Theoretical and Computational Fluid Dynamics},
  volume = {33},
  number = {2},
  pages = {141--159},
  issn = {1432-2250},
  doi = {10.1007/s00162-019-00485-z},
  abstract = {A study on the application of the Tensor Train decomposition method to 3D direct numerical simulation data of channel turbulence flow is presented. The approach is validated with respect to compression rate and storage requirement. In tests with synthetic data, it is found that grid-aligned self-similar patterns are well captured and also the application to non-grid-aligned self-similarity yields satisfying results. It is observed that the shape of the input Tensor significantly affects the compression rate. Applied to data of channel turbulent flow, the Tensor Train format allows for surprisingly high compression rates whilst ensuring low relative errors. However, the results indicate that representation of highly irregular flows at low ranks cannot be expected.},
  language = {en},
  file = {/Users/milan/Zotero/storage/S2EWUEWJ/von Larcher and Klein - 2019 - On identification of self-similar characteristics .pdf}
}

@article{Voosen2020,
  title = {Europe Is Building a `Digital Twin' of {{Earth}} to Revolutionize Climate Forecasts},
  author = {Voosen, Paul},
  year = {2020},
  month = oct,
  journal = {Science | AAAS},
  doi = {10.1126/science.abf0687},
  abstract = {Ingesting more data than ever, exascale model will simulate the impact of climate change on humans},
  language = {en},
  file = {/Users/milan/Zotero/storage/UAZ9PNSQ/europe-building-digital-twin-earth-revolutionize-climate-forecasts.html}
}

@article{Wagner,
  title = {Eulerian and {{Lagrangian}} Tracer Spreading in an High Resolution {{Ocean General Circulation Model}}},
  author = {Wagner, Patrick},
  pages = {59},
  language = {en}
}

@article{Wang2004,
  title = {Image Quality Assessment: From Error Visibility to Structural Similarity},
  shorttitle = {Image Quality Assessment},
  author = {Wang, Zhou and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
  year = {2004},
  month = apr,
  journal = {IEEE Transactions on Image Processing},
  volume = {13},
  number = {4},
  pages = {600--612},
  issn = {1941-0042},
  doi = {10.1109/TIP.2003.819861},
  abstract = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http://www.cns.nyu.edu//spl sim/lcv/ssim/.},
  keywords = {Data mining,Degradation,Humans,Image quality,Indexes,Layout,Quality assessment,Transform coding,Visual perception,Visual system},
  file = {/Users/milan/Zotero/storage/AXW4I2NZ/1284395.html}
}

@article{Wang2004a,
  title = {Image Quality Assessment: From Error Visibility to Structural Similarity},
  shorttitle = {Image Quality Assessment},
  author = {Wang, Zhou and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
  year = {2004},
  month = apr,
  journal = {IEEE Transactions on Image Processing},
  volume = {13},
  number = {4},
  pages = {600--612},
  issn = {1941-0042},
  doi = {10.1109/TIP.2003.819861},
  abstract = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http://www.cns.nyu.edu//spl sim/lcv/ssim/.},
  keywords = {Data mining,Degradation,Humans,Image quality,Indexes,Layout,Quality assessment,Transform coding,Visual perception,Visual system},
  file = {/Users/milan/Zotero/storage/TGEVHGIP/Wang et al. - 2004 - Image quality assessment from error visibility to.pdf;/Users/milan/Zotero/storage/9VVUDIMD/1284395.html}
}

@article{Wang2017,
  title = {Decomposition of the {{Mean Barotropic Transport}} in a {{High}}-{{Resolution Model}} of the {{North Atlantic Ocean}}},
  author = {Wang, Yuan and Claus, Martin and Greatbatch, Richard J. and Sheng, Jinyu},
  year = {2017},
  month = nov,
  journal = {Geophysical Research Letters},
  volume = {44},
  number = {22},
  pages = {11,537--11,546},
  issn = {0094-8276, 1944-8007},
  doi = {10.1002/2017GL074825},
  abstract = {We show how a barotropic shallow water model can be used to decompose the mean barotropic transport from a high-resolution ocean model based on the vertically averaged momentum equations. We apply the method to a high-resolution model of the North Atlantic for which the local vorticity budget is both noisy and dominated by small spatial scales. The shallow water model acts as an effective filter and clearly reveals the transport driven by each term. The potential energy (joint effect of baroclinicity and bottom relief) term is the most important for driving transport, including in the northwest corner, while mean flow advection is important for driving transport along f/H contours around the Labrador Sea continental slope. Both the eddy momentum flux and the mean flow advection terms drive significant transport along the pathway of the Gulf Stream and the North Atlantic Current.},
  language = {en}
}

@article{Wedi2014,
  title = {Increasing Horizontal Resolution in Numerical Weather Prediction and Climate Simulations: Illusion or Panacea?},
  shorttitle = {Increasing Horizontal Resolution in Numerical Weather Prediction and Climate Simulations},
  author = {Wedi, Nils P.},
  year = {2014},
  month = jun,
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {372},
  number = {2018},
  pages = {20130289},
  publisher = {{Royal Society}},
  doi = {10.1098/rsta.2013.0289},
  abstract = {The steady path of doubling the global horizontal resolution approximately every 8 years in numerical weather prediction (NWP) at the European Centre for Medium Range Weather Forecasts may be substan- tially altered with emerging novel computing architectures. It coincides with the need to appropriately address and determine forecast uncertainty with increasing resolution, in particular, when convective-scale motions start to be resolved. Blunt increases in the model resolution will quickly become unaffordable and may not lead to improved NWP forecasts. Consequently, there is a need to accordingly adjust proven numerical techniques. An informed decision on the modelling strategy for harnessing exascale, massively parallel computing power thus also requires a deeper understanding of the sensitivity to uncertainty\textemdash for each part of the model\textemdash and ultimately a deeper understanding of multi-scale interactions in the atmosphere and their numerical realization in ultra-high-resolution NWP and climate simulations. This paper explores opportunities for substantial increases in the forecast efficiency by judicious adjustment of the formal accuracy or relative resolution in the spectral and physical space. One path is to reduce the formal accuracy by which the spectral transforms are computed. The other pathway explores the importance of the ratio used for the horizontal resolution in gridpoint space versus wavenumbers in spectral space. This is relevant for both high-resolution simulations as well as ensemble-based uncertainty estimation.},
  file = {/Users/milan/Zotero/storage/LR89FTQP/Wedi - 2014 - Increasing horizontal resolution in numerical weat.pdf;/Users/milan/Zotero/storage/WSPT2A4A/rsta.2013.html}
}

@article{Williams2016,
  title = {Improved {{Climate Simulations}} through a {{Stochastic Parameterization}} of {{Ocean Eddies}}},
  author = {Williams, Paul D. and Howe, Nicola J. and Gregory, Jonathan M. and Smith, Robin S. and Joshi, Manoj M.},
  year = {2016},
  month = dec,
  journal = {Journal of Climate},
  volume = {29},
  number = {24},
  pages = {8763--8781},
  issn = {0894-8755, 1520-0442},
  doi = {10.1175/JCLI-D-15-0746.1},
  abstract = {In climate simulations, the impacts of the subgrid scales on the resolved scales are conventionally represented using deterministic closure schemes, which assume that the impacts are uniquely determined by the resolved scales. Stochastic parameterization relaxes this assumption, by sampling the subgrid variability in a computationally inexpensive manner. This study shows that the simulated climatological state of the ocean is improved in many respects by implementing a simple stochastic parameterization of ocean eddies into a coupled atmosphere\textendash ocean general circulation model. Simulations from a highresolution, eddy-permitting ocean model are used to calculate the eddy statistics needed to inject realistic stochastic noise into a low-resolution, non-eddy-permitting version of the same model. A suite of four stochastic experiments is then run to test the sensitivity of the simulated climate to the noise definition by varying the noise amplitude and decorrelation time within reasonable limits. The addition of zero-mean noise to the ocean temperature tendency is found to have a nonzero effect on the mean climate. Specifically, in terms of the ocean temperature and salinity fields both at the surface and at depth, the noise reduces many of the biases in the low-resolution model and causes it to more closely resemble the highresolution model. The variability of the strength of the global ocean thermohaline circulation is also improved. It is concluded that stochastic ocean perturbations can yield reductions in climate model error that are comparable to those obtained by refining the resolution, but without the increased computational cost. Therefore, stochastic parameterizations of ocean eddies have the potential to significantly improve climate simulations.},
  language = {en}
}

@misc{WMO2003,
  title = {Guide to the {{WMO Table Driven Code Form Used}} for the {{Representation}} and {{Exchange}} of {{Regularly Spaced Data In Binary Form}}: {{FM}} 92 {{GRIB Edition}} 2},
  author = {WMO},
  year = {2003},
  publisher = {{World Meteorological Organization}}
}

@inproceedings{Woodring2011,
  title = {Revisiting Wavelet Compression for Large-Scale Climate Data Using {{JPEG}} 2000 and Ensuring Data Precision},
  booktitle = {2011 {{IEEE Symposium}} on {{Large Data Analysis}} and {{Visualization}}},
  author = {Woodring, J. and Mniszewski, S. and Brislawn, C. and DeMarle, D. and Ahrens, J.},
  year = {2011},
  month = oct,
  pages = {31--38},
  doi = {10.1109/LDAV.2011.6092314},
  abstract = {We revisit wavelet compression by using a standards-based method to reduce large-scale data sizes for production scientific computing. Many of the bottlenecks in visualization and analysis come from limited bandwidth in data movement, from storage to networks. The majority of the processing time for visualization and analysis is spent reading or writing large-scale data or moving data from a remote site in a distance scenario. Using wavelet compression in JPEG 2000, we provide a mechanism to vary data transfer time versus data quality, so that a domain expert can improve data transfer time while quantifying compression effects on their data. By using a standards-based method, we are able to provide scientists with the state-of-the-art wavelet compression from the signal processing and data compression community, suitable for use in a production computing environment. To quantify compression effects, we focus on measuring bit rate versus maximum error as a quality metric to provide precision guarantees for scientific analysis on remotely compressed POP (Parallel Ocean Program) data.},
  keywords = {Bit rate,Data visualization,Image coding,Quantization,Transform coding,Wavelet transforms},
  file = {/Users/milan/Zotero/storage/56RQA7SD/6092314.html}
}

@article{Wunsch2004,
  title = {{{VERTICAL MIXING}}, {{ENERGY}}, {{AND THE GENERAL CIRCULATION OF THE OCEANS}}},
  author = {Wunsch, Carl and Ferrari, Raffaele},
  year = {2004},
  month = jan,
  journal = {Annual Review of Fluid Mechanics},
  volume = {36},
  number = {1},
  pages = {281--314},
  issn = {0066-4189, 1545-4479},
  doi = {10.1146/annurev.fluid.36.050802.122121},
  language = {en}
}

@article{Wynes2019,
  title = {Academic Air Travel Has a Limited Influence on Professional Success},
  author = {Wynes, Seth and Donner, Simon D. and Tannason, Steuart and Nabors, Noni},
  year = {2019},
  month = jul,
  journal = {Journal of Cleaner Production},
  volume = {226},
  pages = {959--967},
  issn = {0959-6526},
  doi = {10.1016/j.jclepro.2019.04.109},
  abstract = {Lowering the growth in greenhouse gas emissions from air travel may be critical for avoiding dangerous levels of climate change, and yet some individuals perceive frequent air travel to be critical to their professional success. Using a sample of 705 travellers at the University of British Columbia, we investigated the influence of career stage, research productivity, field of expertise, and other variables on academic air travel and the associated emissions. This is the first time that research has evaluated the link between observed air travel and academic success. First, we compared air travel behaviour at different career stages and found that individuals at the start of their careers were responsible for fewer emissions from air travel than senior academics. Second, since career advancement may depend on an academic's ability to form partnerships and disseminate their research abroad, we investigated the relationship between air travel emissions and publicly available bibliometric measurements. We found no relationship between air travel emissions and metrics of academic productivity including hIa (h-index adjusted for academic age and discipline). There was, however, a relationship between emissions and salary that remains significant even when controlling for seniority. Finally, based on the premise that academics studying topics related to sustainability may have greater responsibility or motivation to reduce their emissions, we coded 165 researchers in our sample as either ``Green'' or ``Not-green.'' We found no significant difference between Green and Not-green academics in total air travel emissions, or in the types of emissions that might be easiest to avoid. Taken together, this preliminary evidence suggests that there may be opportunities, especially for academics who study topics related to climate and sustainability, to reduce their emissions from air travel while maintaining productive careers.},
  language = {en},
  keywords = {Air travel,Climate change mitigation,Flying,Pro-environmental behaviors,Professional success,Scholarly metrics},
  file = {/Users/milan/Zotero/storage/WIR32R3I/Wynes et al. - 2019 - Academic air travel has a limited influence on pro.pdf;/Users/milan/Zotero/storage/82QYMZFI/S0959652619311862.html}
}

@article{Yalniz2021,
  title = {Coarse Graining the State Space of a Turbulent Flow Using Periodic Orbits},
  author = {Yaln{\i}z, G{\"o}khan and Hof, Bj{\"o}rn and Budanur, Nazmi Burak},
  year = {2021},
  month = jun,
  journal = {Physical Review Letters},
  volume = {126},
  number = {24},
  eprint = {2007.02584},
  eprinttype = {arxiv},
  pages = {244502},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.126.244502},
  abstract = {We show that turbulent dynamics that arise in simulations of the three-dimensional Navier--Stokes equations in a triply-periodic domain under sinusoidal forcing can be described as transient visits to the neighborhoods of unstable time-periodic solutions. Based on this description, we reduce the original system with more than \$10\^5\$ degrees of freedom to a 17-node Markov chain where each node corresponds to the neighborhood of a periodic orbit. The model accurately reproduces long-term averages of the system's observables as weighted sums over the periodic orbits.},
  archiveprefix = {arXiv},
  keywords = {Nonlinear Sciences - Chaotic Dynamics,Physics - Fluid Dynamics},
  file = {/Users/milan/Zotero/storage/D79DN8KP/Yalnız et al. - 2021 - Coarse graining the state space of a turbulent flo.pdf;/Users/milan/Zotero/storage/VCGI8KQ6/2007.html}
}

@article{Yang2019,
  title = {The Impact of Stochastic Physics on the {{El Ni\~no Southern Oscillation}} in the {{EC}}-{{Earth}} Coupled Model},
  author = {Yang, Chunxue and Christensen, Hannah M. and Corti, Susanna and {von Hardenberg}, Jost and Davini, Paolo},
  year = {2019},
  month = sep,
  journal = {Climate Dynamics},
  volume = {53},
  number = {5},
  pages = {2843--2859},
  issn = {1432-0894},
  doi = {10.1007/s00382-019-04660-0},
  abstract = {The impact of stochastic physics on El Ni\~no Southern Oscillation (ENSO) is investigated in the EC-Earth coupled climate model. By comparing an ensemble of three members of control historical simulations with three ensemble members that include stochastics physics in the atmosphere, we find that in EC-Earth the implementation of stochastic physics improves the excessively weak representation of ENSO. Specifically, the amplitude of both El Ni\~no and, to a lesser extent, La Ni\~na increases. Stochastic physics also ameliorates the temporal variability of ENSO at interannual time scales, demonstrated by the emergence of peaks in the power spectrum with periods of 5\textendash 7~years and 3\textendash 4~years. Based on the analogy with the behaviour of an idealized delayed oscillator model (DO) with stochastic noise, we find that when the atmosphere\textendash ocean coupling is small (large) the amplitude of ENSO increases (decreases) following an amplification of the noise amplitude. The underestimated ENSO variability in the EC-Earth control runs and the associated amplification due to stochastic physics could be therefore consistent with an excessively weak atmosphere\textendash ocean coupling. The activation of stochastic physics in the atmosphere increases westerly wind burst (WWB) occurrences (i.e. amplification of noise amplitude) that could trigger more and stronger El Ni\~no events (i.e. increase of ENSO oscillation) in the coupled EC-Earth model. Further analysis of the mean state bias of EC-Earth suggests that a cold sea surface temperature (SST) and dry precipitation bias in the central tropical Pacific together with a warm SST and wet precipitation bias in the western tropical Pacific are responsible for the coupled feedback bias (weak coupling) in the tropical Pacific that is related to the weak ENSO simulation. The same analysis of the ENSO behaviour is carried out in a future scenario experiment (RCP8.5 forcing), highlighting that in a coupled model with an extreme warm SST, characterized by a strong coupling, the effect of stochastic physics on the ENSO representation is opposite. This corroborates the hypothesis that the mean state bias of the tropical Pacific region is the main reason for the ENSO representation deficiency in EC-Earth.},
  language = {en},
  file = {/Users/milan/Zotero/storage/ZVJQ8587/Yang et al. - 2019 - The impact of stochastic physics on the El Niño So.pdf}
}

@article{Yao2020,
  title = {Transforming Carbon Dioxide into Jet Fuel Using an Organic Combustion-Synthesized {{Fe}}-{{Mn}}-{{K}} Catalyst},
  author = {Yao, Benzhen and Xiao, Tiancun and Makgae, Ofentse A. and Jie, Xiangyu and {Gonzalez-Cortes}, Sergio and Guan, Shaoliang and Kirkland, Angus I. and Dilworth, Jonathan R. and {Al-Megren}, Hamid A. and Alshihri, Saeed M. and Dobson, Peter J. and Owen, Gari P. and Thomas, John M. and Edwards, Peter P.},
  year = {2020},
  month = dec,
  journal = {Nature Communications},
  volume = {11},
  number = {1},
  pages = {6395},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-20214-z},
  abstract = {With mounting concerns over climate change, the utilisation or conversion of carbon dioxide into sustainable, synthetic hydrocarbons fuels, most notably for transportation purposes, continues to attract worldwide interest. This is particularly true in the search for sustainable or renewable aviation fuels. These offer considerable potential since, instead of consuming fossil crude oil, the fuels are produced from carbon dioxide using sustainable renewable hydrogen and energy. We report here a synthetic protocol to the fixation of carbon dioxide by converting it directly into aviation jet fuel using novel, inexpensive iron-based catalysts. We prepare the Fe-Mn-K catalyst by the so-called Organic Combustion Method, and the catalyst shows a carbon dioxide conversion through hydrogenation to hydrocarbons in the aviation jet fuel range of 38.2\%, with a yield of 17.2\%, and a selectivity of 47.8\%, and with an attendant low carbon monoxide (5.6\%) and methane selectivity (10.4\%). The conversion reaction also produces light olefins ethylene, propylene, and butenes, totalling a yield of 8.7\%, which are important raw materials for the petrochemical industry and are presently also only obtained from fossil crude oil. As this carbon dioxide is extracted from air, and re-emitted from jet fuels when combusted in flight, the overall effect is a carbon-neutral fuel. This contrasts with jet fuels produced from hydrocarbon fossil sources where the combustion process unlocks the fossil carbon and places it into the atmosphere, in longevity, as aerial carbon - carbon dioxide.},
  copyright = {2020 The Author(s)},
  language = {en},
  file = {/Users/milan/Zotero/storage/8249A54U/Yao et al. - 2020 - Transforming carbon dioxide into jet fuel using an.pdf}
}

@article{Yuval2021,
  title = {Use of {{Neural Networks}} for {{Stable}}, {{Accurate}} and {{Physically Consistent Parameterization}} of {{Subgrid Atmospheric Processes With Good Performance}} at {{Reduced Precision}}},
  author = {Yuval, Janni and O'Gorman, Paul A. and Hill, Chris N.},
  year = {2021},
  journal = {Geophysical Research Letters},
  volume = {48},
  number = {6},
  pages = {e2020GL091363},
  issn = {1944-8007},
  doi = {10.1029/2020GL091363},
  abstract = {A promising approach to improve climate-model simulations is to replace traditional subgrid parameterizations based on simplified physical models by machine learning algorithms that are data-driven. However, neural networks (NNs) often lead to instabilities and climate drift when coupled to an atmospheric model. Here, we learn an NN parameterization from a high-resolution atmospheric simulation in an idealized domain by accurately calculating subgrid terms through coarse graining. The NN parameterization has a structure that ensures physical constraints are respected, such as by predicting subgrid fluxes instead of tendencies. The NN parameterization leads to stable simulations that replicate the climate of the high-resolution simulation with similar accuracy to a successful random-forest parameterization while needing far less memory. We find that the simulations are stable for different horizontal resolutions and a variety of NN architectures, and that an NN with substantially reduced numerical precision could decrease computational costs without affecting the quality of simulations.},
  copyright = {\textcopyright{} 2021. The Authors.},
  language = {en},
  keywords = {Atmospheric modeling,Convection,machine learning,parameterization,subgrid physics},
  annotation = {\_eprint: https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2020GL091363},
  file = {/Users/milan/Zotero/storage/TYZ5N62A/Yuval et al. - 2021 - Use of Neural Networks for Stable, Accurate and Ph.pdf;/Users/milan/Zotero/storage/PECD7V75/2020GL091363.html}
}

@article{Zamo2018,
  title = {Estimation of the {{Continuous Ranked Probability Score}} with {{Limited Information}} and {{Applications}} to {{Ensemble Weather Forecasts}}},
  author = {Zamo, Micha{\"e}l and Naveau, Philippe},
  year = {2018},
  month = feb,
  journal = {Mathematical Geosciences},
  volume = {50},
  number = {2},
  pages = {209--234},
  issn = {1874-8953},
  doi = {10.1007/s11004-017-9709-7},
  abstract = {The continuous ranked probability score (CRPS) is a much used measure of performance for probabilistic forecasts of a scalar observation. It is a quadratic measure of the difference between the forecast cumulative distribution function (CDF) and the empirical CDF of the observation. Analytic formulations of the CRPS can be derived for most classical parametric distributions, and be used to assess the efficiency of different CRPS estimators. When the true forecast CDF is not fully known, but represented as an ensemble of values, the CRPS is estimated with some error. Thus, using the CRPS to compare parametric probabilistic forecasts with ensemble forecasts may be misleading due to the unknown error of the estimated CRPS for the ensemble. With simulated data, the impact of the type of the verified ensemble (a random sample or a set of quantiles) on the CRPS estimation is studied. Based on these simulations, recommendations are issued to choose the most accurate CRPS estimator according to the type of ensemble. The interest of these recommendations is illustrated with real ensemble weather forecasts. Also, relationships between several estimators of the CRPS are demonstrated and used to explain the differences of accuracy between the estimators.},
  language = {en},
  file = {/Users/milan/Zotero/storage/6HAJ7RTD/Zamo and Naveau - 2018 - Estimation of the Continuous Ranked Probability Sc.pdf}
}

@article{Zanna2017,
  title = {Scale-Aware Deterministic and Stochastic Parametrizations of Eddy-Mean Flow Interaction},
  author = {Zanna, Laure and Porta Mana, PierGianLuca and Anstey, James and David, Tomos and Bolton, Thomas},
  year = {2017},
  month = mar,
  journal = {Ocean Modelling},
  volume = {111},
  pages = {66--80},
  issn = {14635003},
  doi = {10.1016/j.ocemod.2017.01.004},
  abstract = {The role of mesoscale eddies is crucial for the ocean circulation and its energy budget. The sub-grid scale eddy variability needs to be parametrized in ocean models, even at so-called eddy permitting resolutions. Porta Mana and Zanna (2014) propose an eddy parametrization based on a non-Newtonian stress which depends on the partially resolved scales and their variability. In the present study, we test two versions of the parametrization, one deterministic and one stochastic, at coarse and eddy-permitting resolutions in a double gyre quasi-geostrophic model. The parametrization leads to drastic improvements in the mean state and variability of the ocean state, namely in the jet rectification and the kinetic-energy spectra as a function of wavenumber and frequency for eddy permitting models. The parametrization also appears to have a stabilizing effect on the model, especially the stochastic version. The parametrization possesses attractive features for implementation in global models: very little computational cost, it is flow aware and uses the properties of the underlying flow. The deterministic coefficient is scale-aware, while the stochastic parameter is scale- and flow-aware with dependence on resolution, stratification and wind forcing.},
  language = {en}
}

@article{Zender2016,
  title = {Bit {{Grooming}}: Statistically Accurate Precision-Preserving Quantization with Compression, Evaluated in the {{netCDF Operators}} ({{NCO}}, v4.4.8+)},
  shorttitle = {Bit {{Grooming}}},
  author = {Zender, Charles S.},
  year = {2016},
  month = sep,
  journal = {Geoscientific Model Development},
  volume = {9},
  number = {9},
  pages = {3199--3211},
  publisher = {{Copernicus GmbH}},
  issn = {1991-959X},
  doi = {10.5194/gmd-9-3199-2016},
  abstract = {{$<$}p{$><$}strong{$>$}Abstract.{$<$}/strong{$>$} Geoscientific models and measurements generate false precision (scientifically meaningless data bits) that wastes storage space. False precision can mislead (by implying noise is signal) and be scientifically pointless, especially for measurements. By contrast, lossy compression can be both economical (save space) and heuristic (clarify data limitations) without compromising the scientific integrity of data. Data quantization can thus be appropriate regardless of whether space limitations are a concern. We introduce, implement, and characterize a new lossy compression scheme suitable for IEEE floating-point data. Our new Bit Grooming algorithm alternately shaves (to zero) and sets (to one) the least significant bits of consecutive values to preserve a desired precision. This is a symmetric, two-sided variant of an algorithm sometimes called Bit Shaving that quantizes values solely by zeroing bits. Our variation eliminates the artificial low bias produced by always zeroing bits, and makes Bit Grooming more suitable for arrays and multi-dimensional fields whose mean statistics are important. {$<$}br{$><$}br{$>$} Bit Grooming relies on standard lossless compression to achieve the actual reduction in storage space, so we tested Bit Grooming by applying the DEFLATE compression algorithm to bit-groomed and full-precision climate data stored in netCDF3, netCDF4, HDF4, and HDF5 formats. Bit Grooming reduces the storage space required by initially uncompressed and compressed climate data by 25\textendash 80 and 5\textendash 65 \%, respectively, for single-precision values (the most common case for climate data) quantized to retain 1\textendash 5 decimal digits of precision. The potential reduction is greater for double-precision datasets. When used aggressively (i.e., preserving only 1\textendash 2 digits), Bit Grooming produces storage reductions comparable to other quantization techniques such as Linear Packing. Unlike Linear Packing, whose guaranteed precision rapidly degrades within the relatively narrow dynamic range of values that it can compress, Bit Grooming guarantees the specified precision throughout the full floating-point range. Data quantization by Bit Grooming is irreversible (i.e., lossy) yet transparent, meaning that no extra processing is required by data users/readers. Hence Bit Grooming can easily reduce data storage volume without sacrificing scientific precision or imposing extra burdens on users.{$<$}/p{$>$}},
  language = {English},
  file = {/Users/milan/Zotero/storage/II2WHKDH/Zender - 2016 - Bit Grooming statistically accurate precision-pre.pdf;/Users/milan/Zotero/storage/HZX8THMD/2016.html}
}

@article{Zhai2008,
  title = {On the Seasonal Variability of Eddy Kinetic Energy in the {{Gulf Stream}} Region},
  author = {Zhai, Xiaoming and Greatbatch, Richard J. and Kohlmann, Jan-Dirk},
  year = {2008},
  month = dec,
  journal = {Geophysical Research Letters},
  volume = {35},
  number = {24},
  pages = {L24609},
  issn = {0094-8276},
  doi = {10.1029/2008GL036412},
  language = {en}
}

@article{Zhai2010,
  title = {Significant Sink of Ocean-Eddy Energy near Western Boundaries},
  author = {Zhai, Xiaoming and Johnson, Helen L. and Marshall, David P.},
  year = {2010},
  month = sep,
  journal = {Nature Geoscience},
  volume = {3},
  number = {9},
  pages = {608--612},
  issn = {1752-0894, 1752-0908},
  doi = {10.1038/ngeo943},
  language = {en}
}

@article{Zhang2019,
  title = {A Review of {{China}}'s Road Traffic Carbon Emissions},
  author = {Zhang, Linling and Long, Ruyin and Chen, Hong and Geng, Jichao},
  year = {2019},
  month = jan,
  journal = {Journal of Cleaner Production},
  volume = {207},
  pages = {569--581},
  issn = {0959-6526},
  doi = {10.1016/j.jclepro.2018.10.003},
  abstract = {Road traffic is one of the main sources of carbon emissions that cause climate change. Despite numerous studies on road traffic emissions, significant challenges remain in carbon emissions measurement and quantitative evaluation of mitigation effects. This paper first reviews the measurement of carbon emission from road traffic including the top-down model and the bottom-up model. Then, we summarize the main factors that affect the traffic carbon emissions, which are divided into three categories: demand side factors, supply side factors and environmental measurement factors. Finally, traffic mitigation measures from economic, technical and administrative aspects are examined. Based on the review, we can conclude that the results of carbon emissions calculated by different methods are quite different, and there are differences in the accuracy and application scope of various methods. Each type of factor plays a different role in the process of traffic reduction, in which the demand factors are the roots, the supply factors are the means, and the environmental factors are the conditions. The development of traffic mitigation measures is not targeted, and there is a lack of quantitative research on policy effects. In the future, it is necessary to standardize the statistical caliber and error standard of measurement of carbon emission for road traffic, and to clarify the responsibility of emission reduction from various traffic subjects. More research efforts need to be focused on quantifying the effect of mitigation measures.},
  language = {en},
  keywords = {Carbon emissions,Influencing factors,Measurement method,Mitigation measures,Road traffic},
  file = {/Users/milan/Zotero/storage/NYVIWYM2/S0959652618330099.html}
}

@article{Zhang2020,
  title = {Design of {{Power Efficient Posit Multiplier}}},
  author = {Zhang, Hao and Ko, Seok-Bum},
  year = {2020},
  month = may,
  journal = {IEEE Transactions on Circuits and Systems II: Express Briefs},
  volume = {67},
  number = {5},
  pages = {861--865},
  issn = {1558-3791},
  doi = {10.1109/TCSII.2020.2980531},
  abstract = {Posit number system has been used as an alternative to IEEE floating-point number system in many applications, especially the recent popular deep learning. Its non-uniformed number distribution fits well with the data distribution of deep learning and thus can speedup the training process of deep learning. Among all the related arithmetic operations, multiplication is one of the most frequent operations used in applications. However, due to the bit-width flexibility nature of posit numbers, the hardware multiplier is usually designed with the maximum possible mantissa bit-width. As the mantissa bit-width is not always the maximum value, such multiplier design leads to a high power consumption especially when the mantissa bit-width is small. In this brief, a power efficient posit multiplier architecture is proposed. The mantissa multiplier is still designed for the maximum possible bit-width, however, the whole multiplier is divided into multiple smaller multipliers. Only the required small multipliers are enabled at run-time. Those smaller multipliers are controlled by the regime bit-width which can be used to determine the mantissa bit-width. This design technique is applied to 8-bit, 16-bit, and 32-bit posit formats in this brief and an average of 16\% power reduction can be achieved with negligible area and timing overhead.},
  keywords = {computer arithmetic,low-power arithmetic circuit,posit multiplier,Posit number system},
  file = {/Users/milan/Zotero/storage/V7H4Y26E/9035440.html}
}

@inproceedings{Zhao2020,
  title = {Significantly {{Improving Lossy Compression}} for {{HPC Datasets}} with {{Second}}-{{Order Prediction}} and {{Parameter Optimization}}},
  booktitle = {Proceedings of the 29th {{International Symposium}} on {{High}}-{{Performance Parallel}} and {{Distributed Computing}}},
  author = {Zhao, Kai and Di, Sheng and Liang, Xin and Li, Sihuan and Tao, Dingwen and Chen, Zizhong and Cappello, Franck},
  year = {2020},
  month = jun,
  series = {{{HPDC}} '20},
  pages = {89--100},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3369583.3392688},
  abstract = {Today's extreme-scale high-performance computing (HPC) applications are producing volumes of data too large to save or transfer because of limited storage space and I/O bandwidth. Error-bounded lossy compression has been commonly known as one of the best solutions to the big science data issue, because it can significantly reduce the data volume with strictly controlled data distortion based on user requirements. In this work, we develop an adaptive parameter optimization algorithm integrated with a series of optimization strategies for SZ, a state-of-the-art prediction-based compression model. Our contribution is threefold. (1) We exploit effective strategies by using 2nd-order regression and 2nd-order Lorenzo predictors to improve the prediction accuracy significantly for SZ, thus substantially improving the overall compression quality. (2) We design an efficient approach selecting the best-fit parameter setting, by conducting a comprehensive priori compression quality analysis and exploiting an efficient online controlling mechanism. (3) We evaluate the compression quality and performance on a supercomputer with 4,096 cores, as compared with other state-of-the-art error-bounded lossy compressors. Experiments with multiple real-world HPC simulations datasets show that our solution can improve the compression ratio up to 46\% compared with the second-best compressor. Moreover, the parallel I/O performance is improved by up to 40\% thanks to the significant reduction of data size.},
  isbn = {978-1-4503-7052-3},
  keywords = {high-performance computing,lossy compression,parameter optimization,rate distortion,science data},
  file = {/Users/milan/Zotero/storage/NPRAKKHW/Zhao et al. - 2020 - Significantly Improving Lossy Compression for HPC .pdf}
}

@article{Zheng2020,
  title = {Research Trend of Large-Scale Supercomputers and Applications from the {{TOP500}} and {{Gordon Bell Prize}}},
  author = {Zheng, Weimin},
  year = {2020},
  month = jun,
  journal = {Science China Information Sciences},
  volume = {63},
  number = {7},
  pages = {171001},
  issn = {1869-1919},
  doi = {10.1007/s11432-020-2861-0},
  abstract = {China is playing an increasingly important role in international supercomputing. In high-performance computing domain, there are two famous awards: The TOP500 list for the fastest 500 supercomputers in the world and the Gordon Bell Prize for the best HPC (high-performance computing) applications. China has been awarded in both TOP500 list and Gordon Bell Prize. In this paper, we review the supercomputers in the latest TOP500 list and seven Gordon Bell Prize applications to show the research trend of the large-scale supercomputers and applications. The first trend we observe is that heterogeneous architectures are widely used in the construction of supercomputing systems. The second trend is that artificial intelligence applications are expected to become one of the main stream applications of supercomputing. The third trend is that applying heterogeneous systems to complex scientific simulation applications will be more difficult.},
  language = {en},
  file = {/Users/milan/Zotero/storage/M9DPA8J6/Zheng - 2020 - Research trend of large-scale supercomputers and a.pdf}
}

@article{Ziv1977,
  title = {A Universal Algorithm for Sequential Data Compression},
  author = {Ziv, J. and Lempel, A.},
  year = {1977},
  month = may,
  journal = {IEEE Transactions on Information Theory},
  volume = {23},
  number = {3},
  pages = {337--343},
  issn = {1557-9654},
  doi = {10.1109/TIT.1977.1055714},
  abstract = {A universal algorithm for sequential data compression is presented. Its performance is investigated with respect to a nonprobabilistic model of constrained sources. The compression ratio achieved by the proposed universal code uniformly approaches the lower bounds on the compression ratios attainable by block-to-variable codes and variable-to-block codes designed to match a completely specified source.},
  keywords = {Sequential coding,Source coding},
  file = {/Users/milan/Zotero/storage/ZKWDDQP2/Ziv and Lempel - 1977 - A universal algorithm for sequential data compress.pdf;/Users/milan/Zotero/storage/MMLTBAVP/1055714.html}
}

@article{Ziv1978,
  title = {Compression of Individual Sequences via Variable-Rate Coding},
  author = {Ziv, J. and Lempel, A.},
  year = {1978},
  month = sep,
  journal = {IEEE Transactions on Information Theory},
  volume = {24},
  number = {5},
  pages = {530--536},
  issn = {1557-9654},
  doi = {10.1109/TIT.1978.1055934},
  abstract = {Compressibility of individual sequences by the class of generalized finite-state information-lossless encoders is investigated. These encoders can operate in a variable-rate mode as well as a fixed-rate one, and they allow for any finite-state scheme of variable-length-to-variable-length coding. For every individual infinite sequencexa quantity\textbackslash rho(x)is defined, called the compressibility ofx, which is shown to be the asymptotically attainable lower bound on the compression ratio that can be achieved forxby any finite-state encoder. This is demonstrated by means of a constructive coding theorem and its converse that, apart from their asymptotic significance, also provide useful performance criteria for finite and practical data-compression tasks. The proposed concept of compressibility is also shown to play a role analogous to that of entropy in classical information theory where one deals with probabilistic ensembles of sequences rather than with individual sequences. While the definition of\textbackslash rho(x)allows a different machine for each different sequence to be compressed, the constructive coding theorem leads to a universal algorithm that is asymptotically optimal for all sequences.},
  keywords = {Source coding,Variable-rate coding},
  file = {/Users/milan/Zotero/storage/Z7PA5H6I/Ziv and Lempel - 1978 - Compression of individual sequences via variable-r.pdf;/Users/milan/Zotero/storage/K9QCG2RU/1055934.html}
}

@misc{zotero-179,
  title = {The Most Problematic Variable in the Course of Human-Biometeorological Comfort Assessment \textemdash{} the Mean Radiant Temperature in: {{Open Geosciences Volume}} 3 {{Issue}} 1 (2011)},
  howpublished = {https://www.degruyter.com/view/journals/geo/3/1/article-p90.xml},
  file = {/Users/milan/Zotero/storage/LUH32N2D/article-p90.html}
}

@misc{zotero-180,
  title = {Mean Radiant Temperature from Global-Scale Numerical Weather Prediction Models | {{SpringerLink}}},
  howpublished = {https://link.springer.com/article/10.1007\%2Fs00484-020-01900-5},
  file = {/Users/milan/Zotero/storage/GSIFH4VQ/10.html}
}

@misc{zotero-199,
  title = {New {{Methods}} for {{Data Storage}} of {{Model Output}} from {{Ensemble Simulations}}: {{Monthly Weather Review}}: {{Vol}} 147, {{No}} 2},
  howpublished = {https://journals.ametsoc.org/doi/full/10.1175/MWR-D-18-0170.1}
}

@misc{zotero-212,
  title = {Academic Air Travel Has a Limited Influence on Professional Success | {{Elsevier Enhanced Reader}}},
  doi = {10.1016/j.jclepro.2019.04.109},
  howpublished = {https://reader.elsevier.com/reader/sd/pii/S0959652619311862?token=42886EAC4DF264A8D163D7230D834AC3BEDA7C2625256D3C5D4BBB49BD5C74C0A2EFA649F04209C7852024D784EC509D},
  language = {en},
  file = {/Users/milan/Zotero/storage/6J32EAY3/S0959652619311862.html}
}

@misc{zotero-698,
  title = {20 {{Year Passenger Forecast}}},
  abstract = {IATA's 20 Year Passenger Forecast \hspace{0pt}analyzes fundamental drivers of air travel demand to identify major traffic trends and alternate scenarios for the next 20 years.},
  howpublished = {https://www.iata.org/en/publications/store/20-year-passenger-forecast/},
  language = {en},
  file = {/Users/milan/Zotero/storage/EELZ7Z6Q/20-year-passenger-forecast.html}
}

@misc{zotero-801,
  howpublished = {https://sdrbench.github.io/},
  file = {/Users/milan/Zotero/storage/IMRVBHVV/sdrbench.github.io.html}
}

@article{zotero-864,
  type = {Article}
}

@misc{zotero-886,
  title = {{{IPCC Fifth Assessment Synthesis Report}}},
  journal = {IPCC 5th Assessment Synthesis Report},
  abstract = {IPCC 5th Assessment Synthesis Report},
  howpublished = {http://ar5-syr.ipcc.ch/},
  file = {/Users/milan/Zotero/storage/BGQPBFRM/ar5-syr.ipcc.ch.html}
}

@misc{zotero-985,
  title = {Earth {{System Modeling Must Become More Energy Efficient}}},
  journal = {Eos},
  abstract = {As weather and climate models grow larger and more data intensive, the amount of energy needed to run them continues to increase. Are researchers doing enough to minimize the carbon footprint of their computing?},
  howpublished = {https://eos.org/opinions/earth-system-modeling-must-become-more-energy-efficient},
  language = {en-US},
  file = {/Users/milan/Zotero/storage/DJ3R4Q22/earth-system-modeling-must-become-more-energy-efficient.html}
}


