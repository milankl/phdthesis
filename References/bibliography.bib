
@incollection{Arakawa1977,
  title = {Computational {{Design}} of the {{Basic Dynamical Processes}} of the {{UCLA General Circulation Model}}},
  booktitle = {Methods in {{Computational Physics}}: {{Advances}} in {{Research}} and {{Applications}}},
  author = {Arakawa, AKIO and Lamb, VIVIAN R.},
  editor = {Chang, JULIUS},
  year = {1977},
  month = jan,
  volume = {17},
  pages = {173--265},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-0-12-460817-7.50009-4},
  file = {/Users/milan/Zotero/storage/CER48SV4/B9780124608177500094.html},
  language = {en},
  series = {General {{Circulation Models}} of the {{Atmosphere}}}
}

@article{Arakawa1990,
  title = {Energy {{Conserving}} and {{Potential}}-{{Enstrophy Dissipating Schemes}} for the {{Shallow Water Equations}}},
  author = {Arakawa, Akio and Hsu, Yueh-Jiuan G.},
  year = {1990},
  month = oct,
  volume = {118},
  pages = {1960--1969},
  publisher = {{American Meteorological Society}},
  issn = {0027-0644},
  doi = {10.1175/1520-0493(1990)118<1960:ECAPED>2.0.CO;2},
  abstract = {To incorporate potential enstrophy dissipation into discrete shallow water equations with no or arbitrarily small energy dissipation, a family of finite-difference schemes have been derived with which potential enstrophy is guaranteed to decrease while energy is conserved (when the mass flux is nondivergent and time is continuous). Among this family of schemes, there is a member that minimizes the spurious impact of infinite potential vorticities associated with infinitesimal fluid depth. The scheme is, therefore, useful for problems in which the free surface may intersect with the lower boundary.},
  file = {/Users/milan/Zotero/storage/UWI4RRFV/Arakawa and Hsu - 1990 - Energy Conserving and Potential-Enstrophy Dissipat.pdf;/Users/milan/Zotero/storage/8VBFKY27/1520-0493(1990)1181960ECAPED2.0.html},
  journal = {Monthly Weather Review},
  number = {10}
}

@article{Arbic2008,
  title = {On {{Quadratic Bottom Drag}}, {{Geostrophic Turbulence}}, and {{Oceanic Mesoscale Eddies}}},
  author = {Arbic, Brian K. and Scott, Robert B.},
  year = {2008},
  month = jan,
  volume = {38},
  pages = {84--103},
  issn = {0022-3670, 1520-0485},
  doi = {10.1175/2007JPO3653.1},
  abstract = {Many investigators have idealized the oceanic mesoscale eddy field with numerical simulations of geostrophic turbulence forced by a horizontally homogeneous, baroclinically unstable mean flow. To date such studies have employed linear bottom Ekman friction (hereinafter, linear drag). This paper presents simulations of two-layer baroclinically unstable geostrophic turbulence damped by quadratic bottom drag, which is generally thought to be more realistic. The goals of the paper are 1) to describe the behavior of quadratically damped turbulence as drag strength changes, using previously reported behaviors of linearly damped turbulence as a point of comparison, and 2) to compare the eddy energies, baroclinicities, and horizontal scales in both quadratic and linear drag simulations with observations and to discuss the constraints these comparisons place on the form and strength of bottom drag in the ocean. In both quadratic and linear drag simulations, large barotropic eddies develop with weak damping, large equivalent barotropic eddies develop with strong damping, and the comparison in goal 2 above is closest when the nondimensional friction strength parameter is of order 1. Typical values of the quadratic drag coefficient (cd ϳ 0.0025) and of boundary layer depths (Hb ϳ 50 m) imply that the quadratic friction strength parameter cdLd /Hb , where Ld is the deformation radius, may indeed be of order 1 in the ocean. Model eddies are realistic over a wider range of friction strengths when drag is quadratic, because of a reduced sensitivity to friction strength in that case. The quadratic parameter is independent of the mean shear, in contrast to the linear parameter. Plots of eddy length scales, computed from satellite altimeter data, versus mean shear and versus rough estimates of the friction strength parameters suggest that both linear and quadratic bottom drag may be active in the ocean. Topographic wave drag contains terms that are linear in the bottom flow, thus providing some justification for the use of linear bottom drag in models.},
  journal = {Journal of Physical Oceanography},
  language = {en},
  number = {1}
}

@article{Bezanson2017,
  title = {Julia: {{A Fresh Approach}} to {{Numerical Computing}}},
  shorttitle = {Julia},
  author = {Bezanson, Jeff. and Edelman, Alan. and Karpinski, Stefan. and Shah, Viral B.},
  year = {2017},
  month = jan,
  volume = {59},
  pages = {65--98},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0036-1445},
  doi = {10.1137/141000671},
  abstract = {Bridging cultures that have often been distant, Julia combines expertise from the diverse fields of computer science and computational science to create a new approach to numerical  computing. Julia is  designed to be easy and fast and questions notions generally held to be ``laws of nature"  by practitioners of numerical computing: \textbackslash{}beginlist \textbackslash{}item  High-level dynamic programs have to be slow. \textbackslash{}item  One must prototype in one language and then rewrite in another language for speed or deployment. \textbackslash{}item There are parts of a system appropriate for the programmer, and other parts that are best left untouched as they have been built by the experts. \textbackslash{}endlist We introduce the  Julia programming language and its design---a  dance between specialization and abstraction. Specialization allows for custom treatment. Multiple dispatch,  a  technique from computer science, picks  the right algorithm for the right circumstance. Abstraction, which is what good computation is really about, recognizes what remains the same after differences are stripped away. Abstractions in mathematics are captured as code through another technique from computer science, generic programming. Julia shows that  one can achieve machine performance without sacrificing human convenience.},
  file = {/Users/milan/Zotero/storage/SZM3ENMR/Bezanson et al. - 2017 - Julia A Fresh Approach to Numerical Computing.pdf;/Users/milan/Zotero/storage/UG5NFZ28/141000671.html},
  journal = {SIAM Review},
  number = {1}
}

@inproceedings{Burgess2019,
  title = {Bfloat16 {{Processing}} for {{Neural Networks}}},
  booktitle = {2019 {{IEEE}} 26th {{Symposium}} on {{Computer Arithmetic}} ({{ARITH}})},
  author = {Burgess, Neil and Milanovic, Jelena and Stephens, Nigel and Monachopoulos, Konstantinos and Mansell, David},
  year = {2019},
  month = jun,
  pages = {88--91},
  issn = {1063-6889},
  doi = {10.1109/ARITH.2019.00022},
  abstract = {Bfloat16 ("BF16") is a new floating-point format tailored specifically for high-performance processing of Neural Networks and will be supported by major CPU and GPU architectures as well as Neural Network accelerators. This paper proposes a possible implementation of a BF16 multiply-accumulation operation that relaxes several IEEE Floating-Point Standard features to afford low-cost hardware implementations. Specifically, subnorms are flushed to zero; only one non-standard rounding mode (Round-Odd) is supported; NaNs are not propagated; and IEEE exception flags are not provided. The paper shows that this approach achieves the same network-level accuracy as using IEEE single-precision arithmetic ("FP32") for less than half the datapath area cost and with greater throughput.},
  file = {/Users/milan/Zotero/storage/XMKM6HAG/Burgess et al. - 2019 - Bfloat16 Processing for Neural Networks.pdf;/Users/milan/Zotero/storage/VC5Y2QAC/8877390.html},
  keywords = {Artificial neural networks,BF16 multiply-accumulation operation,bfloat16 processing,Computer architecture,CPU architectures,Digital arithmetic,Error analysis,floating point arithmetic,floating-point; rounding mode; neural networks,FP32,GPU architectures,high-performance processing,IEEE exception flags,IEEE floating-point standard features,IEEE single-precision arithmetic,low-cost hardware implementations,network-level accuracy,neural nets,Neural Network accelerators,neural networks,nonstandard rounding mode,Standards,Training}
}

@book{Butcher2016,
  title = {Numerical {{Methods}} for {{Ordinary Differential Equations}}},
  author = {Butcher, J. C.},
  year = {2016},
  month = aug,
  edition = {3 edition},
  publisher = {{Wiley}},
  address = {{Chichester, West Sussex, United Kingdom}},
  abstract = {A new edition of this classic work, comprehensively revised to present exciting new developments in this important subject The study of numerical methods for solving ordinary differential equations is constantly developing and regenerating, and this third edition of a popular classic volume, written by one of the world's leading experts in the field, presents an account of the subject which reflects both its historical and well-established place in computational science and its vital role as a cornerstone of modern applied mathematics. In addition to serving as a broad and comprehensive study of numerical methods for initial value problems, this book contains a special emphasis on Runge-Kutta methods by the mathematician who transformed the subject into its modern form dating from his classic 1963 and 1972 papers.~ A second feature is general linear methods which have now matured and grown from being a framework for a unified theory of a wide range of diverse numerical schemes to a source of new and practical algorithms in their own right.~ As the founder of general linear method research, John Butcher has been a leading contributor to its development; his special role is reflected in the text.~ The book is written in the lucid style characteristic of the author, and combines enlightening explanations with rigorous and precise analysis. In addition to these anticipated features, the book breaks new ground by including the latest results on the highly efficient G-symplectic methods which compete strongly with the well-known symplectic Runge-Kutta methods for long-term integration of conservative mechanical systems. This third edition of Numerical Methods for Ordinary Differential Equations will serve as a key text for senior undergraduate and graduate courses in numerical analysis, and is an essential resource for research workers in applied mathematics, physics and engineering.},
  isbn = {978-1-119-12150-3},
  language = {English}
}

@inproceedings{Chaurasiya2018,
  title = {Parameterized {{Posit Arithmetic Hardware Generator}}},
  booktitle = {2018 {{IEEE}} 36th {{International Conference}} on {{Computer Design}} ({{ICCD}})},
  author = {Chaurasiya, Rohit and Gustafson, John and Shrestha, Rahul and Neudorfer, Jonathan and Nambiar, Sangeeth and Niyogi, Kaustav and Merchant, Farhad and Leupers, Rainer},
  year = {2018},
  month = oct,
  pages = {334--341},
  publisher = {{IEEE}},
  address = {{Orlando, FL, USA}},
  doi = {10.1109/ICCD.2018.00057},
  abstract = {Hardware implementation of Floating Point Units (FPUs) has been a key area of research due to their massive area and energy footprints. Recently, a proposal was made to replace IEEE 754-2008 technical standard compliant FPUs with Posit Arithmetic Units (PAUs) due to the greater accuracy, speed, and simpler hardware design. In this paper, we present the architecture of a parameterized PAU generator that can generate PAU adders and PAU multipliers of any bit-width pre-synthesis. We synthesize generated arithmetic units using the parameterized PAU generator for 8-bit, 16-bit, and 32-bit adders and multipliers and compare them with IEEE 754-2008 compliant adders and multipliers. Both, synthesis for Field Programmable Gate Array (FPGA) and Application Specific Integrated Circuit (ASIC) are performed. In our comparison of m-bit PAU units with n-bit IEEE 754-2008 compliant units, it is observed that the area and energy of a PAU adder and multiplier are comparable to their IEEE 754-2008 compliant counterparts where m = n. We argue that an n-bit IEEE 754-2008 adder and multiplier can be safely replaced with an m-bit PAU adder and multiplier where m {$<$} n, due to superior numerical accuracy of the PAU; we also compare m-bit PAU adders and multipliers with n-bit IEEE 754-2008 compliant adders and multipliers. As an application example, we examine performance in the domain of signal processing with and without PAU adders and multipliers, and show the advantage of our approach.},
  isbn = {978-1-5386-8477-1},
  language = {en}
}

@inproceedings{Chen2018,
  title = {A Matrix-Multiply Unit for Posits in Reconfigurable Logic Leveraging (Open){{CAPI}}},
  booktitle = {Proceedings of the {{Conference}} for {{Next Generation Arithmetic}} on - {{CoNGA}} '18},
  author = {Chen, Jianyu and {Al-Ars}, Zaid and Hofstee, H. Peter},
  year = {2018},
  pages = {1--5},
  publisher = {{ACM Press}},
  address = {{Singapore, Singapore}},
  doi = {10.1145/3190339.3190340},
  abstract = {In this paper, we present the design in reconfigurable logic of a matrix multiplier for matrices of 32-bit posit numbers with es=2 [1]. Vector dot products are computed without intermediate rounding as suggested by the proposed posit standard to maximally retain precision. An initial implementation targets the CAPI 1.0 interface on the POWER8 processor and achieves about 10Gpops (Giga posit operations per second). Follow-on implementations targeting CAPI 2.0 and OpenCAPI 3.0 on POWER9 are expected to achieve up to 64Gpops. Our design is available under a permissive open source license at https://github.com/ChenJianyunp/Unum\_matrix\_multiplier. We hope the current work, which works on CAPI 1.0, along with future community contributions, will help enable a more extensive exploration of this proposed new format.},
  isbn = {978-1-4503-6414-0},
  language = {en}
}

@article{Choo2003,
  title = {Two's Complement Computation Sharing Multiplier and Its Applications to High Performance {{DFE}}},
  author = {Choo, Hunsoo and Muhammad, K. and Roy, K.},
  year = {2003},
  volume = {51},
  pages = {458--469},
  issn = {1053-587X},
  abstract = {Two's complement computation sharing multiplier and its applications to high performance DFE},
  file = {/Users/milan/Zotero/storage/NTB2YSNF/Twos_complement_computation_sharing_multiplier_and_its_applications_to_high_performance_DFE.html},
  journal = {IEEE Transactions on Signal Processing},
  language = {en}
}

@article{Dawson2017,
  title = {Rpe v5: An Emulator for Reduced Floating-Point Precision in Large Numerical Simulations},
  shorttitle = {Rpe V5},
  author = {Dawson, Andrew and D{\"u}ben, Peter D.},
  year = {2017},
  month = jun,
  volume = {10},
  pages = {2221--2230},
  issn = {1991-9603},
  doi = {10.5194/gmd-10-2221-2017},
  abstract = {This paper describes the rpe (reduced-precision emulator) library which has the capability to emulate the use of arbitrary reduced floating-point precision within large numerical models written in Fortran. The rpe software allows model developers to test how reduced floating-point precision affects the result of their simulations without having to make extensive code changes or port the model onto specialized hardware. The software can be used to identify parts of a program that are problematic for numerical precision and to guide changes to the program to allow a stronger reduction in precision.},
  journal = {Geoscientific Model Development},
  language = {en},
  number = {6}
}

@article{Diamantakis2013,
  title = {The Semi-{{Lagrangian}} Technique in Atmospheric Modelling: Current Status and Future Challenges},
  author = {Diamantakis, Michail},
  year = {2013},
  pages = {18},
  abstract = {The semi-Lagrangian method is an established numerical technique for integrating the transport equations in atmospheric models. Coupled with semi-implicit time-stepping offers unconditional stability for all forcing terms in equation sets of such models. This distinct advantage has led to the development of very efficient numerical weather prediction systems such as the ECMWF Integrated Forecasting System (IFS).},
  language = {en}
}

@article{Duben2014a,
  title = {The Use of Imprecise Processing to Improve Accuracy in Weather \& Climate Prediction},
  author = {D{\"u}ben, Peter D. and McNamara, Hugh and Palmer, T.N.},
  year = {2014},
  month = aug,
  volume = {271},
  pages = {2--18},
  issn = {00219991},
  doi = {10.1016/j.jcp.2013.10.042},
  abstract = {The use of stochastic processing hardware and low precision arithmetic in atmospheric models is investigated. Stochastic processors allow hardware-induced faults in calculations, sacrificing bit-reproducibility and precision in exchange for improvements in performance and potentially accuracy of forecasts, due to a reduction in power consumption that could allow higher resolution. A similar trade-off is achieved using low precision arithmetic, with improvements in computation and communication speed and savings in storage and memory requirements. As high-performance computing becomes more massively parallel and power intensive, these two approaches may be important stepping stones in the pursuit of global cloud-resolving atmospheric modelling.},
  journal = {Journal of Computational Physics},
  language = {en}
}

@article{Duben2018,
  title = {A {{New Number Format}} for {{Ensemble Simulations}}},
  author = {D{\"u}ben, Peter D.},
  year = {2018},
  month = nov,
  volume = {10},
  pages = {2983--2991},
  issn = {1942-2466, 1942-2466},
  doi = {10.1029/2018MS001420},
  journal = {Journal of Advances in Modeling Earth Systems},
  language = {en},
  number = {11}
}

@book{Gill1982,
  title = {Atmosphere-Ocean Dynamics},
  author = {Gill, Adrian E.},
  year = {1982},
  publisher = {{Acad. Press}},
  address = {{San Diego}},
  isbn = {978-0-12-283520-9 978-0-12-283522-3},
  language = {en},
  note = {OCLC: 249294465},
  number = {30},
  series = {International Geophysics Series}
}

@article{Glaser2017,
  title = {An 826 {{MOPS}}, 210 {{uW}}/{{MHz Unum ALU}} in 65 Nm},
  author = {Glaser, Florian and Mach, Stefan and Rahimi, Abbas and G{\"u}rkaynak, Frank K. and Huang, Qiuting and Benini, Luca},
  year = {2017},
  month = dec,
  abstract = {To overcome the limitations of conventional floatingpoint number formats, an interval arithmetic and variablewidth storage format called universal number (unum) has been recently introduced [1]. This paper presents the first (to the best of our knowledge) silicon implementation measurements of an application-specific integrated circuit (ASIC) for unum floating-point arithmetic. The designed chip includes a 128-bit wide unum arithmetic unit to execute additions and subtractions, while also supporting lossless (for intermediate results) and lossy (for external data movements) compression units to exploit the memory usage reduction potential of the unum format. Our chip, fabricated in a 65 nm CMOS process, achieves a maximum clock frequency of 413 MHz at 1.2 V with an average measured power of 210 uW/MHz.},
  archivePrefix = {arXiv},
  eprint = {1712.01021},
  eprinttype = {arxiv},
  journal = {arXiv:1712.01021 [cs]},
  keywords = {Computer Science - Hardware Architecture},
  language = {en},
  primaryClass = {cs}
}

@article{Griffies2000,
  title = {Biharmonic {{Friction}} with a {{Smagorinsky}}-{{Like Viscosity}} for {{Use}} in {{Large}}-{{Scale Eddy}}-{{Permitting Ocean Models}}},
  author = {Griffies, Stephen M and Hallberg, Robert W},
  year = {2000},
  volume = {128},
  pages = {12},
  abstract = {This paper discusses a numerical closure, motivated from the ideas of Smagorinsky, for use with a biharmonic operator. The result is a highly scale-selective, state-dependent friction operator for use in eddy-permitting geophysical fluid models. This friction should prove most useful for large-scale ocean models in which there are multiple regimes of geostrophic turbulence. Examples are provided from primitive equation geopotential and isopycnal-coordinate ocean models.},
  journal = {MONTHLY WEATHER REVIEW},
  language = {en}
}

@article{Gupta2015,
  title = {Deep {{Learning}} with {{Limited Numerical Precision}}},
  author = {Gupta, Suyog and Agrawal, Ankur and Gopalakrishnan, Kailash and Narayanan, Pritish},
  year = {2015},
  month = feb,
  abstract = {Training of large-scale deep neural networks is often constrained by the available computational resources. We study the effect of limited precision data representation and computation on neural network training. Within the context of low-precision fixed-point computations, we observe the rounding scheme to play a crucial role in determining the network's behavior during training. Our results show that deep networks can be trained using only 16-bit wide fixed-point number representation when using stochastic rounding, and incur little to no degradation in the classification accuracy. We also demonstrate an energy-efficient hardware accelerator that implements low-precision fixed-point arithmetic with stochastic rounding.},
  archivePrefix = {arXiv},
  eprint = {1502.02551},
  eprinttype = {arxiv},
  file = {/Users/milan/Zotero/storage/M2DCVEID/Gupta et al. - 2015 - Deep Learning with Limited Numerical Precision.pdf;/Users/milan/Zotero/storage/SZ8FYIJX/1502.html},
  journal = {arXiv:1502.02551 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{Gustafson2017,
  title = {Beating {{Floating Point}} at Its {{Own Game}}: {{Posit Arithmetic}}},
  author = {Gustafson, John L and Yonemoto, Isaac},
  year = {2017},
  volume = {4},
  pages = {16},
  abstract = {A new data type called a posit is designed as a direct drop-in replacement for IEEE Standard 754 floating-point numbers (floats). Unlike earlier forms of universal number (unum) arithmetic, posits do not require interval arithmetic or variable size operands; like floats, they round if an answer is inexact. However, they provide compelling advantages over floats, including larger dynamic range, higher accuracy, better closure, bitwise identical results across systems, simpler hardware, and simpler exception handling. Posits never overflow to infinity or underflow to zero, and ``Nota-Number'' (NaN) indicates an action instead of a bit pattern. A posit processing unit takes less circuitry than an IEEE float FPU. With lower power use and smaller silicon footprint, the posit operations per second (POPS) supported by a chip can be significantly higher than the FLOPS using similar hardware resources. GPU accelerators and Deep Learning processors, in particular, can do more per watt and per dollar with posits, yet deliver superior answer quality. A comprehensive series of benchmarks compares floats and posits for decimals of accuracy produced for a set precision. Low precision posits provide a better solution than ``approximate computing'' methods that try to tolerate decreased answer quality. High precision posits provide more correct decimals than floats of the same size; in some cases, a 32-bit posit may safely replace a 64-bit float. In other words, posits beat floats at their own game.},
  journal = {Supercomputing Frontiers and Innovations},
  language = {en},
  number = {2}
}

@misc{Gustafson2017a,
  title = {Posit {{Arithmetic}}},
  author = {Gustafson, John},
  year = {2017},
  language = {English}
}

@article{Hatfield2017,
  title = {Improving {{Weather Forecast Skill}} through {{Reduced}}-{{Precision Data Assimilation}}},
  author = {Hatfield, Sam and Subramanian, Aneesh and Palmer, Tim and D{\"u}ben, Peter},
  year = {2017},
  month = nov,
  volume = {146},
  pages = {49--62},
  publisher = {{American Meteorological Society}},
  issn = {0027-0644},
  doi = {10.1175/MWR-D-17-0132.1},
  abstract = {A new approach for improving the accuracy of data assimilation, by trading numerical precision for ensemble size, is introduced. Data assimilation is inherently uncertain because of the use of noisy observations and imperfect models. Thus, the larger rounding errors incurred from reducing precision may be within the tolerance of the system. Lower-precision arithmetic is cheaper, and so by reducing precision in ensemble data assimilation, computational resources can be redistributed toward, for example, a larger ensemble size. Because larger ensembles provide a better estimate of the underlying distribution and are less reliant on covariance inflation and localization, lowering precision could actually permit an improvement in the accuracy of weather forecasts. Here, this idea is tested on an ensemble data assimilation system comprising the Lorenz '96 toy atmospheric model and the ensemble square root filter. The system is run at double-, single-, and half-precision (the latter using an emulation tool), and the performance of each precision is measured through mean error statistics and rank histograms. The sensitivity of these results to the observation error and the length of the observation window are addressed. Then, by reinvesting the saved computational resources from reducing precision into the ensemble size, assimilation error can be reduced for (hypothetically) no extra cost. This results in increased forecasting skill, with respect to double-precision assimilation.},
  file = {/Users/milan/Zotero/storage/JH9Y4ICI/Hatfield et al. - 2017 - Improving Weather Forecast Skill through Reduced-P.pdf;/Users/milan/Zotero/storage/4Z2XMTPM/MWR-D-17-0132.html},
  journal = {Monthly Weather Review},
  number = {1}
}

@article{Hatfield2018,
  title = {Choosing the {{Optimal Numerical Precision}} for {{Data Assimilation}} in the {{Presence}} of {{Model Error}}},
  author = {Hatfield, Sam and D{\"u}ben, Peter and Chantry, Matthew and Kondo, Keiichi and Miyoshi, Takemasa and Palmer, Tim},
  year = {2018},
  month = sep,
  volume = {10},
  pages = {2177--2191},
  issn = {1942-2466, 1942-2466},
  doi = {10.1029/2018MS001341},
  abstract = {The use of reduced numerical precision within an atmospheric data assimilation system is investigated. An atmospheric model with a spectral dynamical core is used to generate synthetic observations, which are then assimilated back into the same model using an ensemble Kalman filter. The effect on the analysis error of reducing precision from 64 bits to only 22 bits is measured and found to depend strongly on the degree of model uncertainty within the system. When the model used to generate the observations is identical to the model used to assimilate observations, the reduced-precision results suffer substantially. However, when model error is introduced by changing the diffusion scheme in the assimilation model or by using a higher-resolution model to generate observations, the difference in analysis quality between the two levels of precision is almost eliminated. Lower-precision arithmetic has a lower computational cost, so lowering precision could free up computational resources in operational data assimilation and allow an increase in ensemble size or grid resolution.},
  journal = {Journal of Advances in Modeling Earth Systems},
  language = {en},
  number = {9}
}

@inproceedings{Hatfield2019,
  title = {Accelerating {{High}}-{{Resolution Weather Models}} with {{Deep}}-{{Learning Hardware}}},
  booktitle = {Proceedings of the {{Platform}} for {{Advanced Scientific Computing Conference}}},
  author = {Hatfield, Sam and Chantry, Matthew and D{\"u}ben, Peter and Palmer, Tim},
  year = {2019},
  month = jun,
  pages = {1--11},
  publisher = {{Association for Computing Machinery}},
  address = {{Zurich, Switzerland}},
  doi = {10.1145/3324989.3325711},
  abstract = {The next generation of weather and climate models will have an unprecedented level of resolution and model complexity, and running these models efficiently will require taking advantage of future supercomputers and heterogeneous hardware. In this paper, we investigate the use of mixed-precision hardware that supports floating-point operations at double-, single- and half-precision. In particular, we investigate the potential use of the NVIDIA Tensor Core, a mixed-precision matrix-matrix multiplier mainly developed for use in deep learning, to accelerate the calculation of the Legendre transforms in the Integrated Forecasting System (IFS), one of the leading global weather forecast models. In the IFS, the Legendre transform is one of the most expensive model components and dominates the computational cost for simulations at a very high resolution. We investigate the impact of mixed-precision arithmetic in IFS simulations of operational complexity through software emulation. Through a targeted but minimal use of double-precision arithmetic we are able to use either half-precision arithmetic or mixed half/single-precision arithmetic for almost all of the calculations in the Legendre transform without affecting forecast skill.},
  file = {/Users/milan/Zotero/storage/MDBH3WB4/Hatfield et al. - 2019 - Accelerating High-Resolution Weather Models with D.pdf},
  isbn = {978-1-4503-6770-7},
  keywords = {floating-point arithmetic,half-precision,Legendre transforms,Numerical weather prediction,spectral models,Tensor Core},
  series = {{{PASC}} '19}
}

@article{Hatfield2020,
  title = {Single-{{Precision}} in the {{Tangent}}-{{Linear}} and {{Adjoint Models}} of {{Incremental 4D}}-{{Var}}},
  author = {Hatfield, Sam and McRae, Andrew and Palmer, Tim and D{\"u}ben, Peter},
  year = {2020},
  month = feb,
  volume = {148},
  pages = {1541--1552},
  publisher = {{American Meteorological Society}},
  issn = {0027-0644},
  doi = {10.1175/MWR-D-19-0291.1},
  abstract = {The use of single-precision arithmetic in ECMWF's forecasting model gave a 40\% reduction in wall-clock time over double-precision, with no decrease in forecast quality. However, using reduced-precision in 4D-Var data assimilation is relatively unexplored and there are potential issues with using single-precision in the tangent-linear and adjoint models. Here, we present the results of reducing numerical precision in an incremental 4D-Var data assimilation scheme, with an underlying two-layer quasigeostrophic model. The minimizer used is the conjugate gradient method. We show how reducing precision increases the asymmetry between the tangent-linear and adjoint models. For ill-conditioned problems, this leads to a loss of orthogonality among the residuals of the conjugate gradient algorithm, which slows the convergence of the minimization procedure. However, we also show that a standard technique, reorthogonalization, eliminates these issues and therefore could allow the use of single-precision arithmetic. This work is carried out within ECMWF's data assimilation framework, the Object Oriented Prediction System.},
  file = {/Users/milan/Zotero/storage/CIJC4PMH/Hatfield et al. - 2020 - Single-Precision in the Tangent-Linear and Adjoint.pdf;/Users/milan/Zotero/storage/6QQMDAPV/MWR-D-19-0291.html},
  journal = {Monthly Weather Review},
  number = {4}
}

@article{Hopkins2020,
  title = {Stochastic Rounding and Reduced-Precision Fixed-Point Arithmetic for Solving Neural Ordinary Differential Equations},
  author = {Hopkins, Michael and Mikaitis, Mantas and Lester, Dave R. and Furber, Steve},
  year = {2020},
  month = mar,
  volume = {378},
  pages = {20190052},
  publisher = {{Royal Society}},
  doi = {10.1098/rsta.2019.0052},
  abstract = {Although double-precision floating-point arithmetic currently dominates high-performance computing, there is increasing interest in smaller and simpler arithmetic types. The main reasons are potential improvements in energy efficiency and memory footprint and bandwidth. However, simply switching to lower-precision types typically results in increased numerical errors. We investigate approaches to improving the accuracy of reduced-precision fixed-point arithmetic types, using examples in an important domain for numerical computation in neuroscience: the solution of ordinary differential equations (ODEs). The Izhikevich neuron model is used to demonstrate that rounding has an important role in producing accurate spike timings from explicit ODE solution algorithms. In particular, fixed-point arithmetic with stochastic rounding consistently results in smaller errors compared to single-precision floating-point and fixed-point arithmetic with round-to-nearest across a range of neuron behaviours and ODE solvers. A computationally much cheaper alternative is also investigated, inspired by the concept of dither that is a widely understood mechanism for providing resolution below the least significant bit in digital signal processing. These results will have implications for the solution of ODEs in other subject areas, and should also be directly relevant to the huge range of practical problems that are represented by partial differential equations.This article is part of a discussion meeting issue `Numerical algorithms for high-performance computational science'.},
  file = {/Users/milan/Zotero/storage/T8HIKUSG/Hopkins et al. - 2020 - Stochastic rounding and reduced-precision fixed-po.pdf;/Users/milan/Zotero/storage/MNKDL738/rsta.2019.html},
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  number = {2166}
}

@article{Jeffress2017,
  title = {Bitwise Efficiency in Chaotic Models},
  author = {Jeffress, Stephen and D{\"u}ben, Peter and Palmer, Tim},
  year = {2017},
  month = sep,
  volume = {473},
  pages = {20170144},
  issn = {1364-5021, 1471-2946},
  doi = {10.1098/rspa.2017.0144},
  journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  language = {en},
  number = {2205}
}

@article{Johnson2018,
  title = {Rethinking Floating Point for Deep Learning},
  author = {Johnson, Jeff},
  year = {2018},
  month = nov,
  abstract = {Reducing hardware overhead of neural networks for faster or lower power inference and training is an active area of research. Uniform quantization using integer multiply-add has been thoroughly investigated, which requires learning many quantization parameters, fine-tuning training or other prerequisites. Little effort is made to improve floating point relative to this baseline; it remains energy inefficient, and word size reduction yields drastic loss in needed dynamic range. We improve floating point to be more energy efficient than equivalent bit width integer hardware on a 28 nm ASIC process while retaining accuracy in 8 bits with a novel hybrid log multiply/linear add, Kulisch accumulation and tapered encodings from Gustafson's posit format. With no network retraining, and drop-in replacement of all math and float32 parameters via round-to-nearest-even only, this open-sourced 8-bit log float is within 0.9\% top-1 and 0.2\% top-5 accuracy of the original float32 ResNet-50 CNN model on ImageNet. Unlike int8 quantization, it is still a general purpose floating point arithmetic, interpretable out-of-the-box. Our 8/38-bit log float multiply-add is synthesized and power profiled at 28 nm at 0.96\texttimes{} the power and 1.12\texttimes{} the area of 8/32-bit integer multiply-add. In 16 bits, our log float multiply-add is 0.59\texttimes{} the power and 0.68\texttimes{} the area of IEEE 754 float16 fused multiply-add, maintaining the same signficand precision and dynamic range, proving useful for training ASICs as well.},
  archivePrefix = {arXiv},
  eprint = {1811.01721},
  eprinttype = {arxiv},
  journal = {arXiv:1811.01721 [cs]},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{Jouppi2017a,
  ids = {Jouppi2017},
  title = {In-{{Datacenter Performance Analysis}} of a {{Tensor Processing Unit}}},
  booktitle = {Proceedings of the 44th {{Annual International Symposium}} on {{Computer Architecture}}},
  author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
  year = {2017},
  month = jun,
  pages = {1--12},
  publisher = {{Association for Computing Machinery}},
  address = {{Toronto, ON, Canada}},
  doi = {10.1145/3079856.3080246},
  abstract = {Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC---called a Tensor Processing Unit (TPU) --- deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95\% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X -- 30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X -- 80X higher. Moreover, using the CPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU.},
  file = {/Users/milan/Zotero/storage/V7Z7TZAM/Jouppi et al. - 2017 - In-Datacenter Performance Analysis of a Tensor Pro.pdf;/Users/milan/Zotero/storage/ZDX2MXAM/Jouppi et al. - 2017 - In-Datacenter Performance Analysis of a Tensor Pro.pdf},
  isbn = {978-1-4503-4892-8},
  keywords = {accelerator,CNN,deep learning,DNN,domain-specific architecture,GPU,LSTM,MLP,neural network,RNN,TensorFlow,TPU},
  series = {{{ISCA}} '17}
}

@article{Jouppi2018,
  title = {A Domain-Specific Architecture for Deep Neural Networks},
  author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David},
  year = {2018},
  month = aug,
  volume = {61},
  pages = {50--59},
  issn = {0001-0782},
  doi = {10.1145/3154484},
  abstract = {Tensor processing units improve performance per watt of neural networks in Google datacenters by roughly 50x.},
  file = {/Users/milan/Zotero/storage/Z4NRNJF9/Jouppi et al. - 2018 - A domain-specific architecture for deep neural net.pdf},
  journal = {Communications of the ACM},
  number = {9}
}

@article{Jouppi2018a,
  title = {Motivation for and {{Evaluation}} of the {{First Tensor Processing Unit}}},
  author = {Jouppi, Norman and Young, Cliff and Patil, Nishant and Patterson, David},
  year = {2018},
  month = may,
  volume = {38},
  pages = {10--19},
  issn = {1937-4143},
  doi = {10.1109/MM.2018.032271057},
  abstract = {The first-generation tensor processing unit (TPU) runs deep neural network (DNN) inference 15-30 times faster with 30-80 times better energy efficiency than contemporary CPUs and GPUs in similar semiconductor technologies. This domain-specific architecture (DSA) is a custom chip that has been deployed in Google datacenters since 2015, where it serves billions of people.},
  file = {/Users/milan/Zotero/storage/YVFJ4I7D/8358031.html},
  journal = {IEEE Micro},
  keywords = {Central Processing Unit,computer centres,coprocessors,Data centers,deep neural network,deep neural network inference,DNN,domain-specific architecture,energy efficiency,Energy efficiency,feedforward neural nets,first-generation tensor processing unit,Google datacenters,GPU,Graphics processing units,hardware,machine learning,microprocessor,Microprocessors,motivation,Neural networks,parallel architectures,Semiconductor devices,semiconductor technologies,Tensile stress,tensor processing unit,tensors,TPU},
  number = {3}
}

@article{Kalamkar2019,
  title = {A {{Study}} of {{BFLOAT16}} for {{Deep Learning Training}}},
  author = {Kalamkar, Dhiraj and Mudigere, Dheevatsa and Mellempudi, Naveen and Das, Dipankar and Banerjee, Kunal and Avancha, Sasikanth and Vooturi, Dharma Teja and Jammalamadaka, Nataraj and Huang, Jianyu and Yuen, Hector and Yang, Jiyan and Park, Jongsoo and Heinecke, Alexander and Georganas, Evangelos and Srinivasan, Sudarshan and Kundu, Abhisek and Smelyanskiy, Misha and Kaul, Bharat and Dubey, Pradeep},
  year = {2019},
  month = jun,
  abstract = {This paper presents the first comprehensive empirical study demonstrating the efficacy of the Brain Floating Point (BFLOAT16) half-precision format for Deep Learning training across image classification, speech recognition, language modeling, generative networks and industrial recommendation systems. BFLOAT16 is attractive for Deep Learning training for two reasons: the range of values it can represent is the same as that of IEEE 754 floating-point format (FP32) and conversion to/from FP32 is simple. Maintaining the same range as FP32 is important to ensure that no hyper-parameter tuning is required for convergence; e.g., IEEE 754 compliant half-precision floating point (FP16) requires hyper-parameter tuning. In this paper, we discuss the flow of tensors and various key operations in mixed precision training, and delve into details of operations, such as the rounding modes for converting FP32 tensors to BFLOAT16. We have implemented a method to emulate BFLOAT16 operations in Tensorflow, Caffe2, IntelCaffe, and Neon for our experiments. Our results show that deep learning training using BFLOAT16 tensors achieves the same state-of-the-art (SOTA) results across domains as FP32 tensors in the same number of iterations and with no changes to hyper-parameters.},
  archivePrefix = {arXiv},
  eprint = {1905.12322},
  eprinttype = {arxiv},
  file = {/Users/milan/Zotero/storage/IBGRBTIN/Kalamkar et al. - 2019 - A Study of BFLOAT16 for Deep Learning Training.pdf;/Users/milan/Zotero/storage/V4DDGS7C/1905.html},
  journal = {arXiv:1905.12322 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{Klower2019,
  title = {Posits as an Alternative to Floats for Weather and Climate Models},
  booktitle = {Proceedings of the {{Conference}} for {{Next Generation Arithmetic}} 2019 on   - {{CoNGA}}'19},
  author = {Kl{\"o}wer, Milan and D{\"u}ben, Peter D. and Palmer, Tim N.},
  year = {2019},
  pages = {1--8},
  publisher = {{ACM Press}},
  address = {{Singapore, Singapore}},
  doi = {10.1145/3316279.3316281},
  abstract = {Posit numbers, a recently proposed alternative to floating-point numbers, claim to have smaller arithmetic rounding errors in many applications. By studying weather and climate models of low and medium complexity (the Lorenz system and a shallow water model) we present benefits of posits compared to floats at 16 bit. As a standardised posit processor does not exist yet, we emulate posit arithmetic on a conventional CPU. Using a shallow water model, forecasts based on 16-bit posits with 1 or 2 exponent bits are clearly more accurate than half precision floats. We therefore propose 16 bit with 2 exponent bits as a standard posit format, as its wide dynamic range of 32 orders of magnitude provides a great potential for many weather and climate models. Although the focus is on geophysical fluid simulations, the results are also meaningful and promising for reduced precision posit arithmetic in the wider field of computational fluid dynamics.},
  isbn = {978-1-4503-7139-1},
  language = {en}
}

@misc{Klower2019a,
  title = {{{SoftPosit}}.Jl -  {{A}} Posit Arithmetic Emulator},
  shorttitle = {Milankl/{{SoftPosit}}.Jl},
  author = {Kl{\"o}wer, Milan and Giordano, Mose},
  year = {2019},
  month = dec,
  doi = {10.5281/zenodo.3590291},
  file = {/Users/milan/Zotero/storage/U7UMAYDJ/3590291.html},
  howpublished = {Zenodo}
}

@article{Kwasniok2014,
  title = {Enhanced Regime Predictability in Atmospheric Low-Order Models Due to Stochastic Forcing},
  author = {Kwasniok, Frank},
  year = {2014},
  month = jun,
  volume = {372},
  pages = {20130286},
  issn = {1364-503X, 1471-2962},
  doi = {10.1098/rsta.2013.0286},
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  language = {en},
  number = {2018}
}

@article{Langroudi2019,
  title = {Deep {{Learning Training}} on the {{Edge}} with {{Low}}-{{Precision Posits}}},
  author = {Langroudi, Hamed F. and Carmichael, Zachariah and Kudithipudi, Dhireesha},
  year = {2019},
  month = jul,
  abstract = {Recently, the posit numerical format has shown promise for DNN data representation and compute with ultra-low precision ([5..8]-bit). However, majority of studies focus only on DNN inference. In this work, we propose DNN training using posits and compare with the floating point training. We evaluate on both MNIST and Fashion MNIST corpuses, where 16-bit posits outperform 16-bit floating point for end-to-end DNN training.},
  archivePrefix = {arXiv},
  eprint = {1907.13216},
  eprinttype = {arxiv},
  file = {/Users/milan/Zotero/storage/FJE5BC8K/Langroudi et al. - 2019 - Deep Learning Training on the Edge with Low-Precis.pdf;/Users/milan/Zotero/storage/2ST243AB/1907.html},
  journal = {arXiv:1907.13216 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@misc{Leong2020,
  title = {{{SoftPosit}}},
  author = {Leong, S. H.},
  year = {2020},
  month = mar,
  doi = {10.5281/zenodo.3709035},
  file = {/Users/milan/Zotero/storage/DA8WHA99/3709035.html},
  howpublished = {Zenodo},
  keywords = {Posits,SoftPosit,UNUM},
  language = {eng}
}

@article{Lorenz1963,
  title = {Deterministic {{Nonperiodic Flow}}},
  author = {Lorenz, Edward N.},
  year = {1963},
  month = mar,
  volume = {20},
  pages = {130--141},
  publisher = {{American Meteorological Society}},
  issn = {0022-4928},
  doi = {10.1175/1520-0469(1963)020<0130:DNF>2.0.CO;2},
  abstract = {Finite systems of deterministic ordinary nonlinear differential equations may be designed to represent forced dissipative hydrodynamic flow. Solutions of these equations can be identified with trajectories in phase space. For those systems with bounded solutions, it is found that nonperiodic solutions are ordinarily unstable with respect to small modifications, so that slightly differing initial states can evolve into considerably different states. Systems with bounded solutions are shown to possess bounded numerical solutions. A simple system representing cellular convection is solved numerically. All of the solutions are found to be unstable, and almost all of them are nonperiodic. The feasibility of very-long-range weather prediction is examined in the light of these results.},
  file = {/Users/milan/Zotero/storage/LISF2756/Lorenz - 1963 - Deterministic Nonperiodic Flow.pdf;/Users/milan/Zotero/storage/RNFRXD2T/1520-0469(1963)0200130DNF2.0.html},
  journal = {Journal of the Atmospheric Sciences},
  number = {2}
}

@article{Lorenz1998,
  title = {Optimal {{Sites}} for {{Supplementary Weather Observations}}: {{Simulation}} with a {{Small Model}}},
  shorttitle = {Optimal {{Sites}} for {{Supplementary Weather Observations}}},
  author = {Lorenz, Edward N. and Emanuel, Kerry A.},
  year = {1998},
  month = feb,
  volume = {55},
  pages = {399--414},
  publisher = {{American Meteorological Society}},
  issn = {0022-4928},
  doi = {10.1175/1520-0469(1998)055<0399:OSFSWO>2.0.CO;2},
  abstract = {Anticipating the opportunity to make supplementary observations at locations that can depend upon the current weather situation, the question is posed as to what strategy should be adopted to select the locations, if the greatest improvement in analyses and forecasts is to be realized. To seek a preliminary answer, the authors introduce a model consisting of 40 ordinary differential equations, with the dependent variables representing values of some atmospheric quantity at 40 sites spaced equally about a latitude circle. The equations contain quadratic, linear, and constant terms representing advection, dissipation, and external forcing. Numerical integration indicates that small errors (differences between solutions) tend to double in about 2 days. Localized errors tend to spread eastward as they grow, encircling the globe after about 14 days. In the experiments presented, 20 consecutive sites lie over the ocean and 20 over land. A particular solution is chosen as the true weather. Every 6 h observations are made, consisting of the true weather plus small random errors, at every land site, and at one ocean site to be selected by the strategy being considered. An analysis is then made, consisting of observations where observations are made and previously made 6-h forecasts elsewhere. Forecasts are made for each site at ranges from 6 h to 10 days. In all forecasts, a slightly weakened external forcing is used to simulate the model error. This process continues for 5 years, and mean-square forecast errors at each site at each range are accumulated. Strategies that attempt to locate the site where the current analysis, as made without a supplementary observation, is most greatly in error are found to perform better than those that seek the oceanic site to which a chosen land site is most sensitive at a chosen range. Among the former are strategies based on the ``breeding'' method, a variant of singular vectors, and ensembles of ``replicated'' observations; the last of these outperforms the others. The authors speculate as to the applicability of these findings to models with more realistic dynamics or without extensive regions devoid of routine observations, and to the real world.},
  file = {/Users/milan/Zotero/storage/SPTVRZJI/Lorenz and Emanuel - 1998 - Optimal Sites for Supplementary Weather Observatio.pdf;/Users/milan/Zotero/storage/QMVVGJAI/1520-0469(1998)0550399OSFSWO2.0.html},
  journal = {Journal of the Atmospheric Sciences},
  number = {3}
}

@book{MacKay2003,
  title = {Information {{Theory}}, {{Inference}} and {{Learning Algorithms}}},
  author = {MacKay, David},
  year = {2003},
  edition = {First},
  publisher = {{Cambridge University Press}},
  isbn = {978-0-521-64298-9}
}

@inproceedings{Markidis2018,
  title = {{{NVIDIA Tensor Core Programmability}}, {{Performance Precision}}},
  booktitle = {2018 {{IEEE International Parallel}} and {{Distributed Processing Symposium Workshops}} ({{IPDPSW}})},
  author = {Markidis, Stefano and Chien, Steven Wei Der and Laure, Erwin and Peng, Ivy Bo and Vetter, Jeffrey S.},
  year = {2018},
  month = may,
  pages = {522--531},
  issn = {null},
  doi = {10.1109/IPDPSW.2018.00091},
  abstract = {The NVIDIA Volta GPU microarchitecture introduces a specialized unit, called Tensor Core that performs one matrix-multiply-and-accumulate on 4x4 matrices per clock cycle. The NVIDIA Tesla V100 accelerator, featuring the Volta microarchitecture, provides 640 Tensor Cores with a theoretical peak performance of 125 Tflops/s in mixed precision. In this paper, we investigate current approaches to program NVIDIA Tensor Cores, their performances and the precision loss due to computation in mixed precision. Currently, NVIDIA provides three different ways of programming matrix-multiply-and-accumulate on Tensor Cores: the CUDA Warp Matrix Multiply Accumulate (WMMA) API, CUTLASS, a templated library based on WMMA, and cuBLAS GEMM. After experimenting with different approaches, we found that NVIDIA Tensor Cores can deliver up to 83 Tflops/s in mixed precision on a Tesla V100 GPU, seven and three times the performance in single and half precision respectively. A WMMA implementation of batched GEMM reaches a performance of 4 Tflops/s. While precision loss due to matrix multiplication with half precision input might be critical in many HPC applications, it can be considerably reduced at the cost of increased computation. Our results indicate that HPC applications using matrix multiplications can strongly benefit from using of NVIDIA Tensor Cores.},
  file = {/Users/milan/Zotero/storage/9GJDHGPW/Markidis et al. - 2018 - NVIDIA Tensor Core Programmability, Performance Pr.pdf;/Users/milan/Zotero/storage/D2WE2S8V/8425458.html},
  keywords = {application program interfaces,Computer architecture,cuBLAS GEMM,CUDA Warp Matrix Multiply Accumulate API,CUDA WMMA API,CUTLASS,GEMM,GPU Programming,graphics processing units,Graphics processing units,Hardware,HPC applications,Instruction sets,matrix multiplication,Mixed Precision,Neural networks,NVIDIA Tensor Core programmability,NVIDIA Tensor Cores,NVIDIA Tesla V100 accelerator,NVIDIA Volta GPU microarchitecture,parallel architectures,parallel programming,Programming,programming matrix-multiply- accumulate,Tensile stress,tensors,Tesla V100 GPU,Volta microarchitecture}
}

@article{Morris1971,
  title = {Tapered {{Floating Point}}: {{A New Floating}}-{{Point Representation}}},
  shorttitle = {Tapered {{Floating Point}}},
  author = {Morris, R.},
  year = {1971},
  month = dec,
  volume = {C-20},
  pages = {1578--1579},
  issn = {2326-3814},
  doi = {10.1109/T-C.1971.223174},
  abstract = {It is well known that there is a possible tradeoff in the binary representation of floating-point numbers in which one bit of accuracy can be gained at the cost of halving the exponent range, and vice versa. A way in which the exponent range can be greatly increased while preserving full accuracy for most computations is suggested.},
  file = {/Users/milan/Zotero/storage/ILM24D4E/1671767.html},
  journal = {IEEE Transactions on Computers},
  keywords = {Acuracy; exponent range; floating point; number representation.},
  number = {12}
}

@article{Palmer2012,
  title = {Towards the Probabilistic {{Earth}}-System Simulator: A Vision for the Future of Climate and Weather Prediction},
  shorttitle = {Towards the Probabilistic {{Earth}}-System Simulator},
  author = {Palmer, T. N.},
  year = {2012},
  month = apr,
  volume = {138},
  pages = {841--861},
  issn = {00359009},
  doi = {10.1002/qj.1923},
  journal = {Quarterly Journal of the Royal Meteorological Society},
  language = {en},
  number = {665}
}

@article{Palmer2014,
  title = {Stochastic Modelling and Energy-Efficient Computing for Weather and Climate Prediction},
  author = {Palmer, Tim and D{\"u}ben, Peter and McNamara, Hugh},
  year = {2014},
  month = jun,
  volume = {372},
  pages = {20140118},
  issn = {1364-503X, 1471-2962},
  doi = {10.1098/rsta.2014.0118},
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  language = {en},
  number = {2018}
}

@article{Palmer2015,
  title = {Modelling: {{Build}} Imprecise Supercomputers},
  shorttitle = {Modelling},
  author = {Palmer, Tim},
  year = {2015},
  month = oct,
  volume = {526},
  pages = {32},
  doi = {10.1038/526032a},
  abstract = {Energy-optimized hybrid computers with a range of processor accuracies will advance modelling in fields from climate change to neuroscience, says Tim Palmer.},
  file = {/Users/milan/Zotero/storage/QL9D2WFY/modelling-build-imprecise-supercomputers-1.html},
  journal = {Nature News},
  language = {en},
  number = {7571}
}

@article{Palmer2019,
  title = {Stochastic Weather and Climate Models},
  author = {Palmer, T. N.},
  year = {2019},
  month = jul,
  volume = {1},
  pages = {463--471},
  publisher = {{Nature Publishing Group}},
  issn = {2522-5820},
  doi = {10.1038/s42254-019-0062-2},
  abstract = {Although the differential equations that describe the physical climate system are deterministic, there are reasons, both theoretical and practical, why computational representations of these equations should be stochastic. This Perspective surveys the benefits of stochastic modelling of weather and climate.},
  copyright = {2019 The Publisher},
  file = {/Users/milan/Zotero/storage/87DNANFD/s42254-019-0062-2.html},
  journal = {Nature Reviews Physics},
  language = {en},
  number = {7}
}

@article{Rudisuhli2013,
  title = {{{COSMO}} in Single Precision},
  author = {R{\"u}dis{\"u}hli, Stefan and Walser, Andr{\'e} and Fuhrer, Oliver},
  year = {2013},
  pages = {5--1},
  file = {/Users/milan/Zotero/storage/ZJHTAGB2/Rüdisühli et al. - 2013 - COSMO in single precision.pdf},
  journal = {Cosmo Newsletter},
  number = {14}
}

@article{Russell2017,
  title = {Exploiting the Chaotic Behaviour of Atmospheric Models with Reconfigurable Architectures},
  author = {Russell, Francis P. and D{\"u}ben, Peter D. and Niu, Xinyu and Luk, Wayne and Palmer, T.N.},
  year = {2017},
  month = dec,
  volume = {221},
  pages = {160--173},
  issn = {00104655},
  doi = {10.1016/j.cpc.2017.08.011},
  abstract = {Reconfigurable architectures are becoming mainstream: Amazon, Microsoft and IBM are supporting such architectures in their data centres. The computationally intensive nature of atmospheric modelling is an attractive target for hardware acceleration using reconfigurable computing. Performance of hardware designs can be improved through the use of reduced-precision arithmetic, but maintaining appropriate accuracy is essential. We explore reduced-precision optimisation for simulating chaotic systems, targeting atmospheric modelling, in which even minor changes in arithmetic behaviour will cause simulations to diverge quickly. The possibility of equally valid simulations having differing outcomes means that standard techniques for comparing numerical accuracy are inappropriate. We use the Hellinger distance to compare statistical behaviour between reduced-precision CPU implementations to guide reconfigurable designs of a chaotic system, then analyse accuracy, performance and power efficiency of the resulting implementations. Our results show that with only a limited loss in accuracy corresponding to less than 10\% uncertainty in input parameters, the throughput and energy efficiency of a single-precision chaotic system implemented on a Xilinx Virtex-6 SX475T Field Programmable Gate Array (FPGA) can be more than doubled.},
  journal = {Computer Physics Communications},
  language = {en}
}

@article{Smolarkiewicz1992,
  title = {A {{Class}} of {{Semi}}-{{Lagrangian Approximations}} for {{Fluids}}},
  author = {Smolarkiewicz, Piotr K. and Pudykiewicz, Janusz A.},
  year = {1992},
  month = nov,
  volume = {49},
  pages = {2082--2096},
  publisher = {{American Meteorological Society}},
  issn = {0022-4928},
  doi = {10.1175/1520-0469(1992)049<2082:ACOSLA>2.0.CO;2},
  abstract = {This paper discusses a class of finite-difference approximations to the evolution equations of fluid dynamics. These approximations derive from elementary properties of differential forms. Values of a fluid variable {$\psi$} at any two points of a space-time continuum are related through the integral of the space-time gradient of {$\psi$} along an arbitrary contour connecting these two points (Stokes' theorem). Noting that spatial and temporal components of the gradient are related through the fluid equations, and selecting the contour composed of a parcel trajectory and an appropriate residual, leads to the integral form of the fluid equations, which is particularly convenient for finite-difference approximations. In these equations, the inertial and forcing terms are separated such that forces are integrated along a parcel trajectory (the Lagrangian aspect), whereas advection of the variable is evaluated along the residual contour (the Eulerian aspect). The virtue of this method is an extreme simplicity of the resulting solver; the entire model for a fluid may be essentially built upon a single one-dimensional Eulerian advection scheme while retaining the formal accuracy of its constant-coefficient limit. The Lagrangian aspect of the approach allows for large-Courant-number ({$>$}1) computations in a broad spectrum of dynamic applications. Theoretical considerations are illustrated with examples of applications to selected classical problems of atmospheric fluid dynamics. Since the theoretical arguments adopted in this paper assume differentiability of fluid variables, fluid systems admitting truly discontinuous solutions (e.g., shock waves, hydraulic jumps) are formally excluded from our considerations.},
  file = {/Users/milan/Zotero/storage/3ZKFM5ZF/Smolarkiewicz and Pudykiewicz - 1992 - A Class of Semi-Lagrangian Approximations for Flui.pdf;/Users/milan/Zotero/storage/DH4Y6H5B/1520-0469(1992)0492082ACOSLA2.0.html},
  journal = {Journal of the Atmospheric Sciences},
  number = {22}
}

@article{Tantet2018,
  title = {Resonances in a {{Chaotic Attractor Crisis}} of the {{Lorenz Flow}}},
  author = {Tantet, Alexis and Lucarini, Valerio and Dijkstra, Henk A.},
  year = {2018},
  month = feb,
  volume = {170},
  pages = {584--616},
  issn = {0022-4715, 1572-9613},
  doi = {10.1007/s10955-017-1938-0},
  abstract = {Local bifurcations of stationary points and limit cycles have successfully been characterized in terms of the critical exponents of these solutions. Lyapunov exponents and their associated covariant Lyapunov vectors have been proposed as tools for supporting the understanding of critical transitions in chaotic dynamical systems. However, it is in general not clear how the statistical properties of dynamical systems change across a boundary crisis during which a chaotic attractor collides with a saddle. This behavior is investigated here for a boundary crisis in the Lorenz flow, for which neither the Lyapunov exponents nor the covariant Lyapunov vectors provide a criterion for the crisis. Instead, the convergence of the time evolution of probability densities to the invariant measure, governed by the semigroup of transfer operators, is expected to slow down at the approach of the crisis. Such convergence is described by the eigenvalues of the generator of this semigroup, which can be divided into two families, referred to as the stable and unstable Ruelle\textendash{}Pollicott resonances, respectively. The former describes the convergence of densities to the attractor (or escape from a repeller) and is estimated from many short time series sampling the state space. The latter is responsible for the decay of correlations, or mixing, and can be estimated from a long times series, invoking ergodicity. It is found numerically for the Lorenz flow that the stable resonances do approach the imaginary axis during the crisis, as is indicative of the loss of global stability of the attractor. On the other hand, the unstable resonances, and a fortiori the decay of correlations, do not flag the proximity of the crisis, thus questioning the usual design of early warning indicators of boundary crises of chaotic attractors and the applicability of response theory close to such crises.},
  journal = {Journal of Statistical Physics},
  language = {en},
  number = {3}
}

@inproceedings{Targett2015,
  title = {Lower Precision for Higher Accuracy: {{Precision}} and Resolution Exploration for Shallow Water Equations},
  shorttitle = {Lower Precision for Higher Accuracy},
  booktitle = {2015 {{International Conference}} on {{Field Programmable Technology}} ({{FPT}})},
  author = {Targett, James Stanley and Niu, Xinyu and Russell, Francis and Luk, Wayne and Jeffress, Stephen and Duben, Peter},
  year = {2015},
  month = dec,
  pages = {208--211},
  publisher = {{IEEE}},
  address = {{Queenstown, New Zealand}},
  doi = {10.1109/FPT.2015.7393152},
  abstract = {Accurate forecasts of future climate with numerical models of atmosphere and ocean are of vital importance. However, forecast quality is often limited by the available computational power. This paper investigates the acceleration of a C-grid shallow water model through the use of reduced precision targeting FPGA technology. Using a double-gyre scenario, we show that the mantissa length of variables can be reduced to 14 bits without affecting the accuracy beyond the error inherent in the model. Our reduced precision FPGA implementation runs 5.4 times faster than a double precision FPGA implementation, and 12 times faster than a multi-threaded CPU implementation. Moreover, our reduced precision FPGA implementation uses 39 times less energy than the CPU implementation and can compute a 100x100 grid for the same energy that the CPU implementation would take for a 29x29 grid.},
  isbn = {978-1-4673-9091-0},
  language = {en}
}

@article{Thornes2017,
  title = {On the Use of Scale-Dependent Precision in {{Earth System}} Modelling: {{Scale}}-{{Dependent Precision}} in {{Earth System Modelling}}},
  shorttitle = {On the Use of Scale-Dependent Precision in {{Earth System}} Modelling},
  author = {Thornes, Tobias and D{\"u}ben, Peter and Palmer, Tim},
  year = {2017},
  month = jan,
  volume = {143},
  pages = {897--908},
  issn = {00359009},
  doi = {10.1002/qj.2974},
  journal = {Quarterly Journal of the Royal Meteorological Society},
  language = {en},
  number = {703}
}

@article{TintoPrims2019,
  title = {How to Use Mixed Precision in Ocean Models: Exploring a Potential Reduction of Numerical Precision in {{NEMO}} 4.0 and {{ROMS}} 3.6},
  shorttitle = {How to Use Mixed Precision in Ocean Models},
  author = {Tint{\'o} Prims, Oriol and Acosta, Mario C. and Moore, Andrew M. and Castrillo, Miguel and Serradell, Kim and Cort{\'e}s, Ana and {Doblas-Reyes}, Francisco J.},
  year = {2019},
  month = jul,
  volume = {12},
  pages = {3135--3148},
  publisher = {{Copernicus GmbH}},
  issn = {1991-959X},
  doi = {https://doi.org/10.5194/gmd-12-3135-2019},
  abstract = {{$<$}p{$><$}strong{$>$}Abstract.{$<$}/strong{$>$} Mixed-precision approaches can provide substantial speed-ups for both computing- and memory-bound codes with little effort. Most scientific codes have overengineered the numerical precision, leading to a situation in which models are using more resources than required without knowing where they are required and where they are not. Consequently, it is possible to improve computational performance by establishing a more appropriate choice of precision. The only input that is needed is a method to determine which real variables can be represented with fewer bits without affecting the accuracy of the results. This paper presents a novel method that enables modern and legacy codes to benefit from a reduction of the precision of certain variables without sacrificing accuracy. It consists of a simple idea: we reduce the precision of a group of variables and measure how it affects the outputs. Then we can evaluate the level of precision that they truly need. Modifying and recompiling the code for each case that has to be evaluated would require a prohibitive amount of effort. Instead, the method presented in this paper relies on the use of a tool called a reduced-precision emulator (RPE) that can significantly streamline the process. Using the RPE and a list of parameters containing the precisions that will be used for each real variable in the code, it is possible within a single binary to emulate the effect on the outputs of a specific choice of precision. When we are able to emulate the effects of reduced precision, we can proceed with the design of the tests that will give us knowledge of the sensitivity of the model variables regarding their numerical precision. The number of possible combinations is prohibitively large and therefore impossible to explore. The alternative of performing a screening of the variables individually can provide certain insight about the required precision of variables, but, on the other hand, other complex interactions that involve several variables may remain hidden. Instead, we use a divide-and-conquer algorithm that identifies the parts that require high precision and establishes a set of variables that can handle reduced precision. This method has been tested using two state-of-the-art ocean models, the Nucleus for European Modelling of the Ocean (NEMO) and the Regional Ocean Modeling System (ROMS), with very promising results. Obtaining this information is crucial to build an actual mixed-precision version of the code in the next phase that will bring the promised performance benefits.{$<$}/p{$>$}},
  file = {/Users/milan/Zotero/storage/ZLMJCFIP/Tintó Prims et al. - 2019 - How to use mixed precision in ocean models explor.pdf;/Users/milan/Zotero/storage/3XNIE5KW/2019.html},
  journal = {Geoscientific Model Development},
  language = {English},
  number = {7}
}

@book{Vallis2006,
  title = {Atmospheric and {{Oceanic Fluid Dynamics}}},
  author = {Vallis, Geoffrey K.},
  year = {2006},
  publisher = {{Cambridge University Press}}
}

@article{Vana2017,
  title = {Single {{Precision}} in {{Weather Forecasting Models}}: {{An Evaluation}} with the {{IFS}}},
  shorttitle = {Single {{Precision}} in {{Weather Forecasting Models}}},
  author = {V{\'a}{\v n}a, Filip and D{\"u}ben, Peter and Lang, Simon and Palmer, Tim and Leutbecher, Martin and Salmond, Deborah and Carver, Glenn},
  year = {2017},
  month = feb,
  volume = {145},
  pages = {495--502},
  issn = {0027-0644, 1520-0493},
  doi = {10.1175/MWR-D-16-0228.1},
  abstract = {Earth's climate is a nonlinear dynamical system with scale-dependent Lyapunov exponents. As such, an important theoretical question for modeling weather and climate is how much real information is carried in a model's physical variables as a function of scale and variable type. Answering this question is of crucial practical importance given that the development of weather and climate models is strongly constrained by available supercomputer power. As a starting point for answering this question, the impact of limiting almost all real-number variables in the forecasting mode of ECMWF Integrated Forecast System (IFS) from 64 to 32 bits is investigated. Results for annual integrations and medium-range ensemble forecasts indicate no noticeable reduction in accuracy, and an average gain in computational efficiency by approximately 40\%. This study provides the motivation for more scale-selective reductions in numerical precision.},
  journal = {Monthly Weather Review},
  language = {en},
  number = {2}
}

@inproceedings{vanDam2019,
  title = {An {{Accelerator}} for {{Posit Arithmetic Targeting Posit Level}} 1 {{BLAS Routines}} and {{Pair}}-{{HMM}}},
  booktitle = {Proceedings of the {{Conference}} for {{Next Generation Arithmetic}} 2019},
  author = {{van Dam}, Laurens and Peltenburg, Johan and {Al-Ars}, Zaid and Hofstee, H. Peter},
  year = {2019},
  month = mar,
  pages = {1--10},
  publisher = {{Association for Computing Machinery}},
  address = {{Singapore, Singapore}},
  doi = {10.1145/3316279.3316284},
  abstract = {The newly proposed posit number format uses a significantly different approach to represent floating point numbers. This paper introduces a framework for posit arithmetic in reconfigurable logic that maintains full precision in intermediate results. We present the design and implementation of a L1 BLAS arithmetic accelerator on posit vectors leveraging Apache Arrow. For a vector dot product with an input vector length of 106 elements, a hardware speedup of approximately 104 is achieved as compared to posit software emulation. For 32-bit numbers, the decimal accuracy of the posit dot product results improve by one decimal of accuracy on average compared to a software implementation, and two extra decimals compared to the IEEE754 format. We also present a posit-based implementation of pair-HMM. In this case, the hardware speedup vs. a posit-based software implementation ranges from 105 to 106. With appropriate initial scaling constants, accuracy improves on an implementation based on IEEE 754.},
  file = {/Users/milan/Zotero/storage/RR2FCBD4/van Dam et al. - 2019 - An Accelerator for Posit Arithmetic Targeting Posi.pdf},
  isbn = {978-1-4503-7139-1},
  keywords = {accelerator,arithmetic,BLAS,decimal accuracy,FPGA,pair-HMM,posit,unum,unum-III},
  series = {{{CoNGA}}'19}
}


