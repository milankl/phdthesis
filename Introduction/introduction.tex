\chapter{Introduction}
\label{chap:introduction}

Numerical models of weather and climate use binary numbers to calculate and store information.
All information in a climate model, or its data output, is encoded into bits. Representing
real numbers with bits bisects for every bit the real axis into different sections, similar to a binary tree.
While there are various ways to encode a real number into bits, some bits will be more significant, others
less, to encode a good approximation. For example, flipping the sign bit will
generally result in the most significant change of the represented number. Flipping other bits will
cause a much less significant change. Depending on the encoding and the number of bits used,
some bits will be more essential than others in a climate model or its data to obtain a meaningful
simulation. Especially the least significant bits may provide a precision that is more precise than
necessary to resolve the uncertainty inherent in most numbers in climate data or their computation.

The uncertainty of weather or climate data, whether simulated or measured, depends on many different
factors and use cases. While it may be crucial to distinguish between 0\textdegree{C} and slightly warmer
to measure frost on a field, the exact temperature on a single field will have negligible impact on the
global mean surface temperature averaged over decades. On the other hand, a weather forecast for a
given location is rarely more than 1\textdegree{C} accurate, but to quantify global warming from year to year,
a higher precision of 0.1\textdegree{C} or even 0.01\textdegree{C} is needed \citep{Haustein2017}. In general
every number will come with a different uncertainty, some of which will amplify during calculation or cancel out.

While the uncertainty in climate computations and data is variable, the precision of binary number formats
has been standardised since the 1980s \citep{IEEE1985,IEEE2008}. Consequently, only two levels of precision
are widely available (single and double precision) for all areas of computing, regardless of the varying levels of
required precision per application. So-called single-precision floating-point numbers use 32 bits to encode a
real number, providing at least 7 decimal places of precision in the representable range between $10^{-38}$
and $10^{38}$. Double-precision floating-point numbers use 64 bits instead, resolving numbers over more
than 600 orders of magnitude at a precision of more than 16 decimal places. 

64-bit computations are the de facto standard for scientific computing. Most programming languages use
it as the default precision, which enabled decades of scientific calculations while ignoring the remaining,
in most cases indeed negligible, rounding errors. Applications where computational performance is not essential,
or where optimisations for lower precision have not been applied, are generally advised to use 64 bits.
However, many areas of high-performance computing, including the data compression and storage,
can benefit from low-precision computations \citep{Palmer2015}. Requiring the world's largest supercomputers,
numerical weather prediction and climate projections are such high-performance computing applications \citep{Bauer2021a}.

On many processors two 32-bit computations can be vectorised into a single 64-bit operation, effectively
making computations with 32 bits twice as fast as with 64 bits. Similarly, the vectorisation of 16 or even 8-bit
computations, if supported, allow for 4 or 8 times as many operations within the same time \citep{Sato2020}.
But vectorisation is not the only advantage of low-precision computing. The performance bottleneck of many
applications is the speed at which data can be loaded from memory \citep{Muller2019a}. Decreasing the time
it takes for the computation alone will not increase performance if the processor has to wait for data to be loaded
from memory. However, such a memory-bound application would greatly benefit if every number in memory is stored
in only 32, 16 or even 8 bit instead of 64, which reduces the data volume transferred between memory and processor.
An array of 16-bit numbers is loaded four times as fast from memory, compared to an array of the same size but with
64 bits per entry.

While most processors only support 32 and 64-bit computations, more freedom is available for data compression and storage:
In principle, numbers encoded with an arbitrary number of bits can be packed into an array, such that also
10, 17 or 31 bits per number are possible \citep{WMO2003}. Although those bits will need to be unpacked into 32 or 64 bits for 
post-processing, the data volume would be greatly reduced, lowering storage requirements in data archives.
Consequently, the bits per number can directly reflect the uncertainty in data, provided such an uncertainty is known.
Storing more bits to preserve a higher precision therefore corresponds to storing \emph{false information} as the
uncertainty masks the unnecessarily high precision \citep{Jeffress2017}. In general, it is an open question how to distinguish between
real and false information in data, as the uncertainties may be unknown. This question directly translates to
the bits per number, of which only as few as essential should be used to preserve the real information.

Modern hardware has started to support also 16-bit computations next to the traditional 32 and 64 bits,
adding so-called half precision to the set of available levels of precision for computing \citep{Jouppi2018,Odajima2020}.
Fugaku, the fastest supercomputer as of June 2021 (\href{https://top500.org}{TOP500}, \cite{Dongarra2011}), consists
of many million processors all of which are capable to compute 16-bit numbers four times as fast as 64 bits. However, these 
16-bit half-precision floating-point numbers have a reduced precision of less than 4 decimal places over a limited range from
$6 \cdot 10^{-8}$ to $65504$. Numbers with an absolute value smaller or larger cannot be represented. Using 16-bit computations in 
complex simulations is therefore challenging \citep{Klower2020a}: The range of numbers occurring in simulation has to be controlled,
which requires scaling of algorithms to avoid very small and very large numbers outside the representable range.
Rounding errors arising from computations at lower precision should not exceed other errors, which may require
some algorithms to be resilient to low precision such that rounding errors do not accumulate over time \citep{Higham2002}.
For complex simulations it is often difficult to understand in which computations precision or range issues arise
as most error analysis is based on the data output and not on intermediate results within a simulation \citep{Fevotte2019,Jezequel2008}.

Simulations with 64 bits are not rounding error-free, but the error analysis is challenging or impossible in
situations where analytical solutions are unknown. Even higher precision can be used as a reference, assumed to be
error-free, but simulated systems can differ from their analytic counterparts at any finite precision \citep{Boghosian2019}.
As an alternative, interval arithmetic simultaneously rounds up and down to contain the exact result and therefore provides
an upper and lower bound on the rounding error \citep{Gustafson2015}. However, in many chaotic systems these bounds
diverge quickly and the interval becomes meaningless as the effective error is highly overestimated.

A weather or climate model simulates a chaotic system such that every rounding error grows and eventually causes the
simulated weather to change globally \citep{Lorenz1963,Palmer2014b}. In practice, the statistics of any low precision
simulation have to be compared to the statistics from a high precision reference. A transition to lower precision
can then be considered if no significant deviation of the simulated statistics with a changing precision is found. Such a comparison
can be understood as a Turing test \citep{Turing1950, Baker2019} in which a low precision simulation or data set has to pass in
comparison to a high precision reference with respect to any metrics that are considered important for a simulation's use cases.
For weather forecast models higher rounding errors may be acceptable if the skill in predicting the weather at a given location within
a week is not degraded \citep{Duben2014}. However, the impacts of rounding errors on such complex systems can be diverse
and not obvious. While a change in precision impacts the simulation of every chaotic dynamical system, the usefulness of a simulation
can be unaffected even over a large range of precision \citep{Palmer2015}.

In this thesis, several open questions of low-precision climate computing are addressed. Chapter \ref{chap:methods} introduces
the methods that are used throughout several following chapters. Various binary number formats are presented that can be used
to approximate real numbers. Some formats are widely available standards, others are recently proposed alternatives.
The concepts of how different number formats encode information into bits are essential to understanding how changing
precision affects climate simulations and data. How non-standard number formats can be used flexibly in simulations
is explained with code composability. Error norms are discussed which quantify the rounding errors arising from
different available rounding modes. The last section in this chapter introduces core concepts of information theory to analyse the
information contained within the bits of binary numbers.

Chapter \ref{chap:compression} develops the concept of information-preserving compression for climate data. The bitwise real
information content is defined and proposed as a metric to distinguish between the real and false information in climate data.
Using a large set of atmospheric data, the real information content leads to an individual precision for every variable like
temperature or humidity, objectively representing its uncertainty. Consequently, only the real information is subject
to data compression while discarding bits without information.

Chapter \ref{chap:orbits} investigates the impact of rounding errors from various numbers format in representing simple
chaotic dynamical systems. Bitwise periodic orbits are systematically analysed to better understand how the structure
of attractors is affected with lower precision. Invariant measures quantify the statistics of a simulated system, which are
compared as a function of the number format and its precision.

Chapter \ref{chap:shallow_water} explores the scope of various 16-bit formats when simulating the shallow water model,
a medium-complexity fluid circulation model. The impact of rounding errors on the simulated dynamics is analysed,
including geostrophy and gravity waves. Short-term forecasts are used to assess the growth of rounding errors arising
from 16-bit calculations and when mixing 16 and 32 bits of precision used for different calculations within the simulation.

Chapter \ref{chap:hardware} implements the fluid circulation model
\href{https://github.com/milankl/ShallowWaters.jl}{ShallowWaters.jl} on 16-bit hardware. The Fujitsu
processor A64FX is used to investigate the potential of accelerating weather and climate models with 16-bit calculations. The
simulated shallow water equations are systematically rescaled to fit into the limited range of representable numbers
with 16 bit. A compensated time integration is presented that reduces precision issues with 16 bits. The chapter
concludes with an analysis of the performance gain between 64, 32 and 16 bits on A64FX.

Chapter \ref{chap:conclusions} summarises all previous chapters and discusses an outlook of low-precision climate
computing for improved forecasts of the global weather and the Earth's climate system in the future.
